{
  "label": "spec",
  "artifact_type": "scope_spec",
  "artifact_id": "sha256:00449ed9d7f8a3240548c2009abc6e3ccafc743718f92388a22452f2b9699234",
  "input_ids": [
    "sha256:3f6f65d0167c1fdb1bae6d27ac958430756a890c10718761fd3bc217cb3f6284"
  ],
  "prompt_id": null,
  "model_config": null,
  "created_at": "2026-02-17T11:53:44.271544",
  "content": "{\n  \"scope_id\": \"cascading_failure_01\",\n  \"domain\": \"system_logs\",\n  \"description\": \"API gateway log summaries showing cascading dependency failure\",\n  \"generation\": {\n    \"temperature\": 0.7,\n    \"seed\": 42\n  },\n  \"episodes\": {\n    \"count\": 30,\n    \"timeline\": {\n      \"start\": \"2024-01-15\",\n      \"interval\": \"1d\"\n    },\n    \"format\": \"Daily API gateway log summary with endpoint stats, error rates, latencies, and on-call notes\",\n    \"target_words\": 500\n  },\n  \"scenario\": {\n    \"setting\": \"A microservices e-commerce platform with API gateway logging.\\nServices: checkout (service-C), fraud checks (service-B),\\nthird-party geo-lookup API, shared connection pool.\\nThe gateway produces daily summaries with endpoint hit counts,\\np50/p95/p99 latencies, error rates, and on-call engineer notes.\\n\",\n    \"voice\": \"Terse operational log style. Bullet points, metrics,\\nshort incident notes. No narrative prose.\\n\"\n  },\n  \"arc\": [\n    {\n      \"id\": \"baseline\",\n      \"episodes\": \"1-8\",\n      \"description\": \"Normal operations. Establish baseline metrics and patterns. All services healthy. Typical traffic patterns, routine deploys, normal latencies.\",\n      \"signal_density\": \"none\"\n    },\n    {\n      \"id\": \"early_signal\",\n      \"episodes\": \"9-15\",\n      \"description\": \"Geo-lookup latency creeping 200ms\\u2192400ms. Service-B retries slightly elevated. No user-facing impact yet. The degradation is subtle and buried among normal operational noise.\",\n      \"signal_density\": \"low\"\n    },\n    {\n      \"id\": \"red_herring\",\n      \"episodes\": \"14-16\",\n      \"description\": \"Service-C deploy coincides with first checkout errors. On-call blames deploy. Team focuses investigation on the deploy. This is a red herring \\u2014 the real issue is the geo-lookup degradation.\",\n      \"signal_density\": \"medium\"\n    },\n    {\n      \"id\": \"escalation\",\n      \"episodes\": \"16-22\",\n      \"description\": \"Checkout failures increasing. Rollback of service-C attempted but doesn't help. Connection pool exhaustion becoming visible in logs. Service-B retry storms amplifying the problem.\",\n      \"signal_density\": \"high\"\n    },\n    {\n      \"id\": \"root_cause\",\n      \"episodes\": \"23-30\",\n      \"description\": \"Geo-lookup at 800ms+. Service-B retry storms fully visible. Full causal chain apparent: geo-lookup degradation \\u2192 service-B retries \\u2192 connection pool exhaustion \\u2192 checkout failures. Resolution begins.\",\n      \"signal_density\": \"high\"\n    }\n  ],\n  \"noise\": {\n    \"description\": \"Normal traffic fluctuations, routine deploys, disk alerts, cert renewals, CDN stats, A/B tests, standard monitoring noise\",\n    \"examples\": [\n      \"CDN cache hit rate 94.2%, within normal range\",\n      \"Deployed auth-service v2.3.1, no issues\",\n      \"Disk usage alert on metrics-db-02 (82%), cleanup scheduled\",\n      \"A/B test 'checkout-v2' at 15% rollout, no anomalies\",\n      \"SSL cert renewed for api.example.com, expires 2025-01-14\"\n    ]\n  },\n  \"distractors\": {\n    \"count\": 90,\n    \"target_words\": 500,\n    \"themes\": [\n      {\n        \"id\": \"dns_migration\",\n        \"scenario\": \"A DNS infrastructure team migrating from on-prem BIND to\\ncloud-managed DNS. Daily ops logs track zone transfers,\\nTTL changes, propagation delays, DNSSEC signing.\\n\",\n        \"excluded_terms\": [\n          \"geo-lookup\",\n          \"connection pool\",\n          \"retry\",\n          \"service-B\",\n          \"fraud\",\n          \"checkout\",\n          \"cascading\"\n        ]\n      },\n      {\n        \"id\": \"storage_capacity\",\n        \"scenario\": \"A data platform team managing distributed storage clusters.\\nDaily capacity reports: node utilization, rebalance ops,\\ncompaction stats, backup verification, quota enforcement.\\n\",\n        \"excluded_terms\": [\n          \"geo-lookup\",\n          \"connection pool\",\n          \"retry\",\n          \"service-B\",\n          \"fraud\",\n          \"checkout\",\n          \"latency spike\"\n        ]\n      },\n      {\n        \"id\": \"auth_audit\",\n        \"scenario\": \"An identity/access management team running quarterly\\ncompliance audits. Daily audit logs: SSO sessions,\\nMFA enrollment, permission changes, token rotation,\\nRBAC policy updates.\\n\",\n        \"excluded_terms\": [\n          \"geo-lookup\",\n          \"connection pool\",\n          \"retry\",\n          \"service-B\",\n          \"fraud\",\n          \"checkout\",\n          \"cascading\"\n        ]\n      }\n    ],\n    \"seed\": 99,\n    \"max_similarity\": 0.3\n  },\n  \"key_facts\": [\n    {\n      \"id\": \"geo_latency_degradation\",\n      \"fact\": \"geo-lookup API latency increasing\",\n      \"first_appears\": \"early_signal:1\",\n      \"reinforced_in\": [\n        \"early_signal:4\",\n        \"escalation:2\",\n        \"root_cause:3\"\n      ]\n    },\n    {\n      \"id\": \"pool_exhaustion\",\n      \"fact\": \"connection pool exhaustion\",\n      \"first_appears\": \"escalation:3\",\n      \"reinforced_in\": [\n        \"escalation:5\",\n        \"root_cause:2\"\n      ]\n    },\n    {\n      \"id\": \"service_b_retries\",\n      \"fact\": \"service-B retry rate elevated\",\n      \"first_appears\": \"early_signal:2\",\n      \"reinforced_in\": [\n        \"escalation:1\",\n        \"root_cause:4\"\n      ]\n    },\n    {\n      \"id\": \"deploy_red_herring\",\n      \"fact\": \"service-C deploy is not the root cause\",\n      \"first_appears\": \"red_herring:2\",\n      \"reinforced_in\": [\n        \"escalation:1\"\n      ]\n    },\n    {\n      \"id\": \"dns_not_root_cause\",\n      \"fact\": \"the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure\",\n      \"first_appears\": \"baseline:3\",\n      \"reinforced_in\": [\n        \"early_signal:3\",\n        \"escalation:4\"\n      ]\n    },\n    {\n      \"id\": \"storage_systems_normal\",\n      \"fact\": \"the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure\",\n      \"first_appears\": \"baseline:4\",\n      \"reinforced_in\": [\n        \"early_signal:3\"\n      ]\n    },\n    {\n      \"id\": \"auth_systems_normal\",\n      \"fact\": \"user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor\",\n      \"first_appears\": \"baseline:5\",\n      \"reinforced_in\": [\n        \"early_signal:4\"\n      ]\n    },\n    {\n      \"id\": \"latency_progressive_degradation\",\n      \"fact\": \"geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike\",\n      \"first_appears\": \"escalation:1\",\n      \"reinforced_in\": [\n        \"escalation:4\",\n        \"root_cause:2\"\n      ]\n    }\n  ],\n  \"questions\": [\n    {\n      \"id\": \"cf01_q01_longitudinal\",\n      \"checkpoint_after\": 10,\n      \"type\": \"longitudinal\",\n      \"prompt\": \"Based on the logs so far, are there any concerning patterns in service health?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Geo-lookup latency has been gradually increasing. Service-B retry rates are slightly elevated. No user-facing impact yet but worth monitoring.\",\n        \"key_facts\": [\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"latency_progressive_degradation\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"early_signal:2\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q02_null\",\n      \"checkpoint_after\": 10,\n      \"type\": \"null_hypothesis\",\n      \"prompt\": \"What were the key operational events on January 20th?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"A routine day of operations as documented in the daily log summary.\",\n        \"key_facts\": [],\n        \"evidence\": [\n          \"baseline:6\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q03_longitudinal\",\n      \"checkpoint_after\": 25,\n      \"type\": \"longitudinal\",\n      \"prompt\": \"What is the root cause of the checkout failures?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Connection pool exhaustion caused by service-B fraud-check retries against a degrading third-party geo-lookup API. The service-C deploy was a red herring.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"deploy_red_herring\",\n          \"dns_not_root_cause\",\n          \"storage_systems_normal\",\n          \"auth_systems_normal\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"escalation:3\",\n          \"root_cause:2\",\n          \"red_herring:2\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q04_action\",\n      \"checkpoint_after\": 30,\n      \"type\": \"action_recommendation\",\n      \"prompt\": \"What actions should the team take to prevent this from recurring?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Separate connection pools for fraud checks and checkout. Add circuit breaker on geo-lookup calls. Monitor third-party API latency with alerting.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"latency_progressive_degradation\",\n          \"dns_not_root_cause\",\n          \"storage_systems_normal\",\n          \"auth_systems_normal\"\n        ],\n        \"evidence\": [\n          \"escalation:3\",\n          \"root_cause:2\",\n          \"root_cause:5\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q05_negative\",\n      \"checkpoint_after\": 15,\n      \"type\": \"negative\",\n      \"prompt\": \"Is there evidence of DNS infrastructure failure in the logs?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. DNS metrics are stable throughout. The issue is geo-lookup API latency degradation and connection pool exhaustion, not DNS infrastructure failure.\",\n        \"key_facts\": [\n          \"dns_not_root_cause\",\n          \"geo_latency_degradation\"\n        ],\n        \"evidence\": [\n          \"baseline:3\",\n          \"early_signal:3\"\n        ]\n      },\n      \"expected_answer_polarity\": \"negative\"\n    },\n    {\n      \"id\": \"cf01_q06_paraphrase\",\n      \"checkpoint_after\": 25,\n      \"type\": \"paraphrase\",\n      \"prompt\": \"Why are checkout requests failing?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Connection pool exhaustion caused by service-B fraud-check retries against a degrading third-party geo-lookup API. The service-C deploy was a red herring.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"deploy_red_herring\",\n          \"storage_systems_normal\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"escalation:3\",\n          \"root_cause:2\",\n          \"red_herring:2\"\n        ]\n      },\n      \"variant_of\": \"cf01_q03_longitudinal\"\n    },\n    {\n      \"id\": \"cf01_q07_temporal\",\n      \"checkpoint_after\": 25,\n      \"type\": \"temporal\",\n      \"prompt\": \"When did geo-lookup latency first start degrading, and over how many reporting periods did it progress to the current level?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Geo-lookup latency began degrading during the early signal phase, starting around episode 9. It progressed over approximately 15-20 reporting periods from 200ms baseline to 800ms+, accelerating during the escalation phase.\",\n        \"key_facts\": [\n          \"geo_latency_degradation\",\n          \"pool_exhaustion\",\n          \"latency_progressive_degradation\",\n          \"dns_not_root_cause\",\n          \"auth_systems_normal\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"early_signal:4\",\n          \"escalation:2\",\n          \"root_cause:3\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q08_counterfactual\",\n      \"checkpoint_after\": 25,\n      \"type\": \"counterfactual\",\n      \"prompt\": \"If the service-C deployment were responsible for the checkout failures, what pattern would you expect in the data? Does that match what you see?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"If the service-C deploy caused the failures, you would expect a sudden onset coinciding with the deploy and resolution after rollback. Instead, the degradation was gradual (geo-lookup latency creeping over weeks) and the rollback did not resolve the issue. The actual cause is upstream geo-lookup API degradation driving retry storms and pool exhaustion.\",\n        \"key_facts\": [\n          \"deploy_red_herring\",\n          \"geo_latency_degradation\",\n          \"pool_exhaustion\",\n          \"latency_progressive_degradation\",\n          \"auth_systems_normal\"\n        ],\n        \"evidence\": [\n          \"red_herring:2\",\n          \"escalation:1\",\n          \"root_cause:2\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q09_longitudinal\",\n      \"checkpoint_after\": 5,\n      \"type\": \"longitudinal\",\n      \"prompt\": \"Based on the first few days of logs, are there any service health patterns emerging?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No significant patterns are visible yet. All services are operating within baseline parameters. Metrics are consistent with normal operations.\",\n        \"key_facts\": [],\n        \"evidence\": [\n          \"baseline:3\",\n          \"baseline:5\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q10_evidence\",\n      \"checkpoint_after\": 5,\n      \"type\": \"evidence_sufficiency\",\n      \"prompt\": \"Do you have sufficient data to identify any concerning service trends?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. Only 5 days of baseline data are available, which is insufficient to establish trends or identify anomalies. More observation time is needed.\",\n        \"key_facts\": [],\n        \"evidence\": [\n          \"baseline:3\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q11_longitudinal\",\n      \"checkpoint_after\": 20,\n      \"type\": \"longitudinal\",\n      \"prompt\": \"What is the current impact on user-facing services and what appears to be driving it?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Checkout failures are increasing. The root cause appears to be connection pool exhaustion driven by service-B retry storms against the degrading geo-lookup API. The service-C rollback did not resolve the issue.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"deploy_red_herring\"\n        ],\n        \"evidence\": [\n          \"escalation:1\",\n          \"escalation:3\",\n          \"escalation:5\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q12_severity\",\n      \"checkpoint_after\": 20,\n      \"type\": \"severity_assessment\",\n      \"prompt\": \"How would you assess the severity of the current service degradation?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"High severity. Checkout failures are directly impacting revenue. The issue is escalating \\u2014 geo-lookup latency continues to worsen, retry storms are amplifying the problem, and connection pool exhaustion is spreading to affect other services sharing the pool.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"service_b_retries\"\n        ],\n        \"evidence\": [\n          \"escalation:3\",\n          \"escalation:5\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q13_paraphrase\",\n      \"checkpoint_after\": 10,\n      \"type\": \"paraphrase\",\n      \"prompt\": \"Are any upstream dependencies showing degraded performance?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Yes. Geo-lookup latency has been gradually increasing. Service-B retry rates are slightly elevated. No user-facing impact yet but worth monitoring.\",\n        \"key_facts\": [\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"latency_progressive_degradation\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"early_signal:2\"\n        ]\n      },\n      \"variant_of\": \"cf01_q01_longitudinal\"\n    },\n    {\n      \"id\": \"cf01_q14_paraphrase\",\n      \"checkpoint_after\": 25,\n      \"type\": \"paraphrase\",\n      \"prompt\": \"What sequence of events led to the checkout outage?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Geo-lookup API latency degradation caused service-B fraud-check retries to escalate, which exhausted the shared connection pool, leading to checkout failures. The service-C deploy was a coincidence, not the cause.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"service_b_retries\",\n          \"deploy_red_herring\",\n          \"dns_not_root_cause\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"escalation:3\",\n          \"root_cause:2\",\n          \"red_herring:2\"\n        ]\n      },\n      \"variant_of\": \"cf01_q03_longitudinal\"\n    },\n    {\n      \"id\": \"cf01_q15_negative\",\n      \"checkpoint_after\": 20,\n      \"type\": \"negative\",\n      \"prompt\": \"Is there evidence of storage capacity issues contributing to the service failures?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. Storage cluster metrics show normal utilization with no capacity or performance issues. The service degradation is caused by connection pool exhaustion from geo-lookup API latency, not storage.\",\n        \"key_facts\": [\n          \"storage_systems_normal\",\n          \"pool_exhaustion\"\n        ],\n        \"evidence\": [\n          \"baseline:4\",\n          \"early_signal:3\"\n        ]\n      },\n      \"expected_answer_polarity\": \"negative\"\n    },\n    {\n      \"id\": \"cf01_q16_negative\",\n      \"checkpoint_after\": 25,\n      \"type\": \"negative\",\n      \"prompt\": \"Are there authentication or SSO failures contributing to the checkout issues?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. Authentication systems show normal operation with no anomalies. The checkout failures are caused by connection pool exhaustion from geo-lookup API latency degradation.\",\n        \"key_facts\": [\n          \"auth_systems_normal\",\n          \"pool_exhaustion\",\n          \"storage_systems_normal\",\n          \"dns_not_root_cause\"\n        ],\n        \"evidence\": [\n          \"baseline:5\",\n          \"early_signal:4\"\n        ]\n      },\n      \"expected_answer_polarity\": \"negative\"\n    },\n    {\n      \"id\": \"cf01_q17_temporal\",\n      \"checkpoint_after\": 20,\n      \"type\": \"temporal\",\n      \"prompt\": \"How quickly is the checkout error rate escalating?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"The checkout error rate accelerated significantly during the escalation phase. Initially minimal during early signal detection, it ramped up rapidly as connection pool exhaustion worsened, roughly doubling every few reporting periods.\",\n        \"key_facts\": [\n          \"pool_exhaustion\",\n          \"geo_latency_degradation\",\n          \"latency_progressive_degradation\"\n        ],\n        \"evidence\": [\n          \"escalation:1\",\n          \"escalation:3\",\n          \"escalation:5\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q18_temporal\",\n      \"checkpoint_after\": 15,\n      \"type\": \"temporal\",\n      \"prompt\": \"When did service-B retry rates first show elevation above baseline?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Service-B retry rates first showed elevation during the early signal phase, starting around episode 10, coinciding with the initial geo-lookup latency degradation.\",\n        \"key_facts\": [\n          \"service_b_retries\",\n          \"geo_latency_degradation\",\n          \"auth_systems_normal\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"early_signal:2\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q19_distractor\",\n      \"checkpoint_after\": 15,\n      \"type\": \"distractor_resistance\",\n      \"prompt\": \"Are there any DNS zone transfer or propagation issues affecting service resolution?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. DNS infrastructure is operating normally with no zone transfer failures or propagation delays. The service degradation involves the geo-lookup API latency, which is a third-party API call, not a DNS infrastructure issue.\",\n        \"key_facts\": [\n          \"dns_not_root_cause\",\n          \"geo_latency_degradation\"\n        ],\n        \"evidence\": [\n          \"baseline:3\",\n          \"early_signal:1\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q20_distractor\",\n      \"checkpoint_after\": 25,\n      \"type\": \"distractor_resistance\",\n      \"prompt\": \"Is storage rebalancing or compaction activity correlating with the service latency spikes?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. Storage operations are running normally. The latency spikes are caused by the geo-lookup API degradation and resulting connection pool exhaustion, not storage-related activity.\",\n        \"key_facts\": [\n          \"storage_systems_normal\",\n          \"geo_latency_degradation\",\n          \"pool_exhaustion\"\n        ],\n        \"evidence\": [\n          \"baseline:4\",\n          \"escalation:3\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q21_distractor\",\n      \"checkpoint_after\": 20,\n      \"type\": \"distractor_resistance\",\n      \"prompt\": \"Are IAM policy changes or token rotation events related to the service disruption?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"No. IAM operations are routine. The service failures are caused by connection pool exhaustion from geo-lookup API degradation, not authentication or access control issues.\",\n        \"key_facts\": [\n          \"auth_systems_normal\",\n          \"pool_exhaustion\",\n          \"storage_systems_normal\",\n          \"dns_not_root_cause\"\n        ],\n        \"evidence\": [\n          \"baseline:5\",\n          \"escalation:1\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q22_counterfactual\",\n      \"checkpoint_after\": 20,\n      \"type\": \"counterfactual\",\n      \"prompt\": \"If the checkout failures were caused by a spike in traffic volume, what pattern would you expect? Does the data match?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"A traffic volume spike would show increased hit counts across all endpoints proportionally, with latency degradation correlated to load. Instead, the data shows selective latency degradation in geo-lookup calls with normal traffic volumes, indicating a dependency failure rather than a capacity issue.\",\n        \"key_facts\": [\n          \"geo_latency_degradation\",\n          \"pool_exhaustion\",\n          \"latency_progressive_degradation\"\n        ],\n        \"evidence\": [\n          \"escalation:1\",\n          \"escalation:3\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q23_null\",\n      \"checkpoint_after\": 20,\n      \"type\": \"null_hypothesis\",\n      \"prompt\": \"What was the CDN cache hit rate reported on February 1st?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"The CDN cache hit rate as documented in the daily API gateway log summary.\",\n        \"key_facts\": [],\n        \"evidence\": [\n          \"escalation:3\"\n        ]\n      }\n    },\n    {\n      \"id\": \"cf01_q24_evidence\",\n      \"checkpoint_after\": 15,\n      \"type\": \"evidence_sufficiency\",\n      \"prompt\": \"Is there sufficient evidence to begin investigating a specific root cause for the emerging service patterns?\",\n      \"ground_truth\": {\n        \"canonical_answer\": \"Yes. The correlation between geo-lookup latency increase and service-B retry elevation is visible across multiple reporting periods. While the full causal chain is not yet clear, there is enough evidence to investigate the geo-lookup dependency.\",\n        \"key_facts\": [\n          \"geo_latency_degradation\",\n          \"service_b_retries\"\n        ],\n        \"evidence\": [\n          \"early_signal:1\",\n          \"early_signal:2\",\n          \"early_signal:4\"\n        ]\n      }\n    }\n  ],\n  \"_spec_hash\": \"sha256:3f6f65d0167c1fdb1bae6d27ac958430756a890c10718761fd3bc217cb3f6284\"\n}",
  "metadata": {
    "scope_id": "cascading_failure_01",
    "domain": "system_logs",
    "layer_name": "spec",
    "layer_level": 0
  }
}