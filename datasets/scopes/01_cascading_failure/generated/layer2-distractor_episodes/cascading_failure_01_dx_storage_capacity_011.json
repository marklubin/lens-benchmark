{
  "label": "cascading_failure_01_dx_storage_capacity_011",
  "artifact_type": "distractor_episode",
  "artifact_id": "sha256:0534a0c42a70f62fc12bd436afb9ca3a1dedf6107c4d5cdd81ffc82ed0745b41",
  "input_ids": [
    "sha256:00449ed9d7f8a3240548c2009abc6e3ccafc743718f92388a22452f2b9699234",
    "sha256:8bd255056921270a7048555e132196af832705b4e437017f1abea5378ca40697"
  ],
  "prompt_id": null,
  "model_config": null,
  "created_at": "2026-02-17T11:57:43.363477",
  "content": "## 2026-01-12 Daily Operations Summary\n\n### Metrics\n- Cluster Capacity\n  - Raw TB Total: 9820\n  - Raw TB Used: 7918\n  - Raw TB Free: 1902\n  - Effective TB Total: 6540\n  - Effective TB Used: 5656\n  - Effective TB Free: 884\n  - Overall Utilization Percentage: 80.6\n- Node Utilization\n  - Nodes Total: 120\n  - Nodes Over 80 Percent: 64\n  - Nodes Over 90 Percent: 23\n  - Hottest Node Utilization Percentage: 98.8\n  - Coldest Node Utilization Percentage: 47.8\n  - Utilization Standard Deviation Percentage: 13.7\n- Rebalance\n  - Rebalance Jobs Started: 9\n  - Rebalance Jobs Completed: 7\n  - Data Moved TB: 268\n  - Average Rebalance Megabits Per Second: 810\n  - Rebalance Queue Depth: 5\n  - Skew Index: 1.1\n- Compaction\n  - Compactions Run: 1902\n  - Compaction CPU Hours: 128.8\n  - Write Amplification: 1.56\n  - SSTables Compacted: 28640\n  - Pending Compactions: 98\n  - Average Compaction Megabytes Per Second: 160\n- IO\n  - Read IOPS P95: 220000\n  - Write IOPS P95: 118000\n  - Read Megabytes Per Second P95: 3700\n  - Write Megabytes Per Second P95: 1780\n  - Disk Busy Percentage P95: 91.7\n  - Fsync Milliseconds P95: 14.1\n- Storage Efficiency\n  - Replication Factor: 3\n  - Erasure Coding Percentage: 26\n  - Compression Ratio: 1.79\n  - Deduplication Ratio: 1.07\n  - Garbage Percentage: 7.3\n  - Tombstone Percentage: 4.1\n- Backup Verification\n  - Snapshots Taken: 48\n  - Snapshots Verified: 48\n  - Verify Failures: 0\n  - Average Verify Minutes: 37\n  - Restore Drills: 0\n  - Last Restore RTO Minutes: 47\n- Quota Enforcement\n  - Tenants Total: 88\n  - Tenants Over Quota: 5\n  - Quota Soft Limit Hits: 26\n  - Quota Hard Blocks: 4\n  - Largest Tenant Used TB: 461\n  - New Quota Requests: 6\n- Ingest\n  - Ingest TB: 212\n  - Egress TB: 122\n  - Object Count Delta Millions: 74\n  - Average Object Kilobytes: 88\n  - Hot Partition Count: 12\n- Reliability\n  - Disk Failures: 1\n  - Disk Rebuilds Completed: 0\n  - Disk Rebuild Average Hours: 0\n  - Node Restarts: 0\n  - Unhealthy OSDs: 4\n  - Scrub Errors: 29\n- Metadata\n  - Metadata DB Size Gigabytes: 1096\n  - Metadata QPS P95: 71000\n  - Metadata Cache Hit Percentage: 89.2\n  - Manifest Backlog: 3900\n  - Namespace Count: 326\n- Maintenance\n  - Firmware Updates: 0\n  - Kernel Patches: 0\n  - Rack Power Cycles: 0\n  - Network Drops: 21\n  - Audit Log Gigabytes: 89\n\n### Infrastructure\n- stor-a-02\n  - CPU Percentage: 84\n  - Memory Percentage: 84\n  - Disk Used Percentage: 98\n  - Disk Busy Percentage: 96\n  - Network In Gigabits Per Second: 9.1\n  - Network Out Gigabits Per Second: 7.9\n  - IO Wait Percentage: 12.8\n- stor-b-10\n  - CPU Percentage: 68\n  - Memory Percentage: 74\n  - Disk Used Percentage: 95\n  - Disk Busy Percentage: 91\n  - Network In Gigabits Per Second: 7.1\n  - Network Out Gigabits Per Second: 6.4\n  - IO Wait Percentage: 9.2\n- stor-c-18\n  - CPU Percentage: 61\n  - Memory Percentage: 71\n  - Disk Used Percentage: 92\n  - Disk Busy Percentage: 85\n  - Network In Gigabits Per Second: 5.9\n  - Network Out Gigabits Per Second: 5.3\n  - IO Wait Percentage: 6.8\n- meta-01\n  - CPU Percentage: 66\n  - Memory Percentage: 89\n  - Disk Used Percentage: 83\n  - Disk Busy Percentage: 69\n  - Network In Gigabits Per Second: 3.3\n  - Network Out Gigabits Per Second: 2.8\n  - IO Wait Percentage: 4.7\n\n### Events\n- Large batch ingest (+212 TB) drove overall utilization above 80%; initiated emergency capacity review and accelerated procurement timeline.\n- Disk in stor-b-10 reported predictive failure; drained shard set and scheduled replacement window with on-site tech.\n- Quota hard blocks for four tenants; implemented temporary per-tenant ingest rate limits and requested teams to shift older data to erasure-coded pool.\n- Compaction pending approached 100; tuned compaction concurrency down to reduce disk busy saturation and queued overnight catch-up run.\n\n### On Call\n- Shift: Avery C. \n- Pages: 7 \n- Tickets: 14 \n- Status: Degraded; high utilization and heavy ingest causing compaction pressure and quota incidents. Hardware replacement scheduled and rate limits applied.",
  "metadata": {
    "episode_type": "distractor",
    "theme": "storage_capacity",
    "theme_index": 1,
    "local_index": 11,
    "scope_id": "cascading_failure_01",
    "layer_name": "distractor_episodes",
    "layer_level": 2,
    "build_fingerprint": {
      "scheme": "synix:build:v1",
      "digest": "627244d584ba0fb65fbf1c2b4c8f282daab56da1cbe5ba1c29eb255b503cfdee",
      "components": {
        "transform": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
        "inputs": "c50e2b4719758f72"
      }
    },
    "transform_fingerprint": {
      "scheme": "synix:transform:v2",
      "digest": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
      "components": {
        "transform_id": "01137a4997b78f86",
        "source": "c42d9c6282d5331f",
        "model": "1e2ccf75e96d7ee7"
      }
    }
  }
}