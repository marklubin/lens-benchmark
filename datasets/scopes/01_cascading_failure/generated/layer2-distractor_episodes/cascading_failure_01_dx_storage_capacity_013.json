{
  "label": "cascading_failure_01_dx_storage_capacity_013",
  "artifact_type": "distractor_episode",
  "artifact_id": "sha256:0042969a1893c4d5db726640923e9e14cdf3814fef3ab15cf7defc433c35a3f5",
  "input_ids": [
    "sha256:6cb28686746e7839fd3bed94bcd50530c285310fd7a053289c49cbe7c08d55ba",
    "sha256:8bd255056921270a7048555e132196af832705b4e437017f1abea5378ca40697"
  ],
  "prompt_id": null,
  "model_config": null,
  "created_at": "2026-02-16T03:35:01.520537",
  "content": "## 2026-01-14 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS 120 req | p50: 15ms p95: 45ms p99: 78ms | err: 0.0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS 120 req | p50: 10ms p95: 30ms p99: 55ms | err: 0.0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS 120 req | p50: 20ms p95: 60ms p99: 90ms | err: 0.0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS 120 req | p50: 12ms p95: 35ms p99: 65ms | err: 0.0% (0 errors) | success: 100%\n- /storage/io: REQUESTS 120 req | p50: 18ms p95: 50ms p99: 80ms | err: 0.0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS 120 req | p50: 14ms p95: 40ms p99: 70ms | err: 0.0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS 120 req | p50: 16ms p95: 42ms p99: 72ms | err: 0.0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS 120 req | p50: 11ms p95: 33ms p99: 60ms | err: 0.0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS 120 req | p50: 13ms p95: 38ms p99: 68ms | err: 0.0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS 120 req | p50: 17ms p95: 48ms p99: 75ms | err: 0.0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS 120 req | p50: 19ms p95: 52ms p99: 80ms | err: 0.0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS 120 req | p50: 9ms p95: 28ms p99: 50ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-02: CPU 86% | Mem 85% | Disk Used 99% | Conns: 120 | Net: 9.4/8.2 Mbps\n- stor-b-05: CPU 70% | Mem 76% | Disk Used 95% | Conns: 98 | Net: 7.3/6.7 Mbps\n- stor-c-21: CPU 63% | Mem 72% | Disk Used 93% | Conns: 85 | Net: 6.1/5.5 Mbps\n- meta-03: CPU 68% | Mem 90% | Disk Used 84% | Conns: 75 | Net: 3.5/2.9 Mbps\n\n### Connection Pools\n- pool-storage: active 68 | idle 22 | waiting 4 | exhaustion 2 | max 100 | avg_wait 15ms\n- pool-metadata: active 15 | idle 10 | waiting 1 | exhaustion 0 | max 30 | avg_wait 8ms\n- pool-rebalance: active 9 | idle 3 | waiting 2 | exhaustion 1 | max 15 | avg_wait 20ms\n- pool-io: active 20 | idle 5 | waiting 0 | exhaustion 0 | max 25 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 2.3 Gbps | Origin requests: 124,567\n\n### Alerts\n- [CRITICAL] Hottest node hit 99.0% used; initiated emergency hot-spot evacuation rebalance and placed temporary write sheds for two test namespaces\n- [WARNING] Scrub errors climbed to 31; identified correlation with high tombstone percentage in one tenant's workload and scheduled compaction priority for those keyspaces\n- [MAJOR] Quota hard blocks rose to 4; enabled daily quota digest and required tenant owners to acknowledge cleanup SLAs\n- [INFO] Metadata cache hit dipped below 89%; increased cache size on metadata nodes and reduced manifest backlog via batching\n\n### Deployments & Changes\n- Initiated emergency hot-spot evacuation rebalance on hottest node; placed temporary write sheds for two test namespaces\n- Scheduled priority compaction for tenant keyspaces with high tombstone counts\n- Enabled daily quota digest; notified tenant owners to acknowledge cleanup SLAs\n- Increased metadata cache size; reduced manifest backlog through batching\n\n### Events\n- Hottest node hit 99.0% used; initiated emergency hot-spot evacuation rebalance and placed temporary write sheds for two test namespaces\n- Scrub errors climbed to 31; identified correlation with high tombstone percentage in one tenant's workload and scheduled compaction priority for those keyspaces\n- Quota hard blocks rose to 4; enabled daily quota digest and required tenant owners to acknowledge cleanup SLAs\n- Metadata cache hit dipped below 89%; increased cache size on metadata nodes and reduced manifest backlog via batching\n\n### On-Call\n- shift handoff note: Omar T. 8 pages. 15 tickets. Status: Degraded; capacity critically tight on hottest nodes. Active mitigations: write sheds, targeted rebalance, and compaction prioritization for tombstone-heavy tenants",
  "metadata": {
    "episode_type": "distractor",
    "theme": "storage_capacity",
    "theme_index": 1,
    "local_index": 13,
    "scope_id": "cascading_failure_01",
    "layer_name": "distractor_episodes",
    "layer_level": 2,
    "build_fingerprint": {
      "scheme": "synix:build:v1",
      "digest": "689c0ebfb6278f095d9a8ad2f028d8f17a2bc63888d694033e41477cddc6a8e0",
      "components": {
        "transform": "1db107c0320f12aacae9855d2bf7cfba02d6ff84f017b495964716da701d04b0",
        "inputs": "1a71c817b3637bbe"
      }
    },
    "transform_fingerprint": {
      "scheme": "synix:transform:v2",
      "digest": "1db107c0320f12aacae9855d2bf7cfba02d6ff84f017b495964716da701d04b0",
      "components": {
        "transform_id": "01137a4997b78f86",
        "source": "c42d9c6282d5331f",
        "model": "478abb44e4d92bdf"
      }
    }
  }
}