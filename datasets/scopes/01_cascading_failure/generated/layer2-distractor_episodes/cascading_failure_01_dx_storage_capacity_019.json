{
  "label": "cascading_failure_01_dx_storage_capacity_019",
  "artifact_type": "distractor_episode",
  "artifact_id": "sha256:18f2e61aafc62418b2dae55c06c732ba6e19e689d41531291db8a2c48af19774",
  "input_ids": [
    "sha256:00449ed9d7f8a3240548c2009abc6e3ccafc743718f92388a22452f2b9699234",
    "sha256:8bd255056921270a7048555e132196af832705b4e437017f1abea5378ca40697"
  ],
  "prompt_id": null,
  "model_config": null,
  "created_at": "2026-02-17T11:58:06.544636",
  "content": "## 2026-01-20 Daily Operations Summary\n\n### Metrics\n#### Cluster Capacity\n- Raw TB Total: 9820\n- Raw TB Used: 8480\n- Raw TB Free: 1340\n- Effective TB Total: 6540\n- Effective TB Used: 6038\n- Effective TB Free: 502\n- Overall Utilization Percentage: 86.4\n\n#### Node Utilization\n- Nodes Total: 120\n- Nodes Over 80%: 78\n- Nodes Over 90%: 31\n- Hottest Node Utilization Percentage: 98.9\n- Coldest Node Utilization Percentage: 52.3\n- Utilization Standard Deviation Percentage: 13.5\n\n#### Rebalance\n- Rebalance Jobs Started: 12\n- Rebalance Jobs Completed: 10\n- Data Moved TB: 350\n- Average Rebalance MBps: 920\n- Rebalance Queue Depth: 5\n- Skew Index: 1.06\n\n#### Compaction\n- Compactions Run: 2408\n- Compaction CPU Hours: 160.8\n- Write Amplification: 1.6\n- SSTables Compacted: 36180\n- Pending Compactions: 118\n- Average Compaction MB/s: 156\n\n#### IO\n- Read IOPS P95: 234000\n- Write IOPS P95: 128000\n- Read MB/s P95: 3920\n- Write MB/s P95: 1940\n- Disk Busy Percentage P95: 93.2\n- Fsync P95 ms: 15.6\n\n#### Storage Efficiency\n- Replication Factor: 3\n- Erasure Coding Percentage: 30\n- Compression Ratio: 1.75\n- Deduplication Ratio: 1.04\n- Garbage Percentage: 7.9\n- Tombstone Percentage: 4.8\n\n#### Backup Verification\n- Snapshots Taken: 48\n- Snapshots Verified: 47\n- Verify Failures: 1\n- Average Verify Minutes: 39\n- Restore Drills: 0\n- Last Restore RTO Minutes: 50\n\n#### Quota Enforcement\n- Tenants Total: 90\n- Tenants Over Quota: 6\n- Quota Soft Limit Hits: 31\n- Quota Hard Blocks: 5\n- Largest Tenant Used TB: 494\n- New Quota Requests: 4\n\n#### Ingest\n- Ingest TB: 232\n- Egress TB: 132\n- Object Count Delta M: 83\n- Average Object KB: 87\n- Hot Partition Count: 15\n\n#### Reliability\n- Disk Failures: 0\n- Disk Rebuilds Completed: 0\n- Disk Rebuild Average Hours: 0\n- Node Restarts: 0\n- Unhealthy OSDs: 4\n- Scrub Errors: 38\n\n#### Metadata\n- Metadata DB Size GB: 1201\n- Metadata QPS P95: 78000\n- Metadata Cache Hit Percentage: 88.0\n- Manifest Backlog: 5200\n- Namespace Count: 338\n\n#### Maintenance\n- Firmware Updates: 0\n- Kernel Patches: 0\n- Rack Power Cycles: 0\n- Network Drops: 27\n- Audit Log GB: 98\n\n### Infrastructure\n#### stor-a-20\n- CPU Percentage: 90\n- Memory Percentage: 87\n- Disk Used Percentage: 98\n- Disk Busy Percentage: 98\n- Net In Gbps: 10.2\n- Net Out Gbps: 8.9\n- IO Wait Percentage: 15.3\n\n#### stor-b-13\n- CPU Percentage: 72\n- Memory Percentage: 77\n- Disk Used Percentage: 96\n- Disk Busy Percentage: 93\n- Net In Gbps: 7.6\n- Net Out Gbps: 6.9\n- IO Wait Percentage: 10.1\n\n#### stor-c-01\n- CPU Percentage: 65\n- Memory Percentage: 73\n- Disk Used Percentage: 94\n- Disk Busy Percentage: 87\n- Net In Gbps: 6.4\n- Net Out Gbps: 5.8\n- IO Wait Percentage: 7.6\n\n#### meta-03\n- CPU Percentage: 74\n- Memory Percentage: 93\n- Disk Used Percentage: 87\n- Disk Busy Percentage: 76\n- Net In Gbps: 4.0\n- Net Out Gbps: 3.4\n- IO Wait Percentage: 6.1\n\n### Events\n- Heavy ingest (+232 TB) pushed utilization to 86% and increased quota hard blocks to 5; enforced stricter per-tenant ingest caps for non-production namespaces.\n- Scrub errors rose to 38; prioritized deep scrubs on placement groups with recent high churn and scheduled follow-up replica comparisons.\n- Manifest backlog climbed to 5.2k alongside metadata QPS; added temporary metadata throttling for bulk namespace operations.\n- Snapshot verification had one failure due to verifier queue backlog; increased verifier concurrency and re-verified successfully.\n\n### On Call\n- Shift: Paige R. \n- Pages: 10 \n- Tickets: 18 \n- Status: Degraded; utilization and tenant growth are driving repeated quota and metadata backlogs. Immediate controls: ingest caps, metadata throttling, and targeted scrubs.",
  "metadata": {
    "episode_type": "distractor",
    "theme": "storage_capacity",
    "theme_index": 1,
    "local_index": 19,
    "scope_id": "cascading_failure_01",
    "layer_name": "distractor_episodes",
    "layer_level": 2,
    "build_fingerprint": {
      "scheme": "synix:build:v1",
      "digest": "627244d584ba0fb65fbf1c2b4c8f282daab56da1cbe5ba1c29eb255b503cfdee",
      "components": {
        "transform": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
        "inputs": "c50e2b4719758f72"
      }
    },
    "transform_fingerprint": {
      "scheme": "synix:transform:v2",
      "digest": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
      "components": {
        "transform_id": "01137a4997b78f86",
        "source": "c42d9c6282d5331f",
        "model": "1e2ccf75e96d7ee7"
      }
    }
  }
}