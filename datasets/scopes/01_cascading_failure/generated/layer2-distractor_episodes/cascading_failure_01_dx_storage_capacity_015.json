{
  "label": "cascading_failure_01_dx_storage_capacity_015",
  "artifact_type": "distractor_episode",
  "artifact_id": "sha256:fb589c9efc6deb8c54abae4bd04e1b80ffe21811ac83db0f61d6bc65dcbcc4ca",
  "input_ids": [
    "sha256:00449ed9d7f8a3240548c2009abc6e3ccafc743718f92388a22452f2b9699234",
    "sha256:8bd255056921270a7048555e132196af832705b4e437017f1abea5378ca40697"
  ],
  "prompt_id": null,
  "model_config": null,
  "created_at": "2026-02-17T11:57:52.262484",
  "content": "## 2026-01-16 Daily Operations Summary\n\n### Metrics\n- Cluster Capacity\n  - Raw TB Total: 9820\n  - Raw TB Used: 8204\n  - Raw TB Free: 1616\n  - Effective TB Total: 6540\n  - Effective TB Used: 5848\n  - Effective TB Free: 692\n  - Overall Utilization Percentage: 83.6\n- Node Utilization\n  - Nodes Total: 120\n  - Nodes Over 80%: 72\n  - Nodes Over 90%: 27\n  - Hottest Node Utilization Percentage: 98.9\n  - Coldest Node Utilization Percentage: 50.1\n  - Utilization Standard Deviation Percentage: 13.5\n- Rebalance\n  - Rebalance Jobs Started: 10\n  - Rebalance Jobs Completed: 9\n  - Data Moved TB: 318\n  - Average Rebalance MBps: 880\n  - Rebalance Queue Depth: 5\n  - Skew Index: 1.08\n- Compaction\n  - Compactions Run: 2166\n  - Compaction CPU Hours: 144.8\n  - Write Amplification: 1.57\n  - SSTables Compacted: 32550\n  - Pending Compactions: 102\n  - Average Compaction MB/s: 160\n- IO\n  - Read IOPS P95: 226000\n  - Write IOPS P95: 123000\n  - Read MB/s P95: 3800\n  - Write MB/s P95: 1860\n  - Disk Busy P95 Percentage: 92.0\n  - Fsync P95 ms: 14.5\n- Storage Efficiency\n  - Replication Factor: 3\n  - Erasure Coding Percentage: 28\n  - Compression Ratio: 1.77\n  - Deduplication Ratio: 1.05\n  - Garbage Percentage: 7.6\n  - Tombstone Percentage: 4.4\n- Backup Verification\n  - Snapshots Taken: 48\n  - Snapshots Verified: 46\n  - Verify Failures: 2\n  - Average Verify Minutes: 39\n  - Restore Drills: 0\n  - Last Restore RTO Minutes: 47\n- Quota Enforcement\n  - Tenants Total: 89\n  - Tenants Over Quota: 6\n  - Quota Soft Limit Hits: 29\n  - Quota Hard Blocks: 5\n  - Largest Tenant Used TB: 478\n  - New Quota Requests: 3\n- Ingest\n  - Ingest TB: 224\n  - Egress TB: 127\n  - Object Count Delta M: 80\n  - Average Object KB: 87\n  - Hot Partition Count: 14\n- Reliability\n  - Disk Failures: 0\n  - Disk Rebuilds Completed: 1\n  - Disk Rebuild Average Hours: 7.9\n  - Node Restarts: 0\n  - Unhealthy OSDs: 4\n  - Scrub Errors: 33\n- Metadata\n  - Metadata DB Size GB: 1148\n  - Metadata QPS P95: 74000\n  - Metadata Cache Hit Percentage: 88.4\n  - Manifest Backlog: 4500\n  - Namespace Count: 332\n- Maintenance\n  - Firmware Updates: 0\n  - Kernel Patches: 0\n  - Rack Power Cycles: 0\n  - Network Drops: 26\n  - Audit Log GB: 95\n\n### Infrastructure\n- stor-b-03\n  - CPU Percentage: 88\n  - Memory Percentage: 86\n  - Disk Used Percentage: 98\n  - Disk Busy Percentage: 97\n  - Network In Gbps: 9.6\n  - Network Out Gbps: 8.4\n  - IO Wait Percentage: 14.2\n- stor-a-14\n  - CPU Percentage: 72\n  - Memory Percentage: 79\n  - Disk Used Percentage: 96\n  - Disk Busy Percentage: 90\n  - Network In Gbps: 7.5\n  - Network Out Gbps: 6.8\n  - IO Wait Percentage: 9.8\n- stor-c-06\n  - CPU Percentage: 64\n  - Memory Percentage: 73\n  - Disk Used Percentage: 94\n  - Disk Busy Percentage: 88\n  - Network In Gbps: 6.5\n  - Network Out Gbps: 5.9\n  - IO Wait Percentage: 8.1\n- meta-02\n  - CPU Percentage: 70\n  - Memory Percentage: 91\n  - Disk Used Percentage: 85\n  - Disk Busy Percentage: 73\n  - Network In Gbps: 3.7\n  - Network Out Gbps: 3.0\n  - IO Wait Percentage: 5.6\n\n### Events\n- Very heavy ingest (+224 TB) caused quota hard blocks to jump to 5; instituted temporary hard cap for two experimental tenants and escalated cleanup.\n- Two snapshot verification failures due to verifier worker saturation and missing time window; expanded verifier worker pool and reran verification.\n- Completed one rebuild triggered by earlier pre-fail; rebuild took 7.9 hours and increased compaction pending due to backfill I/O.\n- Metadata manifest backlog grew to 4.5k; enabled batching and increased metadata DB write buffer during off-peak.\n\n### On Call\n- Shift: Riley J.\n- Pages: 9\n- Tickets: 17\n- Status: Degraded\n- Primary Issues: Quota incidents and verification failures under load.\n- Capacity Status: Tight\n- Mitigations Applied: Caps, verifier pool expansion, batching.",
  "metadata": {
    "episode_type": "distractor",
    "theme": "storage_capacity",
    "theme_index": 1,
    "local_index": 15,
    "scope_id": "cascading_failure_01",
    "layer_name": "distractor_episodes",
    "layer_level": 2,
    "build_fingerprint": {
      "scheme": "synix:build:v1",
      "digest": "627244d584ba0fb65fbf1c2b4c8f282daab56da1cbe5ba1c29eb255b503cfdee",
      "components": {
        "transform": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
        "inputs": "c50e2b4719758f72"
      }
    },
    "transform_fingerprint": {
      "scheme": "synix:transform:v2",
      "digest": "a88665a5359d5ac5d2c858c77a2a19318c209adeea6fae2b838d79e11f6aba2b",
      "components": {
        "transform_id": "01137a4997b78f86",
        "source": "c42d9c6282d5331f",
        "model": "1e2ccf75e96d7ee7"
      }
    }
  }
}