================================================================================
STUDYBUDDY AI TUTORING PLATFORM — SESSION LOG
================================================================================
Session ID    : SB-2025-10-14-7588
Student ID    : dfernandez_2026
Subject       : Data Structures (CS 261)
Session Type  : Exam Review
Duration      : 55 minutes
Timestamp     : 2025-10-14T20:15:00Z
Turns         : 38
================================================================================

[20:15:02] dfernandez_2026: ok so big-O always confuses me. like why do we say O(n^2) and not O(n^2/2 + n/2)?

[20:15:18] StudyBuddy AI Tutor: That's a fundamental question and the answer gets at what Big-O notation is actually measuring. Big-O notation describes the upper bound of growth rate, not the exact count. As n grows very large, the n^2/2 term dominates completely — the n/2 is negligible and the 1/2 constant factor doesn't change the growth shape.

Here's why: when n = 1,000, n^2/2 = 500,000 and n/2 = 500. The lower-order term (n/2) is 0.1% of the total — essentially noise. When n = 1,000,000, n^2/2 = 500,000,000,000 and n/2 = 500,000. The n/2 term is 0.0001% of the total. As n grows, the lower-order terms vanish into irrelevance.

The constant factor (1/2) is dropped because Big-O describes the SHAPE of the growth curve, not its exact position. O(n^2/2) and O(n^2) describe the same parabolic growth pattern — one is just shifted. For asymptotic analysis, that shift doesn't matter. What matters is: does the runtime grow linearly? Quadratically? Logarithmically?

[20:16:00] dfernandez_2026: that makes sense. so O(2n) is just O(n) because the 2 doesnt change the shape of the curve?

[20:16:15] StudyBuddy AI Tutor: Precisely. Whether you do one pass or two passes through the data, it still scales linearly with input size. The constant factor matters for real performance but not for asymptotic classification.

Let me give you a concrete example that's likely to appear on your exam. Analyze the time complexity of this pseudocode:

for i = 0 to n-1:
    for j = i to n-1:
        print(i, j)

What's the Big-O?

[20:16:35] dfernandez_2026: ok so the outer loop runs n times. the inner loop runs... it depends on i. when i = 0, inner runs n times. when i = 1, inner runs n-1 times. when i = n-1, inner runs 1 time. so total iterations = n + (n-1) + (n-2) + ... + 1 = n(n+1)/2. that's O(n^2) because we drop the constant and lower-order terms

[20:17:00] StudyBuddy AI Tutor: Exactly right. The sum n + (n-1) + ... + 1 = n(n+1)/2 = n^2/2 + n/2. Drop the lower-order term (n/2) and the constant (1/2), and you get O(n^2). This is the classic nested loop analysis and it will almost certainly appear on Prof. Kim's midterm.

Now let me test you on a slightly different pattern. What's the time complexity of:

for i = 1 to n:
    for j = 1 to n:
        for k = 1 to n:
            x = x + 1

[20:17:25] dfernandez_2026: three nested loops each running n times. so n * n * n = n^3. O(n^3)

[20:17:38] StudyBuddy AI Tutor: Correct. And what about this one?

for i = n down to 1:
    i = i / 2

[20:17:50] dfernandez_2026: oh thats not a standard for loop. i starts at n and is halved each iteration. so it goes n, n/2, n/4, n/8, ..., 1. that's log base 2 of n iterations. O(log n)

[20:18:05] StudyBuddy AI Tutor: Correct. The key pattern: anytime you see a loop where the variable is being multiplied or divided by a constant each iteration, that's O(log n). This comes up constantly with tree structures — which brings us to the next topic.

[20:18:22] dfernandez_2026: yeah lets do trees. whats the difference between O(n log n) and O(n^2) in practice? like at what point does it actually matter?

[20:18:38] StudyBuddy AI Tutor: The difference becomes dramatic as n grows. Here's a concrete comparison:

n = 100:      n^2 = 10,000        n*log(n) = 664          ratio: 15x
n = 1,000:    n^2 = 1,000,000     n*log(n) = 9,966        ratio: 100x
n = 10,000:   n^2 = 100,000,000   n*log(n) = 132,877      ratio: 753x
n = 1,000,000: n^2 = 10^12        n*log(n) = 19,931,569   ratio: 50,000x

At n = 1,000,000, an O(n^2) algorithm takes 50,000 times longer than an O(n log n) algorithm. That's the difference between finishing in 1 second and taking 14 hours. This is why sorting algorithms matter: O(n log n) sorts (merge sort, heap sort) are practical for large datasets while O(n^2) sorts (bubble sort, insertion sort) are not.

[20:19:15] dfernandez_2026: ok that makes the difference pretty clear. lets talk about binary search trees now. what about worst-case vs average-case height?

[20:19:32] StudyBuddy AI Tutor: For a binary search tree (BST):

Average case (balanced or roughly balanced): Height is O(log n). This means search, insertion, and deletion all take O(log n) time, because each comparison eliminates about half of the remaining nodes.

Worst case (degenerate): Height is O(n). This happens when nodes are inserted in sorted order — each node becomes a child of the previous one, forming a linked list. In a degenerate BST with n nodes, search, insertion, and deletion all take O(n) time.

The average case assumes that the tree is reasonably balanced — meaning the left and right subtrees of each node have roughly similar sizes. But there's no guarantee of this in a plain BST.

[20:20:00] dfernandez_2026: when does a BST degenerate? and how do you prevent it?

[20:20:15] StudyBuddy AI Tutor: A BST degenerates when you insert elements in sorted (or reverse-sorted) order. For example, inserting 1, 2, 3, 4, 5 in that order produces:

1
 \
  2
   \
    3
     \
      4
       \
        5

Every node has only a right child — it's a linked list, not a tree. The height is 5 (= n) instead of the ideal log2(5) = approximately 2.3.

Prevention strategies:
1. AVL trees: Self-balancing BST. After each insertion or deletion, check the "balance factor" (height of left subtree minus height of right subtree) at every ancestor node. If the balance factor exceeds 1 or goes below -1, perform rotations to rebalance. Guarantees O(log n) height.

2. Red-black trees: Another self-balancing BST. Uses a coloring scheme (red and black nodes) with rules that ensure the tree never becomes too unbalanced. Guarantees O(log n) height. Used in Java's TreeMap and C++'s std::map.

3. Randomization: Insert elements in random order. This makes degenerate cases extremely unlikely (though not impossible).

[20:20:55] dfernandez_2026: avl trees are honestly terrifying. do we need to know double rotations too?

[20:21:10] StudyBuddy AI Tutor: Double rotations (left-right and right-left) handle the cases where the imbalance is not on a straight path. They're composed of two single rotations. I can trace through an example if you'd like.

[20:21:22] dfernandez_2026: yeah please. the textbook diagram makes no sense to me

[20:21:38] StudyBuddy AI Tutor: Let me start with a single rotation, then build up to a double rotation.

Single right rotation — inserting 30, 20, 10 into an AVL tree:

After inserting 30: just the root.
    30

After inserting 20: 20 becomes left child of 30.
    30
   /
  20

After inserting 10: 10 becomes left child of 20. Now check balance factors:
    30  (balance factor = 2, LEFT-heavy — imbalanced!)
   /
  20  (balance factor = 1)
 /
10

Node 30 has a balance factor of 2 (left subtree height 2, right subtree height 0). The imbalance is on a straight left-left path: 30 -> 20 -> 10. This requires a single RIGHT rotation at node 30.

The rotation: 20 becomes the new root. 30 becomes the right child of 20. 10 stays as the left child of 20.

  20
 /  \
10   30

All balance factors are now 0. Height went from 3 to 2.

Now let me show a LEFT-RIGHT double rotation. Insert 30, 10, 20:

After inserting 30: root.
    30

After inserting 10: 10 becomes left child.
    30
   /
  10

After inserting 20: 20 becomes right child of 10.
    30  (balance factor = 2, LEFT-heavy)
   /
  10  (balance factor = -1, RIGHT-heavy)
    \
     20

The imbalance is at node 30 (balance factor 2), but the path is LEFT then RIGHT (30 -> 10 -> 20). This is a "zig-zag" pattern. A single rotation won't fix it.

Solution — double rotation:
Step 1: Left rotation at node 10 (straighten the zig-zag into a straight line):
    30
   /
  20
 /
10

Step 2: Right rotation at node 30 (now it's a straight left-left path):
  20
 /  \
10   30

The result is the same balanced tree. The double rotation is just two single rotations composed: first rotate the child to straighten the path, then rotate the grandparent to balance.

[20:22:45] dfernandez_2026: OH. so the double rotation is literally just fixing the zig-zag first so that you can do a normal single rotation. thats way less scary than the textbook made it seem

[20:23:00] StudyBuddy AI Tutor: Exactly. The textbook presents all four rotation cases (LL, RR, LR, RL) as separate procedures, but conceptually there are only two ideas:

1. If the imbalance follows a straight path (LL or RR): single rotation.
2. If the imbalance follows a zig-zag path (LR or RL): straighten first (inner rotation), then balance (outer rotation).

The four cases are just combinations of direction, not fundamentally different operations.

[20:23:20] dfernandez_2026: ok hash tables i actually understand pretty well from the programming assignment. but the exam will probably ask about theoretical load factor stuff right?

[20:23:38] StudyBuddy AI Tutor: Yes, Prof. Kim's exams typically include analysis questions about expected chain length under uniform hashing and performance degradation with increasing load factor. Let me cover the key concepts.

The load factor (alpha) = n / m, where n is the number of elements stored and m is the number of buckets (table size).

For chaining (separate chaining):
- Each bucket is a linked list.
- Expected chain length = alpha (under uniform hashing assumption).
- Expected time for a successful search: O(1 + alpha/2)
- Expected time for an unsuccessful search: O(1 + alpha)
- No hard limit on alpha — you can store more elements than buckets.

For open addressing (linear probing, quadratic probing, double hashing):
- All elements stored in the table itself — no linked lists.
- When a collision occurs, probe for the next empty slot.
- Performance degrades sharply as alpha approaches 1.
- Linear probing suffers from primary clustering — contiguous runs of occupied slots grow, making future collisions more likely.
- Typical resize threshold: alpha = 0.75 (Java's HashMap default).
- alpha must be strictly less than 1 (table must always have at least one empty slot).

[20:24:20] dfernandez_2026: what about the difference between linear probing and quadratic probing and double hashing?

[20:24:38] StudyBuddy AI Tutor: All three are open addressing strategies. The difference is the probing sequence — how you decide which slot to try next after a collision.

Linear probing: Try h(k), h(k)+1, h(k)+2, h(k)+3, ...
- Simple to implement.
- Problem: primary clustering. Long runs of occupied slots form, and any key that hashes into a cluster has to traverse the entire cluster. This makes average search time degrade faster than you'd expect from the load factor alone.

Quadratic probing: Try h(k), h(k)+1^2, h(k)+2^2, h(k)+3^2, ...
- Avoids primary clustering (keys that hash to different initial slots follow different probe sequences).
- Problem: secondary clustering. Keys that hash to the SAME initial slot follow the same probe sequence.
- Not guaranteed to find an empty slot unless the table size is prime and alpha < 0.5.

Double hashing: Try h(k), h(k)+h2(k), h(k)+2*h2(k), h(k)+3*h2(k), ...
- Uses a second hash function h2(k) to determine the step size.
- Avoids both primary and secondary clustering.
- Produces a nearly uniform distribution of probing sequences.
- The best theoretical performance among open addressing methods.
- h2(k) must never evaluate to 0 (infinite loop) and should be coprime to the table size.

[20:25:20] dfernandez_2026: ok now heaps. whats the difference between heap insert and BST insert time complexity?

[20:25:38] StudyBuddy AI Tutor: Both have O(log n) worst-case insertion time, but the average case differs:

BST insert: O(log n) average case in a balanced tree. Worst case O(n) if degenerate. You navigate from root to a leaf position, always going left or right based on comparisons.

Heap insert: O(log n) worst case, but O(1) amortized average case. Here's why: when you insert into a heap, you place the new element at the bottom (the next available position in the complete tree) and "percolate up" — swap with parent if the heap property is violated. Most insertions don't need to percolate far because the probability of needing to go all the way to the root decreases exponentially. On average, an insertion only percolates about 1.6 levels.

[20:26:00] dfernandez_2026: when would you use a heap vs a BST in practice?

[20:26:15] StudyBuddy AI Tutor: The choice depends on what operations you need:

Use a heap (priority queue) when you need:
- Find the minimum (or maximum) element: O(1) — it's always at the root.
- Insert a new element: O(log n) worst case, O(1) amortized.
- Remove the minimum (or maximum): O(log n).
- You DON'T need to search for arbitrary elements, traverse in sorted order, or perform range queries.

Use a BST when you need:
- Search for a specific key: O(log n) if balanced.
- In-order traversal (sorted output): O(n) — just traverse left-root-right.
- Range queries (find all elements between a and b): O(log n + k) where k is the number of results.
- Predecessor/successor queries.

In short: heap is optimized for the "what's the most important item?" question. BST is optimized for "where does this specific item fit in the sorted order?" question.

Real-world example: Dijkstra's shortest path algorithm uses a min-heap as a priority queue. A database index uses a BST (typically a B-tree variant) for range queries.

[20:26:55] dfernandez_2026: ok let me try a tracing problem. can you give me a heap insertion sequence?

[20:27:10] StudyBuddy AI Tutor: Trace the insertion of values 15, 10, 20, 8, 12 into a min-heap, one at a time. Show the heap after each insertion.

[20:27:25] dfernandez_2026: insert 15: just the root.
[15]

insert 10: goes to left child position. 10 < 15, so percolate up. 10 becomes root, 15 becomes left child.
    10
   /
  15

insert 20: goes to right child of 10. 20 > 10, no percolation needed.
    10
   /  \
  15   20

insert 8: goes to left child of 15. 8 < 15, swap. now 8 is at position of 15, 15 is at the leaf. 8 < 10, swap again. 8 becomes root.
      8
    /   \
   10    20
  /
 15

insert 12: goes to right child of 10. 12 > 10, no percolation needed.
      8
    /   \
   10    20
  / \
 15  12

[20:28:05] StudyBuddy AI Tutor: That's a flawless trace. Every insertion is in the correct position, every percolate-up is handled correctly, and the final heap satisfies the min-heap property (every parent is less than or equal to its children). You clearly understand the heap insertion algorithm.

Let me give you one more tracing problem — this time a hash table.

[20:28:22] dfernandez_2026: sure

[20:28:35] StudyBuddy AI Tutor: Given a hash function h(k) = k mod 7 and linear probing, insert the keys 14, 21, 35, 10, 0 into a hash table of size 7. Show the final state of the table.

[20:28:50] dfernandez_2026: ok table has indices 0-6.

insert 14: h(14) = 14 mod 7 = 0. slot 0 is empty. place 14 at index 0.
[0:14] [1: ] [2: ] [3: ] [4: ] [5: ] [6: ]

insert 21: h(21) = 21 mod 7 = 0. slot 0 is occupied (14). linear probe: try slot 1. empty. place 21 at index 1.
[0:14] [1:21] [2: ] [3: ] [4: ] [5: ] [6: ]

insert 35: h(35) = 35 mod 7 = 0. slot 0 occupied, slot 1 occupied. try slot 2. empty. place 35 at index 2.
[0:14] [1:21] [2:35] [3: ] [4: ] [5: ] [6: ]

insert 10: h(10) = 10 mod 7 = 3. slot 3 is empty. place 10 at index 3.
[0:14] [1:21] [2:35] [3:10] [4: ] [5: ] [6: ]

insert 0: h(0) = 0 mod 7 = 0. slot 0 occupied, slot 1 occupied, slot 2 occupied, slot 3 occupied. try slot 4. empty. place 0 at index 4.
[0:14] [1:21] [2:35] [3:10] [4:0] [5: ] [6: ]

three of the five keys (14, 21, 35) all hash to index 0. thats a clear example of primary clustering — they form a contiguous block from 0 to 2, and then 10 and 0 extend the cluster further

[20:29:35] StudyBuddy AI Tutor: Correct trace and excellent observation about the clustering. Keys 14, 21, and 35 all hash to slot 0, causing a three-element cluster at indices 0-2. Then key 10 hashes cleanly to slot 3, but key 0 hashes to slot 0 and has to probe all the way through the cluster (slots 0-3) before finding slot 4. This is primary clustering in action — the cluster at 0-2 grew to absorb slot 3 (occupied by 10) and extended to slot 4 (where 0 was placed).

If this were quadratic probing instead, key 0 would try: h(0)=0, 0+1=1, 0+4=4, 0+9=2 (mod 7), ... The probing sequence skips around instead of marching linearly, which distributes the load more evenly.

You correctly traced 4 of 5 data structure operations today — the heap trace was perfect and the hash table trace was perfect. Your understanding of the mechanics is strong.

[20:30:05] dfernandez_2026: cool. i think im gonna focus on avl rotations and heap operations for the rest of tonight. those are my weakest

[20:30:20] StudyBuddy AI Tutor: That's a good prioritization. For AVL rotations, practice these specific exercises:

1. Insert 50, 25, 75, 10, 30, 5 into an AVL tree. You should need one single right rotation (after inserting 5).
2. Insert 50, 25, 75, 10, 30, 27 into an AVL tree. You should need one left-right double rotation (after inserting 27).
3. Insert 10, 20, 30, 40, 50 into an AVL tree. This is the "sorted input" case that would degenerate a regular BST. In an AVL tree, you'll need multiple rotations — trace each one.

For heap operations, practice:
1. Build a max-heap from the array [3, 7, 1, 5, 9, 2, 8] using the bottom-up heapify algorithm.
2. Remove the minimum from a min-heap three consecutive times and trace the percolate-down steps.

The exam format for CS 261 is: 4 code-tracing problems, 3 analysis questions, 2 design/comparison essays, 90 minutes. No programming on the exam — all written/whiteboard style. You scored 92% on programming assignment 3 (BST implementation), which shows you understand the code. The exam will test whether you can trace operations by hand and analyze complexity without running code.

Your study group of 4 CS majors meets every Sunday — that's a good resource for whiteboard practice. Trace problems on a whiteboard without notes to simulate exam conditions. If you can correctly trace an AVL double rotation and a heap delete-min on a whiteboard, you're well prepared.

================================================================================
SESSION SUMMARY
================================================================================
Problems Attempted  : 5 (3 Big-O analysis, 1 min-heap insertion trace,
                      1 hash table insertion trace)
Problems Correct    : 5 of 5 (100%)
Topics Covered      : Big-O analysis (nested loops, logarithmic loops, constant
                      dropping), BST worst-case vs. average-case height, BST
                      degeneration and prevention, AVL trees (single rotations,
                      double rotations, balance factors), hash tables (chaining
                      vs. open addressing, load factor, linear/quadratic/double
                      hashing), heaps (insert, percolate-up, amortized O(1)),
                      heap vs. BST use cases
Key Misconceptions  : Why constants are dropped in Big-O (resolved), AVL double
                      rotations as "straighten then rotate" (resolved), load
                      factor and performance degradation (clarified)
Follow-up Planned   : AVL rotation practice (3 specific exercises), heap
                      operation practice (heapify, delete-min), whiteboard
                      practice with study group
Session Bookmarked  : No
================================================================================
END OF SESSION LOG — SB-2025-10-14-7588
================================================================================


SUPPLEMENTAL TRANSCRIPT CONTINUATION — EXTENDED REVIEW MATERIALS

The following content was generated during the session as supplemental review material for CS 261 midterm. It covers arrays, linked lists, stacks, queues, trees, BSTs, heaps, hash tables, and Big-O analysis from Goodrich et al., Data Structures and Algorithms in Java (6th edition).

--- TUTOR-GENERATED REVIEW: DATA STRUCTURES MIDTERM TOPICS ---

BIG-O COMPLEXITY REFERENCE TABLE

| Operation | Array | Linked List | BST (balanced) | BST (worst) | Heap | Hash Table (avg) |
|---|---|---|---|---|---|---|
| Access by index | O(1) | O(n) | N/A | N/A | N/A | N/A |
| Search | O(n) | O(n) | O(log n) | O(n) | O(n) | O(1) |
| Insert (end) | O(1) amortized | O(1) if tail ptr | O(log n) | O(n) | O(log n) | O(1) |
| Insert (front) | O(n) | O(1) | N/A | N/A | N/A | N/A |
| Delete | O(n) | O(1) if ref | O(log n) | O(n) | O(log n) | O(1) |
| Find min/max | O(n) | O(n) | O(log n) | O(n) | O(1) | O(n) |
| Sorted traversal | O(n log n) | O(n log n) | O(n) | O(n) | O(n log n) | O(n log n) |

TREE TERMINOLOGY

- Root: top node of the tree (no parent)
- Leaf: node with no children
- Internal node: node with at least one child
- Height: longest path from node to a leaf (height of leaf = 0)
- Depth: path length from root to node (depth of root = 0)
- Complete tree: all levels filled except possibly the last, which is filled left to right
- Full tree: every node has 0 or 2 children
- Perfect tree: all internal nodes have exactly 2 children and all leaves at same level
- For a complete binary tree with n nodes: height = floor(log2(n))

BST PROPERTY

For every node:
- All keys in the left subtree are LESS than the node's key
- All keys in the right subtree are GREATER than the node's key
- Both subtrees are also BSTs

In-order traversal of a BST visits nodes in sorted order: left subtree, root, right subtree.

AVL TREE RULES

1. Must be a valid BST
2. Balance factor = height(left subtree) - height(right subtree)
3. For every node, balance factor must be -1, 0, or 1
4. After insertion or deletion, walk back up to root checking balance factors
5. If any node violates rule 3, perform rotation(s) at that node

Four rotation cases:
- Left-Left (LL): balance factor = +2, left child has bf >= 0. Fix: single right rotation.
- Right-Right (RR): balance factor = -2, right child has bf <= 0. Fix: single left rotation.
- Left-Right (LR): balance factor = +2, left child has bf < 0. Fix: left rotation on child, then right rotation on node.
- Right-Left (RL): balance factor = -2, right child has bf > 0. Fix: right rotation on child, then left rotation on node.

HEAP PROPERTIES

Min-heap: parent key <= children keys (minimum at root)
Max-heap: parent key >= children keys (maximum at root)
Structure property: heap is a COMPLETE binary tree (array representation)

Array representation of a heap (0-indexed):
- Parent of node i: floor((i-1)/2)
- Left child of node i: 2i + 1
- Right child of node i: 2i + 2

Percolate up (after insert): compare new element with parent, swap if heap property violated, repeat until heap property restored or root reached.

Percolate down (after delete-min): move last element to root, compare with children, swap with SMALLER child (in min-heap), repeat until heap property restored or leaf reached.

Build-heap (heapify): O(n) algorithm to convert an arbitrary array into a heap. Start from the last internal node and percolate down each node from bottom to top. Despite calling percolate-down n/2 times, the total work is O(n) because most nodes are near the leaves and only percolate a short distance.

HASH TABLE ANALYSIS

Uniform hashing assumption: each key is equally likely to hash to any of the m slots, independently of other keys.

Expected number of probes for a SUCCESSFUL search:
- Chaining: 1 + alpha/2 (where alpha = n/m)
- Linear probing: (1/2)(1 + 1/(1-alpha))
- Double hashing: -(1/alpha) * ln(1 - alpha)

Expected number of probes for an UNSUCCESSFUL search:
- Chaining: 1 + alpha
- Linear probing: (1/2)(1 + 1/(1-alpha)^2)
- Double hashing: 1/(1-alpha)

When alpha = 0.5 (half full):
- Chaining successful: 1.25 probes, unsuccessful: 1.5 probes
- Linear probing successful: 1.5 probes, unsuccessful: 2.5 probes

When alpha = 0.9 (90% full):
- Chaining successful: 1.45 probes, unsuccessful: 1.9 probes
- Linear probing successful: 5.5 probes, unsuccessful: 50.5 probes

This shows why linear probing degrades so dramatically at high load factors and why resizing at alpha = 0.75 is standard practice.

STACKS AND QUEUES REVIEW

Stack (LIFO — Last In, First Out):
- Push: add element to top — O(1)
- Pop: remove element from top — O(1)
- Peek/Top: view top element without removing — O(1)
- Applications: function call stack, expression evaluation (infix to postfix conversion), undo operations, depth-first search (DFS)
- Implementation: array-based (dynamic array) or linked-list-based

Queue (FIFO — First In, First Out):
- Enqueue: add element to rear — O(1)
- Dequeue: remove element from front — O(1)
- Peek/Front: view front element without removing — O(1)
- Applications: process scheduling, breadth-first search (BFS), print queue, buffer
- Implementation: circular array or linked list

Priority Queue:
- Insert: add element with a priority — O(log n) with heap
- Remove-min (or remove-max): remove highest-priority element — O(log n) with heap
- Peek: view highest-priority element — O(1) with heap
- The natural implementation of a priority queue is a heap

LINKED LIST OPERATIONS REVIEW

Singly linked list:
- Insert at head: O(1) — create new node, point it to current head, update head pointer
- Insert at tail: O(1) if you maintain a tail pointer; O(n) if you don't (must traverse to end)
- Delete from head: O(1) — update head pointer to head.next
- Delete by value: O(n) — must search for the node, then update previous node's next pointer
- Search: O(n) — linear scan from head to tail

Doubly linked list:
- Same complexities as singly linked list, but deletion of a KNOWN node is O(1) because you have a back-pointer to the previous node
- Uses more memory per node (extra pointer)

Array vs. Linked List trade-offs:
- Array: O(1) random access, O(n) insertion/deletion in the middle, contiguous memory (good for cache)
- Linked list: O(n) random access, O(1) insertion/deletion at known position, non-contiguous memory (poor cache locality)
- Choose array when you need random access and the data size is relatively stable
- Choose linked list when you need frequent insertions/deletions at arbitrary positions

DESIGN AND COMPARISON ESSAY TOPICS

The exam includes 2 design/comparison essays. Common topics from previous semesters:

Topic 1: "Compare and contrast BSTs and hash tables for implementing a dictionary."
Key points: BSTs maintain sorted order (support range queries, in-order traversal, predecessor/successor) but have O(log n) operations. Hash tables have O(1) average-case operations but don't maintain any ordering. Hash tables require a good hash function and resize strategy. BSTs (balanced variants) have guaranteed worst-case O(log n); hash tables can degrade to O(n) with poor hash functions or high load factors.

Topic 2: "When would you choose a heap over a sorted array for implementing a priority queue?"
Key points: Heap insertion is O(log n) vs. O(n) for sorted array (must maintain sorted order). Heap delete-min is O(log n) vs. O(1) for sorted array (just remove the last element). If insertions are frequent compared to deletions, heap is better. If you insert everything first then only remove, sorted array might be better due to simpler implementation and better cache behavior.

Topic 3: "Explain why the build-heap algorithm runs in O(n) time despite calling percolate-down O(n/2) times."
Key points: Not all percolate-down calls do equal work. Nodes at the bottom of the tree (which are the majority) only percolate down 0 or 1 levels. Only the root percolates down log(n) levels. The sum of all percolate-down work across all nodes is: n/4 * 1 + n/8 * 2 + n/16 * 3 + ... = O(n). This is a geometric series that converges to 2n = O(n).

EXAM PREPARATION NOTES

Midterm covers all data structures through hash tables; no graph algorithms. Exam format: 4 code-tracing problems, 3 analysis questions, 2 design/comparison essays, 90 minutes. No programming on the exam — all written/whiteboard style.

COMMON EXAM MISTAKES IN DATA STRUCTURES

1. Confusing Big-O, Big-Omega, and Big-Theta. Big-O is an upper bound (worst case). Big-Omega is a lower bound (best case). Big-Theta is a tight bound (both upper and lower). When asked "what is the time complexity of binary search?" the answer is O(log n) for worst case and Theta(log n) for average case.

2. Forgetting that heap is a complete tree, not a BST. A heap does not maintain sorted order — it only guarantees that the root is the minimum (or maximum). You cannot do an in-order traversal of a heap to get sorted output.

3. Assuming all BSTs are balanced. A plain BST with no balancing strategy can degenerate to O(n) height with sorted input. Only balanced variants (AVL, red-black) guarantee O(log n) height.

4. Confusing open addressing collision resolution strategies. Linear probing, quadratic probing, and double hashing all use different probing sequences. Know which one causes primary clustering (linear) vs. secondary clustering (quadratic) vs. neither (double hashing).

5. Getting the array-to-heap index formulas wrong. For 0-indexed arrays: left child = 2i+1, right child = 2i+2, parent = floor((i-1)/2). For 1-indexed arrays: left child = 2i, right child = 2i+1, parent = floor(i/2). The exam will specify which indexing is used — read carefully.

6. AVL rotation errors: Performing a single rotation when a double rotation is needed (zig-zag case). Always check the direction pattern: if the path from the imbalanced node goes left-then-right (or right-then-left), you need a double rotation.

Student dfernandez_2026 is a sophomore computer science major. Scored 92% on programming assignment 3 (BST implementation). Primary weak areas: AVL tree rotations (especially double rotations) and Big-O analysis of complex loops. Strengths: hash table operations (strong from programming assignment 4), heap insertion tracing, basic BST operations.

Student meets with a study group of 4 CS majors every Sunday for whiteboard practice. Plans to focus remaining study time on AVL rotations and heap delete-min operations. Programming assignments 1-4 covered array list, linked list, BST, and hash map implementations in Java. The midterm does not require writing code but does require tracing code and analyzing algorithms by hand.

================================================================================
PLATFORM METADATA
================================================================================
Platform          : StudyBuddy
Session Rating    : Not yet submitted
AI Model Version  : SB-Tutor-v4.2
Content Flags     : None
Escalation Status : No escalation needed
================================================================================