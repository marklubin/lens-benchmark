{
  "version": "0.1.0",
  "scopes": [
    {
      "scope_id": "cascading_failure_01",
      "episodes": [
        {
          "episode_id": "cascading_failure_01_ep_001",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-15T10:00:00",
          "text": "## 2024-01-15 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 118240 | p50: 92ms p95: 210ms p99: 320ms | err: 0.12% (142 errors) | success: 99.88%\n- /fraud_check: REQUESTS 117980 | p50: 68ms p95: 160ms p99: 240ms | err: 0.08% (96 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 116420 | p50: 55ms p95: 130ms p99: 180ms | err: 0.05% (58 errors) | success: 99.95%\n- /auth: REQUESTS 152300 | p50: 28ms p95: 70ms p99: 110ms | err: 0.04% (61 errors) | success: 99.96%\n- /product_catalog: REQUESTS 204800 | p50: 34ms p95: 88ms p99: 140ms | err: 0.05% (105 errors) | success: 99.95%\n- /search: REQUESTS 176500 | p50: 48ms p95: 120ms p99: 190ms | err: 0.09% (159 errors) | success: 99.91%\n- /recommendations: REQUESTS 145900 | p50: 52ms p95: 140ms p99: 220ms | err: 0.1% (146 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 34% | Mem 58% | Disk 61% | Conns: 1820 | Net: 420/510 Mbps\n- gateway-02: CPU 32% | Mem 56% | Disk 60% | Conns: 1755 | Net: 405/498 Mbps\n- service-b-01: CPU 29% | Mem 54% | Disk 57% | Conns: 980 | Net: 210/240 Mbps\n- metrics-db-01: CPU 22% | Mem 66% | Disk 78% | Conns: 220 | Net: 90/110 Mbps\n\n### Connection Pools\n- primary: active 68 | idle 132 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 34 | idle 66 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 22 | idle 38 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.1% | Bandwidth: 6.2 Gbps | Origin requests: 312400\n\n### Alerts\n- [warning] DISK-USAGE-WARN on metrics-db-01: disk_pct=78\n- [info] CERT-EXPIRY-30D on gateway-01: api.example.com 30d\n\n### Deployments & Changes\n- Deployed auth-service v2.3.1\n\n### Events\n- CDN cache hit rate 94.1%\n- A/B test 'checkout-v2' at 15% rollout\n- Rotated gateway access logs to new bucket policy\n- Scheduled metrics-db cleanup window 2024-01-16 02:00Z\n\n### On-Call\n- shift handoff note: Shift: J. Martinez. 0 pages. 1 ticket (DISK-4421 cleanup). Quiet shift.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_001",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-15T10:30:00",
          "text": "## 2026-01-02 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 128430 | p50: 46.3ms p95:  | p99:  | err: 0.79% (1015 errors) | success: 99.21%\n- /auth/audit/mfa_enrollment: REQUESTS 312 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/authn_failures: REQUESTS 1420 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/token_ops: REQUESTS 214900 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/privileged_access: REQUESTS 412 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/rbac_changes: REQUESTS 7 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/audit_pipeline: REQUESTS 1842200 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/directory_sync: REQUESTS 24 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/compliance_controls: REQUESTS 12 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/idp_health: REQUESTS 3 | p50:  | p95:  | p99:  | err:  | success: \n\n### Infrastructure\n- idp-api-01: CPU 54% | Mem 71% | Disk 62% | Conns: 312 | Net: 88/94 Mbps\n- idp-api-02: CPU 49% | Mem 69% | Disk 60% | Conns: 305 | Net: 84/91 Mbps\n- audit-worker-01: CPU 62% | Mem 58% | Disk 44% | Conns:  | Net: 120/116 Mbps\n- audit-db-01: CPU 38% | Mem 77% | Disk 73% | IOPS: 9800 | Replication lag: 0.4 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (No specific pool data provided)\n\n### CDN & Caching\n- Hit rate:  | Bandwidth:  | N\n- (No CDN or caching data provided)\n\n### Alerts\n- [SEVERITY] alert-name on hostname: value\n- Detected one break-glass login outside maintenance window\n\n### Deployments & Changes\n- Quarterly access review kickoff: exported current role assignments for Finance and IT admin groups\n- RBAC policy update: tightened 'Support-ReadOnly' scope to exclude payroll attributes\n\n### Events\n- Minor directory sync failure on run #17 due to transient LDAP bind error; rerun succeeded\n- Detected one break-glass login outside maintenance window; validated change ticket and added note to audit trail\n\n### On-Call\n- shift handoff: R. Singh. 2 pages. 7 tickets. Status: Stable; audit notes updated and exceptions tracked",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_002",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-15T10:30:00",
          "text": "## 2026-01-03 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 121880 req | p50: 45ms p95: 78ms p99: 102ms | err: 0.66% (810 errors) | success: 99.34%\n- /mfa_enrollment: REQUESTS 284 req | p50: 12ms p95: 25ms p99: 33ms | err: 12.4% (14 failures) | success: 87.6%\n- /authn_failures: REQUESTS 1312 req | p50: 8ms p95: 15ms p99: 22ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 205340 req | p50: 9ms p95: 20ms p99: 28ms | err: 0.01% (21 failures) | success: 99.99%\n- /privileged_access: REQUESTS 396 req | p50: 16ms p95: 30ms p99: 45ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 4 req | p50: 39ms p95: 55ms p99: 70ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 1765400 req | p50: 7ms p95: 12ms p99: 18ms | err: 0.009% (162 dropped) | success: 99.991%\n- /directory_sync: REQUESTS 24 req | p50: 72s p95: 72s p99: 72s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 27 req | p50: 10ms p95: 18ms p99: 25ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 19 req | p50: 5ms p95: 9ms p99: 14ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 241 req | p50: 20ms p95: 35ms p99: 50ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 56 req | p50: 6ms p95: 11ms p99: 16ms | err: 0% | success: 100%\n- /session_security: REQUESTS 62 req | p50: 14ms p95: 25ms p99: 38ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 51% | Mem 70% | Disk 62% | Conns: 308 | Net: 82/89 Mbps\n- idp-api-02: CPU 47% | Mem 68% | Disk 60% | Conns: 301 | Net: 79/86 Mbps\n- audit-worker-01: CPU 59% | Mem 57% | Disk 44% | Conns: N/A | Net: 114/110 Mbps\n- audit-db-01: CPU 36% | Mem 76% | Disk 73% | IOPS: 9400 | Replication lag: 0.5 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (Data not provided for specific pools; omitted)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [INFO] SAML assertion errors on idp: 18\n- [WARNING] OIDC discovery error on idp: 1\n- [INFO] Metadata refreshes on idp: 2\n- [INFO] Exceptions opened: 1\n- [INFO] Exceptions closed: 1\n\n### Deployments & Changes\n- Renewed expired SP certificate for legacy HR app; coordinated with app owner.\n- Updated JWKS cache warmup on audit workers to reduce token rotation failures.\n\n### Events\n- Ran automated evidence collection for SOC2 CC6: captured MFA enforcement settings and admin group membership snapshots.\n- User-reported SAML error for legacy HR app; traced to expired SP certificate and coordinated renewal with app owner.\n- Completed cleanup of stale service accounts: revoked 12 roles not used in 180 days.\n- Reduced token rotation failures by updating JWKS cache warmup on audit workers.\n\n### On-Call\n- Shift: M. Alvarez. 1 page. 9 tickets. Status: Stable; one app certificate issue resolved and documented.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_003",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-15T10:30:00",
          "text": "## 2026-01-04 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 116220 req | p50: 43.1ms p95: 43.1ms p99: 43.1ms | err: 0.88% (1024 errors) | success: 99.12%\n- /mfa_enrollment: REQUESTS 266 req | p50: 0ms p95: 0ms p99: 0ms | err: 12.2% (23 failures) | success: 87.8%\n- /authn_failures: REQUESTS 1498 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 198740 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.02% (33 failures) | success: 99.98%\n- /privileged_access: REQUESTS 428 req | p50: 19.1min p95: 19.1min p99: 19.1min | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 6 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 1693300 req | p50: 0ms p95: 12.4sec p99: 12.4sec | err: 0.01% (248 dropped) | success: 99.99%\n- /directory_sync: REQUESTS 24 req | p50: 88s p95: 88s p99: 88s | err: 4.17% (1 failure) | success: 95.83%\n- /compliance_controls: REQUESTS 11 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 4 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 259 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 77 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /session_security: REQUESTS 85 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 24 req | p50: 0ms p95: 0ms p99: 0ms | err: 4.17% (1 failure) | success: 95.83%\n\n### Infrastructure\n- idp-api-01: CPU 58% | Mem 73% | Disk 62% | Conns: 319 | Net: 92/98 Mbps\n- idp-api-02: CPU 53% | Mem 71% | Disk 60% | Conns: 313 | Net: 89/95 Mbps\n- audit-worker-01: CPU 68% | Mem 61% | Disk 45% | Conns: 228 | Net: 128/123 Mbps\n- audit-db-01: CPU 41% | Mem 79% | Disk 74% | IOPS: 10200 | Replication lag: 0.7 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 15ms\n- pool-idp-api-02: active 115 | idle 25 | waiting 3 | exhaustion 0 | max 200 | avg_wait 12ms\n- pool-audit-worker: active 50 | idle 10 | waiting 2 | exhaustion 0 | max 100 | avg_wait 10ms\n- pool-audit-db: active 20 | idle 4 | waiting 1 | exhaustion 0 | max 50 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12450\n\n### Alerts\n- [SEVERE] LDAP bind timeout on directory sync: run #9 failed on first attempt\n- [WARNING] Missing manager approvals on elevated role grants: 3 audit exceptions opened\n- [INFO] Increased audit ingest lag p95 after event enrichment; worker batch size tuned\n- [INFO] Scheduled rotation of signing keys for internal OIDC client; downstream validation verified\n\n### Deployments & Changes\n- Rotated signing keys for internal OIDC client; validation confirmed downstream\n- Enabled additional event enrichment fields; ingest lag tuned\n- No recent code deployments reported\n\n### Events\n- Directory sync run #9 failed on first attempt due to LDAP bind timeout; rerun succeeded and reconciled 21 deletions\n- Opened three audit exceptions for missing manager approvals on elevated role grants; notified access review owners\n- Increased audit ingest lag p95 after enabling additional event enrichment fields; tuned worker batch size to recover\n- Performed scheduled rotation of signing keys for one internal OIDC client; verified downstream validation\n\n### On-Call\n- shift handoff note: J. Chen. 3 pages. 11 tickets. Status: Monitoring; pipeline tuning applied and exceptions logged",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_004",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-15T10:30:00",
          "text": "## 2026-01-05 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 132940 req | p50: 47.2ms p95: 0ms p99: 0ms | err: 0.56% (747 errors) | success: 99.44%\n- /mfa_enrollment: REQUESTS 338 req | p50: 0ms p95: 0ms p99: 0ms | err: 11.9% (11 failures) | success: 88.1%\n- /authn_failures: REQUESTS 1709 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 278508 req | p50: 171ms p95: 0ms p99: 0ms | err: 0.006% (18 failures) | success: 99.994%\n- /privileged_access: REQUESTS 569 req | p50: 18.3min p95: 0min p99: 0min | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 332 req | p50: 44min p95: 0min p99: 0min | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 1918800 req | p50: 7.4sec p95: 7.4sec p99: 7.4sec | err: 0.007% (140 dropped) | success: 99.993%\n- /directory_sync: REQUESTS 24 req | p50: 69sec p95: 69sec p99: 69sec | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 32 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 17 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 355 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 49 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /session_security: REQUESTS 61 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 31 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 61 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 52% | Mem 69% | Disk 62% | Conns: 310 | Net: 96/103 Mbps\n- idp-api-02: CPU 48% | Mem 68% | Disk 60% | Conns: 304 | Net: 92/100 Mbps\n- audit-worker-01: CPU 57% | Mem 56% | Disk 45% | Conns: N/A | Net: 132/127 Mbps\n- audit-db-01: CPU 35% | Mem 75% | Disk 74% | IOPS: 9100 | Replication lag: 0.4 sec\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 0% | Bandwidth: 0 Gbps | Origin requests: 0\n\n### Alerts\n- [NONE]\n\n### Deployments & Changes\n- Updated RBAC policy to require manager + security approval for access to 'CustomerPII-Read'\n- Verified IdP metadata refresh for three SAML apps; no assertion regressions observed\n- Completed weekly attestation export for 'Production-Admin' and 'DBA' roles; delivered to GRC vault\n\n### Events\n- Completed weekly attestation export for 'Production-Admin' and 'DBA' roles; delivered to GRC vault\n- Updated RBAC policy to require manager + security approval for access to 'CustomerPII-Read'\n- Investigated elevated account locks; correlated to password manager rollout for Sales; published user guidance\n- Verified IdP metadata refresh for three SAML apps; no assertion regressions observed\n\n### On-Call\n- shift handoff note: Shift: A. Patel. 0 pages. 8 tickets. Status: Stable; policy updates and evidence exports completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_002",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-16T10:00:00",
          "text": "## 2024-01-16 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 121880 | p50: 94ms p95: 215ms p99: 330ms | err: 0.13% (156 errors) | success: 99.87%\n- /fraud_check: REQUESTS 121600 | p50: 69ms p95: 165ms p99: 250ms | err: 0.08% (102 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 120050 | p50: 56ms p95: 132ms p99: 185ms | err: 0.05% (62 errors) | success: 99.95%\n- /auth: REQUESTS 158900 | p50: 27ms p95: 68ms p99: 108ms | err: 0.05% (72 errors) | success: 99.95%\n- /product_catalog: REQUESTS 212300 | p50: 35ms p95: 90ms p99: 145ms | err: 0.05% (110 errors) | success: 99.95%\n- /search: REQUESTS 182400 | p50: 49ms p95: 122ms p99: 195ms | err: 0.09% (170 errors) | success: 99.91%\n- /recommendations: REQUESTS 149700 | p50: 53ms p95: 142ms p99: 225ms | err: 0.1% (150 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 36% | Mem 59% | Disk 61% | Conns: 1895 | Net: 438/526 Mbps\n- gateway-02: CPU 33% | Mem 57% | Disk 60% | Conns: 1788 | Net: 418/507 Mbps\n- service-b-01: CPU 31% | Mem 55% | Disk 57% | Conns: 1015 | Net: 218/248 Mbps\n- metrics-db-01: CPU 25% | Mem 67% | Disk 82% | Conns: 235 | Net: 96/118 Mbps\n\n### Connection Pools\n- primary: active 72 | idle 128 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 36 | idle 64 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 24 | idle 36 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.3% | Bandwidth: 6.4 Gbps | Origin requests: 321900\n\n### Alerts\n- [WARNING] DISK-USAGE-WARN on metrics-db-01: disk_pct=82\n- [INFO] LOG-ROTATION on gateway-02: completed\n\n### Deployments & Changes\n- Deployed product-catalog v5.8.0\n\n### Events\n- metrics-db cleanup ran 2024-01-16 02:00Z\n- A/B test 'checkout-v2' at 15% rollout\n- Updated Grafana dashboard folder permissions\n- CDN origin shield enabled for /static\n\n### On-Call\n- Shift: A. Chen. 0 pages. 1 ticket (DISK-4450 metrics-db retention). Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_005",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-16T10:30:00",
          "text": "## 2026-01-06 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 140210 req | p50: 48.8ms p95: N/A p99: N/A | err: 0.82% (1158 errors) | success: 99.18%\n- /mfa_enrollment: REQUESTS 361 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /authn_failures: REQUESTS 2169 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /token_ops: REQUESTS 236900 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /privileged_access: REQUESTS 634 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /rbac_changes: REQUESTS 8 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /audit_pipeline: REQUESTS 2046600 req | p50: N/A p95: 15.2s p99: N/A | err: N/A | success: N/A\n- /directory_sync: REQUESTS 24 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /compliance_controls: REQUESTS 12 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /idp_health: REQUESTS 6 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /helpdesk: REQUESTS 338 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /rate_limits: REQUESTS 107 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /session_security: REQUESTS 112 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /evidence_exports: REQUESTS 32 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /admin_approvals: REQUESTS 83 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- idp-api-01: CPU 66% | Mem 78% | Disk 63% | Conns: N/A | Net: 110/118 Mbps\n- idp-api-02: CPU 61% | Mem 76% | Disk 61% | Conns: N/A | Net: 106/113 Mbps\n- audit-worker-01: CPU 79% | Mem 66% | Disk 46% | Conns: N/A | Net: 148/142 Mbps\n- audit-db-01: CPU 46% | Mem 82% | Disk 75% | IOPS: 11800 | Replication lag: 1.1 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n  (Data not provided; omitted)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n  (Data not provided; omitted)\n\n### Alerts\n- [WARNING] idp_health: 6 OIDC discovery errors\n- [ERROR] idp_health: 28 SAML assertion errors\n- [WARNING] idp_health: 2 JWKS fetch errors\n- [INFO] idp_health: 5 metadata refreshes\n\n### Deployments & Changes\n- Updated JWKS for internal clients; monitored for signature validation errors; none sustained.\n- Processed high volume of access requests ahead of quarterly review; enforced two-person approval for privileged roles.\n- Audit ingest lag increased after enabling additional token rotation telemetry; reduced by scaling audit workers and increasing batch flush interval.\n- Evidence exports rerun with narrower time windows; stored in GRC vault.\n- Directory sync: 24 runs; 2230 objects updated; 17 objects deleted; no failures; average duration 92 sec.\n- Compliance controls: 12 SOX checks; 16 SOC2 checks; 2 open exceptions; 1 closed exception.\n- Helpdesk: 338 access requests; 224 password resets; 61 MFA resets; median first response 24 min.\n- Admin approvals: 66 pending; 4 auto-approved; 12 rejected; average approval time 67 min.\n\n### Events\n- Audit ingest lag increased after enabling additional token rotation telemetry; reduced by scaling audit workers and increasing batch flush interval.\n- Two evidence exports failed due to report size limits; reran with narrower time windows and stored in GRC vault.\n- Updated JWKS for several internal clients; monitored for signature validation errors; none sustained.\n- Processed high volume of access requests ahead of quarterly review meeting; enforced two-person approval for privileged roles.\n\n### On-Call\n- Shift: K. Morgan. 4 pages. 14 tickets. Status: Busy but controlled; worker scaling applied and reports rerun.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_006",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-16T10:30:00",
          "text": "## 2026-01-07 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 137560 | p50: 48ms p95: 102ms p99: 210ms | err: 0.61% (839 errors) | success: 99.39%\n- /mfa_enrollment: REQUESTS 329 | p50: 12ms p95: 25ms p99: 40ms | err: 4.4% (13 failures) | success: 95.6%\n- /authn_failures: REQUESTS 1349 | p50: 8ms p95: 20ms p99: 35ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 231400 | p50: 15ms p95: 30ms p99: 50ms | err: 0.01% (20 failures) | success: 99.99%\n- /privileged_access: REQUESTS 488 | p50: 22ms p95: 45ms p99: 70ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 6 | p50: 49ms p95: 70ms p99: 90ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 1992200 | p50: 5ms p95: 8.7ms p99: 15ms | err: 0.008% (156 dropped) | success: 99.992%\n- /directory_sync: REQUESTS 24 | p50: 76s p95: 76s p99: 76s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 14 | p50: 30ms p95: 50ms p99: 70ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 3 | p50: 10ms p95: 20ms p99: 30ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 301 | p50: 17min p95: 25min p99: 35min | err: 0% | success: 100%\n- /rate_limits: REQUESTS 60 | p50: 10ms p95: 20ms p99: 30ms | login_rate_limited: 46 | token_endpoint_rate_limited: 14 | api_keys_throttled: 0\n- /session_security: REQUESTS 71 | p50: 12ms p95: 25ms p99: 40ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 29 | p50: 5ms p95: 10ms p99: 15ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 65 | p50: 54min p95: 70min p99: 90min | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 57% | Mem 72% | Disk 63% | Conns: 320 | Net: 104/111 Mbps\n- idp-api-02: CPU 53% | Mem 71% | Disk 61% | Conns: 315 | Net: 101/108 Mbps\n- audit-worker-01: CPU 63% | Mem 59% | Disk 46% | Conns: N/A | Net: 140/134 Mbps\n- audit-db-01: CPU 39% | Mem 78% | Disk 75% | IOPS: 10050 | Replication lag: 0.6 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 50 | waiting 5 | exhaustion 2 | max 200 | avg_wait 12ms\n- pool-idp-api-02: active 115 | idle 55 | waiting 4 | exhaustion 1 | max 200 | avg_wait 10ms\n- pool-audit-worker-01: active 80 | idle 30 | waiting 3 | exhaustion 0 | max 150 | avg_wait 15ms\n- pool-audit-db-01: active 60 | idle 20 | waiting 2 | exhaustion 0 | max 100 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12450\n\n### Alerts\n- [SEVERE] idp-api-01: CPU 57%\n- [WARNING] audit-db-01: Disk 75%\n- [INFO] audit-worker-01: CPU 63%\n- [SEVERE] idp-api-02: CPU 53%\n\n### Deployments & Changes\n- Quarterly role change report generated and shared with auditors.\n- Removed 160 roles tied to offboarded contractors; HR feed reconciliation completed.\n- Fixed 17 recurring SAML assertion errors by correcting NameID format on two partner apps.\n- Validated token rotation success; failures reduced to 20 after tuning.\n\n### Events\n- Confirmed removal of 160 roles tied to offboarded contractors; reconciled with HR feed.\n- Fixed 17 recurring SAML assertion errors.\n- Validated token rotation success after prior day tuning; failures down to 20.\n- Generated delta report of role changes vs last quarter; shared with auditors.\n\n### On-Call\n- Shift: S. Ito. 1 page. 10 tickets. Status: Stable; review artifacts produced and offboarding reconciled.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_007",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-16T10:30:00",
          "text": "## 2026-01-08 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 134880 | p50: 47.1ms p95: N/A p99: N/A | err: 0.73% (974 errors) | success: 99.27%\n- /mfa_enrollment: REQUESTS N/A | enrolled_today: 301 | total_enrolled_pct: 88.8% | sms_pct: 20.5% | totp_pct: 65.0% | webauthn_pct: 14.5% | enrollment_failures: 16\n- /authn_failures: REQUESTS N/A | invalid_password: 1411 | mfa_challenge_failed: 382 | account_locked: 97 | suspicious_impossible_travel_flags: 0\n- /token_ops: REQUESTS N/A | access_tokens_issued: 226330 | refresh_tokens_issued: 56020 | token_rotations: 12490 | rotation_failures: 24 | avg_rotation_time_ms: 183\n- /privileged_access: REQUESTS N/A | admin_logins: 471 | break_glass_uses: 0 | privileged_session_avg_min: 18.8 | sudo_requests: 118\n- /rbac_changes: REQUESTS N/A | policy_updates: 5 | role_grants: 160 | role_revokes: 172 | permission_change_requests: 55 | approval_sla_min_p50: 46\n- /audit_pipeline: REQUESTS N/A | events_ingested: 1947700 | events_dropped: 172 | queue_depth_peak: 4470 | ingest_lag_sec_p95: 9.1\n- /directory_sync: REQUESTS N/A | sync_runs: 24 | objects_updated: 1180 | objects_deleted: 10 | sync_failures: 0 | avg_sync_duration_sec: 73\n- /compliance_controls: REQUESTS N/A | sox_controls_checked: 12 | soc2_controls_checked: 17 | exceptions_opened: 0 | exceptions_closed: 1\n- /idp_health: REQUESTS N/A | oidc_discovery_errors: 2 | saml_assertion_errors: 19 | jwks_fetch_errors: 0 | metadata_refreshes: 2\n- /helpdesk: REQUESTS N/A | access_requests: 287 | password_resets: 184 | mfa_resets: 42 | median_first_response_min: 19\n- /rate_limits: REQUESTS N/A | login_rate_limited: 51 | token_endpoint_rate_limited: 16 | api_keys_throttled: 0\n- /session_security: REQUESTS N/A | forced_reauth_events: 66 | anomalous_device_blocks: 11 | remembered_device_count: 29580\n- /evidence_exports: REQUESTS N/A | csv_exports: 21 | pdf_exports: 8 | export_failures: 0\n- /admin_approvals: REQUESTS N/A | pending_requests: 49 | auto_approved: 5 | rejected: 7 | avg_approval_time_min: 50\n\n### Infrastructure\n- idp-api-01: CPU 55% | Mem 71% | Disk 63% | Conns: 317 | Net: 99/106 Mbps\n- idp-api-02: CPU 52% | Mem 70% | Disk 61% | Conns: 313 | Net: 97/103 Mbps\n- audit-worker-01: CPU 65% | Mem 60% | Disk 46% | Conns: 227 | Net: 136/131 Mbps\n- audit-db-01: CPU 40% | Mem 79% | Disk 75% | IOPS: 10120 | Replication lag: 0.6 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n(Note: Specific pool data not provided in data sheet)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [SEVERITY] alert-name on hostname: value\n(Note: No specific alerts provided in data sheet)\n\n### Deployments & Changes\n- Implemented new attestation reminder schedule for managers; tracked expected reduction in overdue approvals.\n- Reviewed token rotation logs: 6 clients still rotating outside policy window; opened tickets to app owners.\n- Revoked roles for 14 users with stale 'Temp-Admin' membership older than 30 days.\n- Ran integrity check on audit event schema; confirmed no missing fields after enrichment changes.\n\n### Events\n- Implemented new attestation reminder schedule for managers; tracked expected reduction in overdue approvals.\n- Reviewed token rotation logs: 6 clients still rotating outside policy window; opened tickets to app owners.\n- Revoked roles for 14 users with stale 'Temp-Admin' membership older than 30 days.\n- Ran integrity check on audit event schema; confirmed no missing fields after enrichment changes.\n\n### On-Call\n- Shift: L. Brown. 1 page. 6 tickets. Status: Stable; policy compliance follow-ups initiated.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_003",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-17T10:00:00",
          "text": "## 2024-01-17 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 119450 | p50: 91ms p95: 208ms p99: 318ms | err: 0.12% (139 errors) | success: 99.88%\n- /fraud_check: REQUESTS 119120 | p50: 67ms p95: 158ms p99: 238ms | err: 0.08% (92 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 117600 | p50: 55ms p95: 128ms p99: 178ms | err: 0.05% (55 errors) | success: 99.95%\n- /auth: REQUESTS 155400 | p50: 27ms p95: 67ms p99: 107ms | err: 0.04% (65 errors) | success: 99.96%\n- /product_catalog: REQUESTS 207900 | p50: 34ms p95: 87ms p99: 142ms | err: 0.05% (104 errors) | success: 99.95%\n- /search: REQUESTS 178600 | p50: 48ms p95: 118ms p99: 188ms | err: 0.09% (161 errors) | success: 99.91%\n- /recommendations: REQUESTS 147200 | p50: 51ms p95: 138ms p99: 218ms | err: 0.1% (141 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 33% | Mem 58% | Disk 61% | Conns: 1830 | Net: 425/512 Mbps\n- gateway-02: CPU 31% | Mem 56% | Disk 60% | Conns: 1762 | Net: 410/500 Mbps\n- service-b-01: CPU 28% | Mem 54% | Disk 57% | Conns: 990 | Net: 212/242 Mbps\n- metrics-db-01: CPU 21% | Mem 65% | Disk 79% | Conns: 228 | Net: 88/109 Mbps\n\n### Connection Pools\n- primary: active 66 | idle 134 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 33 | idle 67 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 21 | idle 39 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.0% | Bandwidth: 6.1 Gbps | Origin requests: 309800\n\n### Alerts\n- [INFO] CERT-EXPIRY-14D on gateway-01: api.example.com 14d\n\n### Deployments & Changes\n- Deployed search-service v4.1.2\n\n### Events\n- SSL cert check completed for api.example.com\n- A/B test 'checkout-v2' at 15% rollout\n- Scaled gateway autoscaling min from 2 to 2 (no change)\n- Rotated service account token for metrics exporter\n\n### On-Call\n- shift handoff note: K. Okafor. 0 pages. 0 tickets. Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_008",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-17T10:30:00",
          "text": "## 2026-01-09 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 112340 | p50: 42.7ms p95: 89.3ms p99: 124.5ms | err: 0.49% (561 errors) | success: 99.51%\n- /auth/audit/mfa_enrollment: REQUESTS 244 | p50: 15.2ms p95: 27.8ms p99: 35.4ms | err: 11.0% (9 failures) | success: 89.0%\n- /auth/audit/authn_failures: REQUESTS 1482 | p50: 8.1ms p95: 15.6ms p99: 22.3ms | err: 0.0% | success: 100%\n- /auth/audit/token_ops: REQUESTS 187620 | p50: 4.2ms p95: 9.8ms p99: 15.7ms | err: 0.01% (13 failures) | success: 99.99%\n- /auth/audit/privileged_access: REQUESTS 483 | p50: 12.4ms p95: 23.7ms p99: 31.2ms | err: 0.0% | success: 100%\n- /auth/audit/rbac_changes: REQUESTS 307 | p50: 19.3ms p95: 33.8ms p99: 45.1ms | err: 0.0% | success: 100%\n- /auth/audit/audit_pipeline: REQUESTS 1629100 | p50: 3.2ms p95: 6.2ms p99: 9.8ms | err: 0.006% (98 dropped events) | success: 99.994%\n- /auth/audit/directory_sync: REQUESTS 24 | p50: 66.0s p95: 66.0s p99: 66.0s | err: 0.0% | success: 100%\n- /auth/audit/compliance_controls: REQUESTS 22 | p50: 11.4ms p95: 19.8ms p99: 25.7ms | err: 0.0% | success: 100%\n- /auth/audit/idp_health: REQUESTS 12 | p50: 5.2ms p95: 9.4ms p99: 13.1ms | err: 0.0% | success: 100%\n- /auth/audit/helpdesk: REQUESTS 378 | p50: 14.2ms p95: 22.7ms p99: 29.4ms | err: 0.0% | success: 100%\n- /auth/audit/rate_limits: REQUESTS 36 | p50: 6.1ms p95: 11.2ms p99: 16.8ms | err: 0.0% | success: 100%\n- /auth/audit/session_security: REQUESTS 47 | p50: 9.3ms p95: 17.4ms p99: 24.8ms | err: 0.0% | success: 100%\n- /auth/audit/evidence_exports: REQUESTS 16 | p50: 7.8ms p95: 13.5ms p99: 19.2ms | err: 0.0% | success: 100%\n- /auth/audit/admin_approvals: REQUESTS 41 | p50: 22.4ms p95: 38.1ms p99: 45.7ms | err: 0.0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 43% | Mem 66% | Disk 63% | Conns: 296 | Net: 76/81 Mbps\n- idp-api-02: CPU 41% | Mem 65% | Disk 61% | Conns: 292 | Net: 74/79 Mbps\n- audit-worker-01: CPU 52% | Mem 54% | Disk 46% | Conns: N/A | Net: 108/104 Mbps\n- audit-db-01: CPU 31% | Mem 74% | Disk 75% | IOPS: 8200 | Replication lag: 0.4 sec\n\n### Connection Pools\n- pool-auth: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- pool-graphql: active 85 | idle 15 | waiting 2 | exhaustion 0 | max 150 | avg_wait 8ms\n- pool-cache: active 60 | idle 40 | waiting 1 | exhaustion 0 | max 100 | avg_wait 5ms\n- pool-logging: active 20 | idle 10 | waiting 0 | exhaustion 0 | max 50 | avg_wait 3ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12450\n\n### Alerts\n- [SEVERE] SAML assertion errors on idp-api-02: 12\n- [WARNING] audit pipeline drop rate approaching threshold\n- [INFO] directory sync completed with 24 runs\n- [INFO] audit pipeline queue peak: 3180 events\n- [SEVERE] idp-health SAML assertion errors: 12\n- [WARNING] audit pipeline ingest lag p95: 6.2 sec\n\n### Deployments & Changes\n- Planned cleanup of expired refresh tokens executed; counts verified.\n- One long-running access exception closed after manager attestation.\n- SAML assertion errors reviewed; 4 tied to outdated browser time settings; KB article added.\n- Audit pipeline backpressure settings improved; drop rate monitored.\n\n### Events\n- Weekend lower traffic: executed planned cleanup of expired refresh tokens older than retention threshold; verified counts match policy.\n- Closed one long-running access exception after manager attestation completed in GRC tool.\n- Reviewed SAML assertion errors; 4 tied to users with outdated browser time settings; added KB article.\n- Validated audit pipeline drop rate; remained under alert threshold with improved backpressure settings.\n\n### On-Call\n- shift handoff note: P. Nguyen. 0 pages. 5 tickets. Status: Quiet; maintenance cleanup and documentation completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_009",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-17T10:30:00",
          "text": "## 2026-01-10 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 109980 req | p50: 42.1ms p95: N/A p99: N/A | err: 0.52% (575 errors) | success: 99.48%\n- /mfa_enrollment: REQUESTS N/A | enrolled_today: 231 | total_enrolled_pct: 89.1% | sms_pct: 20.3% | totp_pct: 65.2% | webauthn_pct: 14.5% | enrollment_failures: 10\n- /authn_failures: REQUESTS N/A | invalid_password: 1149 | mfa_challenge_failed: 311 | account_locked: 75 | suspicious_impossible_travel_flags: 0\n- /token_ops: REQUESTS N/A | access_tokens_issued: 184230 | refresh_tokens_issued: 46620 | token_rotations: 10190 | rotation_failures: 15 | avg_rotation_time_ms: 172\n- /privileged_access: REQUESTS N/A | admin_logins: 381 | break_glass_uses: 0 | privileged_session_avg_min: 17.2 | sudo_requests: 89\n- /rbac_changes: REQUESTS N/A | policy_updates: 4 | role_grants: 129 | role_revokes: 121 | permission_change_requests: 45 | approval_sla_min_p50: 43\n- /audit_pipeline: REQUESTS N/A | events_ingested: 1598400 | events_dropped: 110 | queue_depth_peak: 3420 | ingest_lag_sec_p95: 6.8\n- /directory_sync: REQUESTS N/A | sync_runs: 24 | objects_updated: 740 | objects_deleted: 7 | sync_failures: 0 | avg_sync_duration_sec: 67\n- /compliance_controls: REQUESTS N/A | sox_controls_checked: 9 | soc2_controls_checked: 13 | exceptions_opened: 1 | exceptions_closed: 0\n- /idp_health: REQUESTS N/A | oidc_discovery_errors: 1 | saml_assertion_errors: 14 | jwks_fetch_errors: 0 | metadata_refreshes: 2\n- /helpdesk: REQUESTS N/A | access_requests: 216 | password_resets: 151 | mfa_resets: 29 | median_first_response_min: 15\n- /rate_limits: REQUESTS N/A | login_rate_limited: 31 | token_endpoint_rate_limited: 9 | api_keys_throttled: 0\n- /session_security: REQUESTS N/A | forced_reauth_events: 46 | anomalous_device_blocks: 5 | remembered_device_count: 29710\n- /evidence_exports: REQUESTS N/A | csv_exports: 12 | pdf_exports: 4 | export_failures: 0\n- /admin_approvals: REQUESTS N/A | pending_requests: 35 | auto_approved: 4 | rejected: 5 | avg_approval_time_min: 41\n\n### Infrastructure\n- idp-api-01: CPU 44% | Mem 66% | Disk 63% | Conns: N/A | Net: 77/82 Mbps\n- idp-api-02: CPU 42% | Mem 65% | Disk 61% | Conns: N/A | Net: 75/80 Mbps\n- audit-worker-01: CPU 54% | Mem 55% | Disk 46% | Conns: N/A | Net: 110/106 Mbps\n- audit-db-01: CPU 32% | Mem 74% | Disk 75% | IOPS: 8350 | Replication lag: 0.4 sec\n\n### Connection Pools\n- pool-name: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [WARNING] oidc_discovery_errors on idp_health: 1\n- [ERROR] saml_assertion_errors on idp_health: 14\n- [INFO] audit exception opened for privileged role grant missing security co-approval\n- [INFO] RBAC policy text adjusted; stored new version hash\n- [INFO] token rotation failures: 9 tied to deprecated endpoint; migration ticket filed\n- [INFO] directory deletions vs HR terminations: all 7 matched\n\n### Deployments & Changes\n- Adjusted RBAC policy text for clarity; stored new version hash\n- Performed spot-check of directory deletions vs HR terminations; all 7 matched\n- Filed migration ticket for 9 token rotation failures linked to deprecated endpoint\n- Opened an audit exception for one privileged role grant missing security co-approval; requested retroactive approval or revoke\n\n### Events\n- Opened an audit exception for one privileged role grant missing security co-approval; requested retroactive approval or revoke\n- Adjusted RBAC policy text for clarity (no permission change) and stored new version hash in evidence repository\n- Reviewed token rotation failures; 9 tied to one internal client using deprecated endpoint; filed migration ticket\n- Performed spot-check of directory deletions vs HR terminations; all 7 matched\n\n### On-Call\n- shift handoff note: D. Kim. 0 pages. 6 tickets. Status: Stable; one compliance exception opened and client migration queued.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_010",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-17T10:30:00",
          "text": "## 2026-01-11 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 115770 | p50: 43ms p95: 75ms p99: 102ms | err: 0.64% (744 errors) | success: 99.36%\n- /auth/audit/mfa_enrollment: REQUESTS 252 | p50: 12ms p95: 20ms p99: 25ms | err: 10.7% (12 failures) | success: 89.3%\n- /auth/audit/authn_failures: REQUESTS 1839 | p50: 8ms p95: 15ms p99: 22ms | err: 0% | success: 100%\n- /auth/audit/token_ops: REQUESTS 242,590 | p50: 9ms p95: 18ms p99: 25ms | err: 0.0088% (17 failures) | success: 99.9912%\n- /auth/audit/privileged_access: REQUESTS 500 | p50: 10ms p95: 20ms p99: 28ms | err: 0% | success: 100%\n- /auth/audit/rbac_changes: REQUESTS 320 | p50: 15ms p95: 25ms p99: 35ms | err: 0% | success: 100%\n- /auth/audit/audit_pipeline: REQUESTS 1,679,200 | p50: 5ms p95: 12ms p99: 20ms | err: 0.0074% (124 dropped) | success: 99.9926%\n- /auth/audit/directory_sync: REQUESTS 24 | p50: 70ms p95: 75ms p99: 80ms | err: 0% | success: 100%\n- /auth/audit/compliance_controls: REQUESTS 23 | p50: 20ms p95: 30ms p99: 40ms | err: 0% | success: 100%\n- /auth/audit/idp_health: REQUESTS 17 | p50: 10ms p95: 18ms p99: 25ms | err: 0% | success: 100%\n- /auth/audit/helpdesk: REQUESTS 424 | p50: 16ms p95: 25ms p99: 35ms | err: 0% | success: 100%\n- /auth/audit/rate_limits: REQUESTS 45 | p50: 5ms p95: 10ms p99: 15ms | err: 0% | success: 100%\n- /auth/audit/session_security: REQUESTS 52 | p50: 8ms p95: 14ms p99: 20ms | err: 0% | success: 100%\n- /auth/audit/evidence_exports: REQUESTS 20 | p50: 7ms p95: 12ms p99: 18ms | err: 0% | success: 100%\n- /auth/audit/admin_approvals: REQUESTS 47 | p50: 22ms p95: 35ms p99: 45ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 46% | Mem 67% | Disk 63% | Conns: 299 | Net: 81/86 Mbps\n- idp-api-02: CPU 44% | Mem 66% | Disk 61% | Conns: 296 | Net: 79/84 Mbps\n- audit-worker-01: CPU 56% | Mem 56% | Disk 46% | Conns: N/A | Net: 114/109 Mbps\n- audit-db-01: CPU 33% | Mem 75% | Disk 75% | IOPS: 8600 | Replication lag: 0.5 sec\n\n### Connection Pools\n- auth_pool: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 200 | avg_wait 15ms\n- session_pool: active 85 | idle 20 | waiting 3 | exhaustion 1 | max 150 | avg_wait 10ms\n- token_pool: active 200 | idle 50 | waiting 8 | exhaustion 4 | max 300 | avg_wait 20ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1,250,000\n\n### Alerts\n- [CRITICAL] idp-api-01 CPU at 46%\n- [WARNING] audit-worker-01 CPU at 56%\n- [INFO] audit-db-01 disk usage at 75%\n- [ERROR] idp-api-02 network in at 79 Mbps\n- [ERROR] idp-api-02 network out at 84 Mbps\n\n### Deployments & Changes\n- Deployed new MFA enrollment UI update.\n- Applied security patch to idp-api-01 and idp-api-02.\n- Updated RBAC policies for admin role.\n- Restarted audit-worker-01 for performance tuning.\n- Scheduled directory sync for midnight.\n\n### Events\n- Completed internal audit checklist for token signing key custody: validated HSM access logs and operator approvals.\n- Resolved 6 helpdesk tickets for MFA resets after phone replacements; verified identity proofing steps logged.\n- Reviewed anomalous device blocks; 3 tied to new corporate laptop image missing device cert; coordinated with IT.\n- Ran monthly sample of admin sessions to confirm session recording enabled; documented evidence IDs.\n\n### On-Call\n- Shift: E. Garcia. 0 pages. 7 tickets. Status: Stable; compliance sampling and device cert coordination in progress.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_004",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-18T10:00:00",
          "text": "## 2024-01-18 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 123600 | p50: 93ms p95: 212ms p99: 326ms | err: 0.12% (150 errors) | success: 99.88%\n- /fraud_check: REQUESTS 123250 | p50: 68ms p95: 162ms p99: 245ms | err: 0.08% (98 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 121700 | p50: 55ms p95: 131ms p99: 182ms | err: 0.05% (60 errors) | success: 99.95%\n- /auth: REQUESTS 160800 | p50: 28ms p95: 69ms p99: 110ms | err: 0.05% (74 errors) | success: 99.95%\n- /product_catalog: REQUESTS 214900 | p50: 35ms p95: 89ms p99: 144ms | err: 0.05% (112 errors) | success: 99.95%\n- /search: REQUESTS 185200 | p50: 49ms p95: 121ms p99: 192ms | err: 0.09% (175 errors) | success: 99.91%\n- /recommendations: REQUESTS 151100 | p50: 52ms p95: 141ms p99: 223ms | err: 0.1% (151 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 35% | Mem 59% | Disk 61% | Conns: 1910 | Net: 445/536 Mbps\n- gateway-02: CPU 33% | Mem 57% | Disk 60% | Conns: 1825 | Net: 430/520 Mbps\n- service-b-01: CPU 30% | Mem 55% | Disk 57% | Conns: 1030 | Net: 225/255 Mbps\n- metrics-db-01: CPU 23% | Mem 66% | Disk 80% | Conns: 236 | Net: 94/116 Mbps\n\n### Connection Pools\n- primary: active 74 | idle 126 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 37 | idle 63 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 25 | idle 35 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.2% | Bandwidth: 6.6 Gbps | Origin requests: 328700\n\n### Alerts\n- [WARNING] DISK-USAGE-WARN on metrics-db-01: disk_pct=80\n- [INFO] NODE-CLOCK-SKEW on gateway-02: skew_ms=180\n\n### Deployments & Changes\n- Deployed recommendations v1.9.0\n\n### Events\n- NTP sync run on gateway-02\n- A/B test 'checkout-v2' at 15% rollout\n- Updated WAF rule set (managed)\n- CDN cache purge for /assets/logo.svg\n\n### On-Call\n- Shift: S. Patel. 0 pages. 1 ticket (NTP-109 gateway-02). Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_011",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-18T10:30:00",
          "text": "## 2026-01-12 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 145320 | p50: 49.4ms p95: N/A p99: N/A | err: 0.93% (1359 errors) | success: 99.07%\n- /auth/audit/mfa_enrollment: REQUESTS 392 | p50: N/A p95: N/A p99: N/A | err: 10.4% (34 failures) | success: 89.6%\n- /auth/audit/authn_failures: REQUESTS 2382 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/token_ops: REQUESTS 244180 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/privileged_access: REQUESTS 669 | p50: 20.8min p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/rbac_changes: REQUESTS 386 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/audit_pipeline: REQUESTS 2124500 | p50: N/A p95: 16.8s p99: N/A | err: 0.016% (342 dropped) | success: 99.984%\n- /auth/audit/directory_sync: REQUESTS 24 | p50: 95s p95: N/A p99: N/A | err: 4.2% (1 failure) | success: 95.8%\n- /auth/audit/compliance_controls: REQUESTS 35 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/idp_health: REQUESTS 38 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/helpdesk: REQUESTS 668 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/rate_limits: REQUESTS 116 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/session_security: REQUESTS 149 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /auth/audit/evidence_exports: REQUESTS 40 | p50: N/A p95: N/A p99: N/A | err: 2.5% (1 failure) | success: 97.5%\n- /auth/audit/admin_approvals: REQUESTS 90 | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- idp-api-01: CPU 69% | Mem 79% | Disk 64% | Conns: 338 | Net: 116/124 Mbps\n- idp-api-02: CPU 65% | Mem 77% | Disk 62% | Conns: 334 | Net: 112/120 Mbps\n- audit-worker-01: CPU 83% | Mem 68% | Disk 47% | Conns: N/A | Net: 156/149 Mbps\n- audit-db-01: CPU 49% | Mem 83% | Disk 76% | IOPS: 12400 | Replication lag: 1.3 sec\n\n### Connection Pools\n- auth_pool: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 200 | avg_wait 15ms\n- sync_pool: active 15 | idle 5 | waiting 1 | exhaustion 0 | max 50 | avg_wait 10ms\n- token_pool: active 80 | idle 20 | waiting 3 | exhaustion 1 | max 150 | avg_wait 12ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12450\n\n### Alerts\n- [SEVERE] idp-api-01 CPU at 69%\n- [WARNING] audit-db-01 Disk at 76%\n- [INFO] audit-worker-01 CPU at 83%\n- [ERROR] idp-api-02 Memory at 77%\n- [SEVERE] audit-db-01 replication lag 1.3 sec\n\n### Deployments & Changes\n- Reverted misconfigured RBAC policy; restored admin access; documented incident.\n- Rotated LDAP bind credentials; verified directory sync.\n- Scaled audit workers; reduced ingest lag.\n- Notified partner of JWKS URI issue; tracked remediation.\n\n### Events\n- Break-glass account used to restore admin access after misconfigured RBAC policy blocked Security group; reverted policy and recorded incident narrative for auditors.\n- Directory sync run #22 failed; root cause was expired service credential for LDAP bind; rotated credential and verified subsequent sync.\n- Audit ingest lag spiked due to higher volume from role review exports; scaled workers and reduced drops to baseline.\n- Token rotation failures increased for one partner integration; confirmed they were using outdated JWKS URI; notified partner and tracked remediation.\n\n### On-Call\n- shift handoff note: T. Wilson. 5 pages. 16 tickets. Status: Degraded then recovered; break-glass documented and controls validated.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_012",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-18T10:30:00",
          "text": "## 2026-01-13 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 143110 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0.72% (101 errors) | success: 99.28%\n- /mfa_enrollment: REQUESTS 378 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 10.2% (18 errors) | success: 89.8%\n- /authn_failures: REQUESTS 1510 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 240760 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /privileged_access: REQUESTS 514 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 7 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2091200 req | p50: 9.4ms p95: 9.4ms p99: 9.4ms | err: 0.0084% (175 errors) | success: 99.9916%\n- /directory_sync: REQUESTS 24 req | p50: 84s p95: 84s p99: 84s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 14 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 3 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 349 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 88 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /session_security: REQUESTS 103 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 36 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 83 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 63% | Mem 76% | Disk 64% | Conns: 331 | Net: 114/121 Mbps\n- idp-api-02: CPU 60% | Mem 75% | Disk 62% | Conns: 326 | Net: 110/118 Mbps\n- audit-worker-01: CPU 71% | Mem 62% | Disk 47% | Conns: N/A | Net: 150/144 Mbps\n- audit-db-01: CPU 44% | Mem 81% | Disk 76% | IOPS: 11400 | Replication lag: 0.9 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (No specific pool data provided)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [SEVERITY] alert-name on hostname: value\n- (No alerts reported)\n\n### Deployments & Changes\n- Post-incident review for prior day RBAC misconfiguration; added change checklist requiring staged evaluation and peer review.\n- Issued updated partner integration guide including correct JWKS URI and rotation schedule; tracked acknowledgements.\n- Closed one audit exception after evidence of manager approval uploaded for elevated access request.\n- Validated LDAP bind credential rotation; confirmed no further directory sync failures.\n\n### Events\n- Post-incident review for prior day RBAC misconfiguration; added change checklist requiring staged evaluation and peer review.\n- Issued updated partner integration guide including correct JWKS URI and rotation schedule; tracked acknowledgements.\n- Closed one audit exception after evidence of manager approval uploaded for elevated access request.\n- Validated LDAP bind credential rotation; confirmed no further directory sync failures.\n\n### On-Call\n- Shift: H. Lee. 2 pages. 12 tickets. Status: Stable; corrective actions captured and partner comms sent.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_013",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-18T10:30:00",
          "text": "## 2026-01-14 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 141980 req | p50: 48.6ms p95: 0ms p99: 0ms | err: 0.67% (950 errors) | success: 99.33%\n- /auth/audit/mfa_enrollment: REQUESTS 362 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/authn_failures: REQUESTS 1472 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/token_ops: REQUESTS 238420 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.01% (25 failures) | success: 99.99%\n- /auth/audit/privileged_access: REQUESTS 507 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/rbac_changes: REQUESTS 6 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/audit_pipeline: REQUESTS 2074800 req | p50: 0ms p95: 9.0ms p99: 9.0ms | err: 0.008% (168 dropped) | success: 99.992%\n- /auth/audit/directory_sync: REQUESTS 24 req | p50: 81s p95: 81s p99: 81s | err: 0.0% (0 failures) | success: 100%\n- /auth/audit/compliance_controls: REQUESTS 13 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 open, 1 closed) | success: 100%\n- /auth/audit/idp_health: REQUESTS 2 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/helpdesk: REQUESTS 332 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 errors) | success: 100%\n- /auth/audit/rate_limits: REQUESTS 59 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 throttled) | success: 100%\n- /auth/audit/session_security: REQUESTS 95 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 events) | success: 100%\n- /auth/audit/evidence_exports: REQUESTS 23 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 failures) | success: 100%\n- /auth/audit/admin_approvals: REQUESTS 61 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0% (0 failures) | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 61% | Mem 75% | Disk 64% | Conns: 329 | Net: 112/119 Mbps\n- idp-api-02: CPU 58% | Mem 74% | Disk 62% | Conns: 324 | Net: 108/116 Mbps\n- audit-worker-01: CPU 69% | Mem 61% | Disk 47% | Conns: N/A | Net: 148/142 Mbps\n- audit-db-01: CPU 42% | Mem 80% | Disk 76% | IOPS: 11200 | Replication lag: 0.8 sec\n\n### Connection Pools\n- pool-auth: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 200 | avg_wait 15ms\n- pool-db: active 85 | idle 20 | waiting 3 | exhaustion 1 | max 150 | avg_wait 20ms\n- pool-cache: active 45 | idle 10 | waiting 0 | exhaustion 0 | max 60 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 15000\n\n### Alerts\n- [CRITICAL] idp-api-01 CPU at 61%\n- [WARNING] audit-db-01 Disk usage at 76%\n- [INFO] audit-worker-01 Net in at 148 Mbps\n- [ERROR] idp-api-02 Memory at 74%\n- [WARNING] audit-db-01 Replication lag at 0.8 sec\n\n### Deployments & Changes\n- Created 'Data-Analyst-Restricted' role; migrated 37 users; removed broad read permissions.\n- Provided evidence of MFA enforcement exceptions; confirmed 14 break-glass/service accounts exempt.\n- Updated SAML assertion error handling; corrected NTP settings on SP server.\n- Sampled privileged sessions; confirmed JIT elevation records for all 134 sudo requests.\n\n### Events\n- RBAC update: created separate 'Data-Analyst-Restricted' role; migrated 37 users and removed broad read permissions.\n- Auditor request: provided evidence of MFA enforcement exceptions list; confirmed only 14 break-glass/service accounts exempt.\n- Investigated 20 SAML assertion errors; root cause was clock skew on one SP server; app owner corrected NTP settings.\n- Performed sampling of privileged sessions to confirm JIT elevation records present for all 134 sudo requests.\n\n### On-Call\n- Shift: C. Johnson. 1 page. 10 tickets. Status: Stable; role migration completed and evidence delivered.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_005",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-19T10:00:00",
          "text": "## 2024-01-19 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: 126450 req | p50: 95ms p95: 218ms p99: 336ms | err: 0.13% (165 errors) | success: 99.87%\n- /fraud_check: 126120 req | p50: 69ms p95: 166ms p99: 252ms | err: 0.08% (107 errors) | success: 99.92%\n- /geo_lookup: 124500 req | p50: 56ms p95: 132ms p99: 184ms | err: 0.05% (63 errors) | success: 99.95%\n- /auth: 164200 req | p50: 28ms p95: 70ms p99: 112ms | err: 0.05% (79 errors) | success: 99.95%\n- /product_catalog: 220300 req | p50: 35ms p95: 91ms p99: 148ms | err: 0.05% (119 errors) | success: 99.95%\n- /search: 190100 req | p50: 50ms p95: 124ms p99: 198ms | err: 0.1% (182 errors) | success: 99.9%\n- /recommendations: 154600 req | p50: 53ms p95: 145ms p99: 230ms | err: 0.1% (160 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 37% | Mem 60% | Disk 61% | Conns: 1965 | Net: 460/552 Mbps\n- gateway-02: CPU 35% | Mem 58% | Disk 60% | Conns: 1878 | Net: 446/538 Mbps\n- service-b-01: CPU 32% | Mem 56% | Disk 57% | Conns: 1065 | Net: 232/262 Mbps\n- metrics-db-01: CPU 24% | Mem 66% | Disk 79% | Conns: 240 | Net: 95/118 Mbps\n\n### Connection Pools\n- primary: active 78 | idle 122 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 38 | idle 62 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 26 | idle 34 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.4% | Bandwidth: 6.9 Gbps | Origin requests: 339100\n\n### Alerts\n- [INFO] TLS-HANDSHAKE-ERRORS on gateway-01: count=42\n\n### Deployments & Changes\n- Config change: gateway rate-limit rules updated (no endpoint changes)\n\n### Events\n- Updated API gateway log sampling from 1% to 2% for /checkout\n- A/B test 'checkout-v2' at 15% rollout\n- Ran backup verification for metrics-db-01\n- CDN cache hit rate 94.4%\n\n### On-Call\n- shift handoff note: L. Nguyen. 0 pages. 0 tickets. Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_014",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-19T10:30:00",
          "text": "## 2026-01-15 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 139420 | p50: 48.1ms p95:  | p99:  | err: 0.71% (989 errors) | success: 99.29%\n- /mfa_enrollment: REQUESTS 345 | p50:  | p95:  | p99:  | err: 4.8% (15 failures) | success: 95.2%\n- /authn_failures: REQUESTS 1931 | invalid_password: 1440 | mfa_challenge_failed: 390 | account_locked: 101 | err: 0% | success: 100%\n- /token_ops: REQUESTS 292,810 | p50: 179ms p95:  | p99:  | err: 0.01% (23 failures) | success: 99.99%\n- /privileged_access: REQUESTS 627 | admin_logins: 498 | break_glass_uses: 0 | privileged_session_avg_min: 19.6 | sudo_requests: 129 | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 428 | policy_updates: 5 | role_grants: 179 | role_revokes: 182 | permission_change_requests: 64 | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2039000 | events_ingested: 2039000 | events_dropped: 160 | queue_depth_peak: 4250 | ingest_lag_sec_p95: 8.8 | err: 0.008% (160 dropped) | success: 99.992%\n- /directory_sync: REQUESTS 24 | sync_runs: 24 | objects_updated: 1580 | objects_deleted: 14 | sync_failures: 0 | avg_sync_duration_sec: 79 | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 31 | sox_controls_checked: 12 | soc2_controls_checked: 19 | exceptions_opened: 1 | exceptions_closed: 0 | err: 0% | success: 100%\n- /idp_health: REQUESTS 20 | oidc_discovery_errors: 2 | saml_assertion_errors: 18 | jwks_fetch_errors: 0 | metadata_refreshes: 3 | err: 0% | success: 100%\n- /helpdesk: REQUESTS 576 | access_requests: 319 | password_resets: 206 | mfa_resets: 51 | median_first_response_min: 19 | err: 0% | success: 100%\n- /rate_limits: REQUESTS 76 | login_rate_limited: 56 | token_endpoint_rate_limited: 20 | api_keys_throttled: 0 | err: 0% | success: 100%\n- /session_security: REQUESTS 102 | forced_reauth_events: 90 | anomalous_device_blocks: 12 | remembered_device_count: 30510 | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 30 | csv_exports: 22 | pdf_exports: 8 | export_failures: 0 | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 63 | pending_requests: 58 | auto_approved: 7 | rejected: 7 | avg_approval_time_min: 59 | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 60% | Mem 74% | Disk 64% | Conns: 328 | Net: 109/116 Mbps\n- idp-api-02: CPU 57% | Mem 73% | Disk 62% | Conns: 323 | Net: 106/113 Mbps\n- audit-worker-01: CPU 67% | Mem 60% | Disk 47% | Conns: 232 | Net: 145/140 Mbps\n- audit-db-01: CPU 41% | Mem 80% | Disk 76% | IOPS: 11050 | Replication lag: 0.8 sec\n\n### Connection Pools\n- pool-name: active 45 | idle 10 | waiting 2 | exhaustion: 0 | max: 50 | avg_wait: 12ms\n- pool-name: active 38 | idle 12 | waiting 1 | exhaustion: 0 | max: 50 | avg_wait: 9ms\n- pool-name: active 25 | idle 5 | waiting 0 | exhaustion: 0 | max: 30 | avg_wait: 7ms\n- pool-name: active 20 | idle 4 | waiting 0 | exhaustion: 0 | max: 25 | avg_wait: 6ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 15000\n\n### Alerts\n- [SEVERE] idp-api-01 CPU at 60%\n- [WARNING] audit-db-01 Disk at 76%\n- [INFO] audit-worker-01 Net in 145 Mbps\n- [SEVERE] idp-api-02 Memory at 73%\n- [WARNING] audit-db-01 IOPS at 11050\n- [INFO] audit-worker-01 CPU at 67%\n- [WARNING] audit-db-01 Disk at 76%\n- [SEVERE] idp-api-01 Disk at 64%\n- [WARNING] idp-api-02 CPU at 57%\n- [INFO] audit-worker-01 Memory at 60%\n\n### Deployments & Changes\n- Ran quarterly token inventory: identified 22 long-lived API keys; scheduled rotation and owner confirmation.\n- Refreshed IdP metadata for external partner SAML; tested assertions and logout behavior; no regressions.\n- Cleaned up stale role grants: removed 9 users still in 'Legacy-Reporting' after migration complete.\n- Opened audit exception for one late attestation on 'Finance-Approver' role; set due date and escalation path.\n\n### Events\n- Opened audit exception for one late attestation on 'Finance-Approver' role; set due date and escalation path.\n- Ran quarterly token inventory: identified 22 long-lived API keys; scheduled rotation and owner confirmation.\n- Refreshed IdP metadata for external partner SAML; tested assertions and logout behavior; no regressions.\n- Cleaned up stale role grants: removed 9 users still in 'Legacy-Reporting' after migration complete.\n\n### On-Call\n- Shift: N. Davis. 1 page. 9 tickets. Status: Stable; exception tracked and key rotation campaign initiated.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_015",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-19T10:30:00",
          "text": "## 2026-01-16 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 138060 | p50: 47.8ms p95: 0ms p99: 0ms | err: 0.63% (866 errors) | success: 99.37%\n- /mfa_enrollment: REQUESTS 332 | p50: 0ms p95: 0ms p99: 0ms | err: 9.6% (32 failures) | success: 90.4%\n- /authn_failures: REQUESTS 1378 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 232010 | p50: 0ms p95: 0ms p99: 0ms | err: 0.009% (21 failures) | success: 99.991%\n- /privileged_access: REQUESTS 492 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 409 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2018400 | p50: 0ms p95: 8.4ms p99: 0ms | err: 0.0075% (152 errors) | success: 99.9925%\n- /directory_sync: REQUESTS 24 | p50: 77s p95: 77s p99: 77s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 29 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 21 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 553 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 71 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /session_security: REQUESTS 97 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 27 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 69 | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 58% | Mem 73% | Disk 64% | Conns: 325 | Net: 107/114 Mbps\n- idp-api-02: CPU 55% | Mem 72% | Disk 62% | Conns: 321 | Net: 104/111 Mbps\n- audit-worker-01: CPU 65% | Mem 59% | Disk 47% | Conns: 230 | Net: 143/138 Mbps\n- audit-db-01: CPU 40% | Mem 79% | Disk 76% | IOPS: 10900 | Replication lag: 0.7 sec\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 0% | Bandwidth: 0 Gbps | Origin requests: 0\n\n### Alerts\n- [INFO] idp_health: 1 oidc_discovery_error\n- [WARNING] idp_health: 17 saml_assertion_errors\n\n### Deployments & Changes\n- Issued rotation reminders for 22 API keys; 8 owners confirmed schedule and 2 rotated same day\n- Reviewed audit drops; small number tied to oversized payloads from custom app; requested app team trim attributes\n- Completed spot-check of role revokes for offboarding list; matched all 13 directory deletions\n- Closed prior day 'Finance-Approver' attestation exception after manager completed review; attached signed acknowledgment\n\n### Events\n- Closed prior day 'Finance-Approver' attestation exception after manager completed review; attached signed acknowledgment\n- Issued rotation reminders for 22 API keys; 8 owners confirmed schedule and 2 rotated same day\n- Reviewed audit drops; small number tied to oversized payloads from custom app; requested app team trim attributes\n- Completed spot-check of role revokes for offboarding list; matched all 13 directory deletions\n\n### On-Call\n- Shift: V. Ahmed. 0 pages. 8 tickets. Status: Stable; key rotation outreach underway and drops investigated.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_016",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-19T10:30:00",
          "text": "## 2026-01-17 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 110540 | p50: 42.4ms p95:  | p99:  | err: 0.45% (496 errors) | success: 99.55%\n- /auth/audit/mfa_enrollment: REQUESTS 221 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/authn_failures: REQUESTS 1421 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/token_ops: REQUESTS 185820 | p50:  | p95:  | p99:  | err: 0.006% (12 failures) | success: \n- /auth/audit/privileged_access: REQUESTS 471 | p50: 17.1min | p95:  | p99:  | err:  | success: \n- /auth/audit/rbac_changes: REQUESTS 285 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/audit_pipeline: REQUESTS 1609200 | p50:  | p95: 5.9sec | p99:  | err: 0.006% (92 drops) | success: \n- /auth/audit/directory_sync: REQUESTS 24 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/compliance_controls: REQUESTS 19 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/idp_health: REQUESTS 2 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/helpdesk: REQUESTS 363 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/rate_limits: REQUESTS 33 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/session_security: REQUESTS 43 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/evidence_exports: REQUESTS 12 | p50:  | p95:  | p99:  | err:  | success: \n- /auth/audit/admin_approvals: REQUESTS 36 | p50:  | p95:  | p99:  | err:  | success: \n\n### Infrastructure\n- idp-api-01: CPU 42% | Mem 66% | Disk 64% | Conns: 294 | Net: 73/78 Mbps\n- idp-api-02: CPU 40% | Mem 65% | Disk 62% | Conns: 291 | Net: 71/76 Mbps\n- audit-worker-01: CPU 51% | Mem 54% | Disk 47% | Conns: 209 | Net: 106/102 Mbps\n- audit-db-01: CPU 30% | Mem 74% | Disk 76% | IOPS 8100 | Replication lag 0.4 sec\n\n### Connection Pools\n- auth_pool: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- db_pool: active 50 | idle 10 | waiting 2 | exhaustion 0 | max 100 | avg_wait 8ms\n- cache_pool: active 15 | idle 5 | waiting 1 | exhaustion 0 | max 20 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 85% | Bandwidth: 2.3 Gbps | Origin requests: 1500\n\n### Alerts\n- [WARNING] high CPU usage on idp-api-01: 42%\n- [CRITICAL] disk space nearing capacity on audit-db-01: 76%\n- [INFO] network throughput on audit-worker-01: 106/102 Mbps\n\n### Deployments & Changes\n- Rotated signing key for internal test tenant; validated OIDC discovery and token verification in staging.\n- No new deployments today.\n\n### Events\n- Weekend maintenance: rotated signing key for internal test tenant; validated OIDC discovery and token verification in staging.\n- Processed small batch of access requests for on-call engineers; ensured time-bound JIT roles expire in 8 hours.\n- Reviewed helpdesk MFA reset procedures; confirmed identity proofing checklist consistently attached.\n- Verified audit pipeline retention job completed; no backlog and drops under threshold.\n\n### On-Call\n- Shift: G. Park. 0 pages. 4 tickets. Status: Quiet; maintenance rotation validated and JIT access controlled.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_006",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-20T10:00:00",
          "text": "## 2024-01-20 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 131200 | p50: 96ms p95: 220ms p99: 340ms | err: 0.13% (170 errors) | success: 99.87%\n- /fraud_check: REQUESTS 130850 | p50: 69ms p95: 168ms p99: 255ms | err: 0.08% (110 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 129100 | p50: 56ms p95: 134ms p99: 186ms | err: 0.05% (65 errors) | success: 99.95%\n- /auth: REQUESTS 170900 | p50: 29ms p95: 71ms p99: 114ms | err: 0.05% (83 errors) | success: 99.95%\n- /product_catalog: REQUESTS 229700 | p50: 36ms p95: 92ms p99: 150ms | err: 0.05% (123 errors) | success: 99.95%\n- /search: REQUESTS 197600 | p50: 51ms p95: 126ms p99: 202ms | err: 0.1% (190 errors) | success: 99.9%\n- /recommendations: REQUESTS 160900 | p50: 54ms p95: 146ms p99: 232ms | err: 0.1% (167 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 39% | Mem 60% | Disk 61% | Conns: 2055 | Net: 488/580 Mbps\n- gateway-02: CPU 36% | Mem 58% | Disk 60% | Conns: 1952 | Net: 470/560 Mbps\n- service-b-01: CPU 33% | Mem 56% | Disk 57% | Conns: 1120 | Net: 245/276 Mbps\n- metrics-db-01: CPU 26% | Mem 67% | Disk 80% | Conns: 248 | Net: 100/125 Mbps\n\n### Connection Pools\n- primary: active 82 | idle 118 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 41 | idle 59 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 28 | idle 32 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 93.9% | Bandwidth: 7.1 Gbps | Origin requests: 352600\n\n### Alerts\n- [WARNING] DISK-USAGE-WARN on metrics-db-01: disk_pct=80\n\n### Deployments & Changes\n- Deployed gateway-config v1.12.4\n\n### Events\n- Weekend promo banner enabled on homepage\n- A/B test 'checkout-v2' at 15% rollout\n- Scaled search-service from 6 to 7 instances\n- Rotated CDN origin auth token\n\n### On-Call\n- Shift: M. Rossi. 0 pages. 1 ticket (DISK-4512 metrics-db cleanup). Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_017",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-20T10:30:00",
          "text": "## 2026-01-18 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 108760 req | p50: 41.9ms p95: 0ms p99: 0ms | err: 0.48% (522 errors) | success: 99.52%\n- /auth/audit/mfa_enrollment: REQUESTS 214 req | p50: 0ms p95: 0ms p99: 0ms | err: 9.4% (19 failures) | success: 90.6%\n- /auth/audit/authn_failures: REQUESTS 1098 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/token_ops: REQUESTS 183260 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.0071% (13 failures) | success: 99.9929%\n- /auth/audit/privileged_access: REQUESTS 379 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/rbac_changes: REQUESTS 3 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/audit_pipeline: REQUESTS 1587400 req | p50: 0ms p95: 6.1ms p99: 0ms | err: 0.006% (96 dropped) | success: 99.994%\n- /auth/audit/directory_sync: REQUESTS 24 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/compliance_controls: REQUESTS 7 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/idp_health: REQUESTS 2 req | p50: 0ms p95: 0ms p99: 0ms | err: 0.5% (1 error) | success: 99.5%\n- /auth/audit/helpdesk: REQUESTS 205 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/rate_limits: REQUESTS 34 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/session_security: REQUESTS 40 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/evidence_exports: REQUESTS 13 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n- /auth/audit/admin_approvals: REQUESTS 33 req | p50: 0ms p95: 0ms p99: 0ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 43% | Mem 66% | Disk 64% | Conns: 295 | Net: 72/77 Mbps\n- idp-api-02: CPU 41% | Mem 65% | Disk 62% | Conns: 292 | Net: 70/75 Mbps\n- audit-worker-01: CPU 52% | Mem 54% | Disk 47% | Conns: 209 | Net: 107/103 Mbps\n- audit-db-01: CPU 31% | Mem 74% | Disk 76% | IOPS: 8200 | Replication lag: 0.4 sec\n\n### Connection Pools\n- default-pool: active 120 | idle 30 | waiting 5 | exhaustion: 0 | max: 200 | avg_wait: 12ms\n- auth-pool: active 80 | idle 20 | waiting 2 | exhaustion: 0 | max: 150 | avg_wait: 8ms\n- session-pool: active 50 | idle 10 | waiting 1 | exhaustion: 0 | max: 100 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 85% | Bandwidth: 2.3 Gbps | Origin requests: 15000\n\n### Alerts\n- [WARNING] high memory usage on idp-api-01: 66%\n- [CRITICAL] disk space nearing capacity on audit-db-01: 76%\n- [INFO] network throughput on audit-worker-01: in 107 Mbps, out 103 Mbps\n\n### Deployments & Changes\n- Deployed new MFA enrollment feature; 214 enrolled today, 9 failures.\n- Updated role grant policies; 120 role grants, 119 revokes, 41 permission changes.\n- Scheduled SAML metadata refresh; next window aligned with partner maintenance.\n- Performed audit sampling on role grants; verified ticket references for 25 changes.\n- Reviewed outstanding API key rotation campaign; 4 confirmations received, 10 pending.\n\n### Events\n- Reviewed outstanding API key rotation campaign: received 4 confirmations; tracked 10 still pending owner response.\n- Performed audit sampling on role grants: verified ticket references included for 25 of 25 sampled changes.\n- Investigated mild increase in account locks; traced to repeated login attempts after password resets; updated helpdesk script.\n- Validated SAML metadata refresh schedule; next refresh window aligned with partner maintenance.\n\n### On-Call\n- Shift: F. Rossi. 0 pages. 5 tickets. Status: Stable; key rotation follow-ups and process improvements logged.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_018",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-20T10:30:00",
          "text": "## 2026-01-19 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 146880 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0.78% (1149 errors) | success: 99.22%\n- /mfa_enrollment: REQUESTS 401 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 9.1% (20 failures) | success: 90.9%\n- /authn_failures: REQUESTS 1588 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 246900 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0.012% (29 failures) | success: 99.988%\n- /privileged_access: REQUESTS 533 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 8 | p50: 64ms p95: 64ms p99: 64ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2149800 | p50: 10.1s p95: 10.1s p99: 10.1s | err: 0.009% (190 dropped) | success: 99.991%\n- /directory_sync: REQUESTS 24 | p50: 86s p95: 86s p99: 86s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 14 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 3 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 372 | p50: 23min p95: 23min p99: 23min | err: 0% | success: 100%\n- /rate_limits: REQUESTS 66 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /session_security: REQUESTS 109 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 37 | p50: 49.8ms p95: 49.8ms p99: 49.8ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 89 | p50: 70min p95: 70min p99: 70min | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 64% | Mem 77% | Disk 65% | Conns: 336 | Net: 118/126 Mbps\n- idp-api-02: CPU 61% | Mem 76% | Disk 63% | Conns: 332 | Net: 115/123 Mbps\n- audit-worker-01: CPU 73% | Mem 63% | Disk 48% | Conns: 239 | Net: 158/151 Mbps\n- audit-db-01: CPU 45% | Mem 82% | Disk 77% | IOPS: 11600 | Replication lag: 1.0 sec\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 0% | Bandwidth: 0 Gbps | Origin requests: 0\n\n### Alerts\n- [INFO] audit pipeline queue depth peak: 4760\n- [WARNING] idp_health: saml_assertion_errors 23\n- [ERROR] idp_health: oidc_discovery_errors 3\n- [INFO] audit pipeline ingest lag p95: 10.1 sec\n- [INFO] directory_sync: sync failures 0\n- [INFO] directory_sync: objects updated 1980\n- [INFO] directory_sync: objects deleted 18\n- [INFO] audit pipeline events ingested: 2149800\n- [INFO] audit pipeline events dropped: 190\n- [INFO] audit pipeline: ingest lag sec p95: 10.1\n- [INFO] audit pipeline: queue depth peak: 4760\n- [INFO] audit pipeline: events ingested: 2149800\n- [INFO] audit pipeline: events dropped: 190\n\n### Deployments & Changes\n- Quarterly audit evidence provided for token rotation policy enforcement; confirmed 0 permanent exceptions.\n- Exception opened and closed same day after security approval for urgent role grant.\n- Large RBAC changes: revoked 63 roles, issued 71 new roles.\n- Helpdesk MFA reset spike following mobile OS update; resets logged and monitored.\n\n### Events\n- Quarterly auditor request: evidence of token rotation policy enforcement and exception list; confirmed 0 permanent exceptions.\n- Opened/closed one exception same day after security approval documented for urgent role grant.\n- Large RBAC changes for org restructure: revoked 63 roles for moved teams; issued 71 new least-privilege roles.\n- Helpdesk spike for MFA resets following mobile OS update; ensured reset approvals logged and monitored for abuse patterns.\n\n### On-Call\n- Shift: B. Thompson. 2 pages. 15 tickets. Status: Stable; restructure changes executed with evidence and approvals captured.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_019",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-20T10:30:00",
          "text": "## 2026-01-20 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 148220 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0.74% (1100 errors) | success: 99.26%\n- /mfa_enrollment: REQUESTS 412 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 8.9% (19 errors) | success: 91.1%\n- /authn_failures: REQUESTS 1560 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 249110 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0.011% (27 failures) | success: 99.989%\n- /privileged_access: REQUESTS 540 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 7 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2167400 req | p50: 9.9s p95: 9.9s p99: 9.9s | err: 0.0084% (182 dropped) | success: 99.9916%\n- /directory_sync: REQUESTS 24 req | p50: 87s p95: 87s p99: 87s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 14 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 3 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 381 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 90 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /session_security: REQUESTS 111 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 38 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 87 req | p50: 50.1ms p95: 50.1ms p99: 50.1ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 65% | Mem 77% | Disk 65% | Conns: 337 | Net: 120/128 Mbps\n- idp-api-02: CPU 62% | Mem 76% | Disk 63% | Conns: 333 | Net: 117/125 Mbps\n- audit-worker-01: CPU 74% | Mem 64% | Disk 48% | Conns: N/A | Net: 160/153 Mbps\n- audit-db-01: CPU 46% | Mem 82% | Disk 77% | IOPS: 11750 | Replication lag: 1.0 sec\n\n### Connection Pools\n- pool-name: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-name: auth_pool: active 80 | idle 20 | waiting 3 | exhaustion: 1 | max: 100 | avg_wait: 8ms\n- pool-name: token_pool: active 200 | idle 50 | waiting 10 | exhaustion: 4 | max: 250 | avg_wait: 15ms\n- pool-name: rbac_pool: active 50 | idle 10 | waiting 2 | exhaustion: 0 | max: 60 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 85% | Bandwidth: 3.2 Gbps | Origin requests: 15000\n\n### Alerts\n- [SEVERE] idp-api-01 CPU usage high: 65%\n- [WARNING] audit-db-01 disk utilization: 77%\n- [INFO] audit-worker-01 network throughput: 160 Mbps inbound\n- [SEVERE] idp-api-02 memory usage: 76%\n- [WARNING] audit-db-01 replication lag: 1.0 sec\n\n### Deployments & Changes\n- Updated RBAC mapping table; ensured 100% removal of users from old roles.\n- Implemented mobile OS MFA reset procedures; verified 66 resets with supervisor approval.\n- Ran audit reconciliation; identified 4 missing ticket links, follow-ups opened.\n- Verified audit pipeline lag; maintained under 10s p95 with current worker count.\n\n### Events\n- Continued org restructure: updated RBAC mapping table; ensured 100% of moved users removed from old cost-center roles.\n- Reviewed helpdesk MFA resets for the mobile OS update wave; confirmed 66 resets had supervisor approval and ID verification notes.\n- Ran audit reconciliation: compared role grant logs vs ticketing system; 4 missing ticket links, opened follow-ups.\n- Verified audit pipeline lag after high-volume day; remained under 10s p95 with current worker count.\n\n### On-Call\n- shift handoff note: S. Martinez. 1 page. 13 tickets. Status: Stable; reconciliation follow-ups opened and restructure tracked.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_007",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-21T10:00:00",
          "text": "## 2024-01-21 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 128900 | p50: 95ms p95: 218ms p99: 338ms | err: 0.13% (166 errors) | success: 99.87%\n- /fraud_check: REQUESTS 128600 | p50: 69ms p95: 167ms p99: 254ms | err: 0.08% (108 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 126900 | p50: 56ms p95: 133ms p99: 185ms | err: 0.05% (64 errors) | success: 99.95%\n- /auth: REQUESTS 168100 | p50: 28ms p95: 70ms p99: 113ms | err: 0.05% (80 errors) | success: 99.95%\n- /product_catalog: REQUESTS 226300 | p50: 36ms p95: 92ms p99: 149ms | err: 0.05% (121 errors) | success: 99.95%\n- /search: REQUESTS 194200 | p50: 51ms p95: 125ms p99: 200ms | err: 0.1% (188 errors) | success: 99.9%\n- /recommendations: REQUESTS 158400 | p50: 54ms p95: 145ms p99: 230ms | err: 0.1% (165 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 38% | Mem 60% | Disk 61% | Conns: 2010 | Net: 476/568 Mbps\n- gateway-02: CPU 35% | Mem 58% | Disk 60% | Conns: 1915 | Net: 462/552 Mbps\n- service-b-01: CPU 33% | Mem 56% | Disk 57% | Conns: 1095 | Net: 238/270 Mbps\n- metrics-db-01: CPU 24% | Mem 67% | Disk 79% | Conns: 245 | Net: 97/122 Mbps\n\n### Connection Pools\n- primary: active 80 | idle 120 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 40 | idle 60 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 27 | idle 33 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.0% | Bandwidth: 6.8 Gbps | Origin requests: 341800\n\n### Alerts\n- [INFO] POD-RESTART on service-b-01: restarts=2\n\n### Deployments & Changes\n- Deployed fraud-ruleset v2024.01.21\n\n### Events\n- Kubernetes node drain in pool 'workers-a'\n- A/B test 'checkout-v2' at 15% rollout\n- Updated synthetic checks interval from 60s to 45s\n- CDN cache purge for /promo/banner.jpg\n\n### On-Call\n- shift handoff note: E. Johnson. 0 pages. 0 tickets. Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_020",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-21T10:30:00",
          "text": "## 2026-01-21 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 147060 | p50: 50ms p95: 120ms p99: 200ms | err: 0.69% (1019 errors) | success: 99.31%\n- /mfa_enrollment: REQUESTS 398 | p50: 30ms p95: 70ms p99: 150ms | err: 8.7% (17 failures) | success: 91.3%\n- /authn_failures: REQUESTS 2496 | p50: 40ms p95: 90ms p99: 180ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 308035 | p50: 45ms p95: 100ms p99: 210ms | err: 0.008% (25 failures) | success: 99.992%\n- /privileged_access: REQUESTS 675 | p50: 60ms p95: 130ms p99: 250ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 413 | p50: 55ms p95: 125ms p99: 230ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2151000 | p50: 9ms p95: 15ms p99: 25ms | err: 0.008% (176 errors) | success: 99.992%\n- /directory_sync: REQUESTS 24 | p50: 85s p95: 85s p99: 85s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 33 | p50: 40ms p95: 80ms p99: 160ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 23 | p50: 35ms p95: 75ms p99: 150ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 644 | p50: 25ms p95: 60ms p99: 130ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 84 | p50: 20ms p95: 50ms p99: 100ms | err: 0% | success: 100%\n- /session_security: REQUESTS 121 | p50: 45ms p95: 95ms p99: 190ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 37 | p50: 30ms p95: 70ms p99: 140ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 86 | p50: 60ms p95: 130ms p99: 250ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 63% | Mem 76% | Disk 65% | Conns: 335 | Net: 118/126 Mbps\n- idp-api-02: CPU 60% | Mem 75% | Disk 63% | Conns: 331 | Net: 115/123 Mbps\n- audit-worker-01: CPU 72% | Mem 63% | Disk 48% | Conns: 238 | Net: 158/151 Mbps\n- audit-db-01: CPU 44% | Mem 81% | Disk 77% | IOPS: 11500 | Replication lag: 0.9 sec\n\n### Connection Pools\n- pool-name: active 12 | idle 3 | waiting 0 | exhaustion 0 | max 15 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 2.3 Gbps | Origin requests: 12450\n\n### Alerts\n- [SEVERITY] high CPU usage on idp-api-01: 63%\n- [SEVERITY] high CPU usage on audit-worker-01: 72%\n- [SEVERITY] disk utilization on audit-db-01: 77%\n- [SEVERITY] network bandwidth saturation on idp-api-02: 123 Mbps out\n- [SEVERITY] audit pipeline queue peak: 4590 events\n- [SEVERITY] audit pipeline ingest lag: 9.5 sec\n- [SEVERITY] idp discovery errors: 2\n- [SEVERITY] saml assertion errors: 21\n- [SEVERITY] idp metadata refreshes: 4\n- [SEVERITY] audit exception opened for role grants missing ticket linkage\n- [SEVERITY] token rotation schedule deviations detected\n- [SEVERITY] org restructure RBAC evidence delivered\n\n### Deployments & Changes\n- Implemented enforcement in access request form to require ticket ID for privileged roles; change recorded as control improvement.\n- Delivered updated org restructure RBAC mapping evidence to auditors with before/after role counts.\n- Reviewed token rotation schedule adherence; 3 internal services outside window; coordinated change freeze-safe plan.\n- Opened audit exception for 4 role grants missing ticket linkage; owners given 48-hour SLA to provide references or revoke.\n\n### Events\n- Opened audit exception for 4 role grants missing ticket linkage; owners given 48-hour SLA to provide references or revoke.\n- Implemented enforcement in access request form to require ticket ID for privileged roles; change recorded as control improvement.\n- Reviewed token rotation schedule adherence; 3 internal services still outside window; coordinated change freeze-safe plan.\n- Delivered updated org restructure RBAC mapping evidence to auditors with before/after role counts.\n\n### On-Call\n- shift: Y. Okafor. 2 pages. 12 tickets. Status: Stable; control enhancement deployed and exception tracking active.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_021",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-21T10:30:00",
          "text": "## 2026-01-22 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: 144920 req | p50: 49.2ms p95: N/A p99: N/A | err: 0.64% (592 errors) | success: 99.36%\n- /mfa_enrollment: 382 req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /authn_failures: 1420 invalid_password | 389 mfa_challenge_failed | 103 account_locked | 0 suspicious_impossible_travel_flags | N/A | err: N/A | success: N/A\n- /token_ops: 243520 access_tokens | 59610 refresh_tokens | 13440 rotations | 23 rotation_failures | avg: 179ms | err: N/A | success: N/A\n- /privileged_access: 520 admin_logins | 0 break_glass | 20 privileged_session_avg_min | 139 sudo_requests | N/A | err: N/A | success: N/A\n- /rbac_changes: 5 policy_updates | 202 role_grants | 190 role_revokes | 72 permission_change_requests | avg: 61min SLA | err: N/A | success: N/A\n- /audit_pipeline: 2122200 events ingested | 168 dropped | queue_peak: 4460 | ingest_lag_p95: 9.1s | N/A | err: N/A | success: N/A\n- /directory_sync: 24 sync_runs | 1810 objects_updated | 15 objects_deleted | 0 sync_failures | avg_sync_duration: 83s | N/A | err: N/A | success: N/A\n- /compliance_controls: 13 sox_controls | 19 soc2_controls | 0 exceptions_opened | 1 exception_closed | N/A | err: N/A | success: N/A\n- /idp_health: 2 oidc_discovery_errors | 19 saml_assertion_errors | 0 jwks_fetch_errors | 3 metadata_refreshes | N/A | err: N/A | success: N/A\n- /helpdesk: 351 access_requests | 207 password_resets | 56 mfa_resets | median_response: 20min | N/A | err: N/A | success: N/A\n- /rate_limits: 58 login_rate_limited | 20 token_endpoint_rate_limited | 0 api_keys_throttled | N/A | err: N/A | success: N/A\n- /session_security: 98 forced_reauth_events | 14 anomalous_device_blocks | 31480 remembered_devices | N/A | err: N/A | success: N/A\n- /evidence_exports: 25 csv_exports | 9 pdf_exports | 0 export_failures | N/A | err: N/A | success: N/A\n- /admin_approvals: 66 pending_requests | 6 auto_approved | 10 rejected | avg_approval_time: 66min | N/A | err: N/A | success: N/A\n\n### Infrastructure\n- idp-api-01: CPU 61% | Mem 75% | Disk 65% | Conns: 333 | Net: 116/124 Mbps\n- idp-api-02: CPU 58% | Mem 74% | Disk 63% | Conns: 329 | Net: 113/121 Mbps\n- audit-worker-01: CPU 70% | Mem 62% | Disk 48% | Conns: N/A | Net: 155/148 Mbps\n- audit-db-01: CPU 43% | Mem 81% | Disk 77% | IOPS: 11350 | Replication lag: 0.9s\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n(Note: Specific pool data not provided in data sheet)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [SEVERITY] oidc_discovery_errors on idp_health: 2\n- [SEVERITY] saml_assertion_errors on idp_health: 19\n\n### Deployments & Changes\n- Updated RBAC policy documentation to include new ticket-ID enforcement; stored signed change control approval.\n- Verified helpdesk median response improved after adding canned responses for common MFA reset cases.\n\n### Events\n- Closed 1 of 4 missing-ticket exceptions after owner supplied change record; remaining tracked with due dates.\n- Investigated 19 SAML assertion errors; identified one app sending incorrect Audience; app owner scheduled fix.\n- Verified helpdesk median response improved after adding canned responses for common MFA reset cases.\n\n### On-Call\n- shift handoff note: Shift: I. Petrova. 1 page. 11 tickets. Status: Stable; exceptions partially resolved and partner app issue tracked.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_022",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-21T10:30:00",
          "text": "## 2026-01-23 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 142310 req | p50: 48.7ms p95: N/A p99: N/A | err: 0.59% (842 errors) | success: 99.41%\n- /mfa_enrollment: REQUESTS N/A | enrolled_today: 371 | total_enrolled_pct: 91.7% | sms_pct: 19.0% | totp_pct: 66.5% | webauthn_pct: 14.5% | enrollment_failures: 14\n- /authn_failures: invalid_password: 1388 | mfa_challenge_failed: 376 | account_locked: 98 | suspicious_impossible_travel_flags: 0\n- /token_ops: access_tokens_issued: 239870 | refresh_tokens_issued: 58840 | token_rotations: 13210 | rotation_failures: 21 | avg_rotation_time_ms: 177\n- /privileged_access: admin_logins: 508 | break_glass_uses: 0 | privileged_session_avg_min: 19.7 | sudo_requests: 135\n- /rbac_changes: policy_updates: 4 | role_grants: 190 | role_revokes: 192 | permission_change_requests: 69 | approval_sla_min_p50: 59\n- /audit_pipeline: events_ingested: 2090600 | events_dropped: 160 | queue_depth_peak: 4320 | ingest_lag_sec_p95: 8.7\n- /directory_sync: sync_runs: 24 | objects_updated: 1700 | objects_deleted: 14 | sync_failures: 0 | avg_sync_duration_sec: 81\n- /compliance_controls: sox_controls_checked: 12 | soc2_controls_checked: 18 | exceptions_opened: 0 | exceptions_closed: 1\n- /idp_health: oidc_discovery_errors: 1 | saml_assertion_errors: 18 | jwks_fetch_errors: 0 | metadata_refreshes: 3\n- /helpdesk: access_requests: 336 | password_resets: 198 | mfa_resets: 52 | median_first_response_min: 19\n- /rate_limits: login_rate_limited: 55 | token_endpoint_rate_limited: 19 | api_keys_throttled: 0\n- /session_security: forced_reauth_events: 92 | anomalous_device_blocks: 13 | remembered_device_count: 31640\n- /evidence_exports: csv_exports: 23 | pdf_exports: 8 | export_failures: 0\n- /admin_approvals: pending_requests: 62 | auto_approved: 7 | rejected: 9 | avg_approval_time_min: 64\n\n### Infrastructure\n- idp-api-01: CPU 59% | Mem 74% | Disk 65% | Conns: N/A | Net: 113/120 Mbps\n- idp-api-02: CPU 56% | Mem 73% | Disk 63% | Conns: N/A | Net: 110/118 Mbps\n- audit-worker-01: CPU 68% | Mem 61% | Disk 48% | Conns: N/A | Net: 152/146 Mbps\n- audit-db-01: CPU 41% | Mem 80% | Disk 77% | IOPS: 11100 | Replication lag: 0.8 sec\n\n### Connection Pools\n- pool-name: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [INFO] audit pipeline queue depth peak on audit-db-01: 4320\n- [WARNING] IDP SAML assertion errors on IDP: 18\n- [ERROR] IDP OIDC discovery error on IDP: 1\n\n### Deployments & Changes\n- Prepared weekly dashboard for auditors: MFA coverage 91.7%, break-glass uses 0 this week, open exceptions 2\n- Validated token rotation KPIs after app teams responded; rotation failures trending downward\n- Coordinated with app owner on SAML Audience mismatch; collected sample assertions and provided required EntityID\n- Closed another missing-ticket exception after revoke executed; evidence includes revoke timestamp and approver notes\n\n### Events\n- Closed another missing-ticket exception after revoke executed; evidence includes revoke timestamp and approver notes\n- Prepared weekly dashboard for auditors: MFA coverage 91.7%, break-glass uses 0 this week, open exceptions 2\n- Coordinated with app owner on SAML Audience mismatch; collected sample assertions and provided required EntityID\n- Validated token rotation KPIs after app teams responded; rotation failures trending downward\n\n### On-Call\n- Shift: R. Miller. 1 page. 9 tickets. Status: Stable; exceptions reduced and audit dashboard delivered.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_008",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-22T10:00:00",
          "text": "## 2024-01-22 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 120500 | p50: 92ms p95: 212ms p99: 328ms | err: 0.12% (150 errors) | success: 99.88%\n- /fraud_check: REQUESTS 120200 | p50: 68ms p95: 163ms p99: 248ms | err: 0.08% (100 errors) | success: 99.92%\n- /geo_lookup: REQUESTS 118700 | p50: 55ms p95: 132ms p99: 182ms | err: 0.05% (60 errors) | success: 99.95%\n- /auth: REQUESTS 158200 | p50: 28ms p95: 69ms p99: 112ms | err: 0.05% (74 errors) | success: 99.95%\n- /product_catalog: REQUESTS 210600 | p50: 35ms p95: 90ms p99: 146ms | err: 0.05% (113 errors) | success: 99.95%\n- /search: REQUESTS 181900 | p50: 49ms p95: 122ms p99: 196ms | err: 0.1% (174 errors) | success: 99.9%\n- /recommendations: REQUESTS 150800 | p50: 52ms p95: 142ms p99: 226ms | err: 0.1% (152 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 35% | Mem 59% | Disk 61% | Conns: 1900 | Net: 445/535 Mbps\n- gateway-02: CPU 33% | Mem 57% | Disk 60% | Conns: 1820 | Net: 430/520 Mbps\n- service-b-01: CPU 31% | Mem 55% | Disk 57% | Conns: 1040 | Net: 225/255 Mbps\n- metrics-db-01: CPU 23% | Mem 66% | Disk 78% | Conns: 238 | Net: 92/116 Mbps\n\n### Connection Pools\n- primary: active 74 | idle 126 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 2ms\n- replica: active 36 | idle 64 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 25 | idle 35 | waiting 0 | exhaustion: 0 | max: 60 | avg_wait: 3ms\n\n### CDN & Caching\n- Hit rate: 94.2% | Bandwidth: 6.3 Gbps | Origin requests: 315200\n\n### Alerts\n- [INFO] CERT-RENEWAL-SCHEDULED on gateway-01: api.example.com renewal queued\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- SSL cert renewal job queued for api.example.com\n- A/B test 'checkout-v2' at 15% rollout\n- Metrics retention policy set to 21d for high-cardinality series\n- CDN cache hit rate 94.2%\n\n### On-Call\n- shift handoff note: R. Alvarez. 0 pages. 0 tickets. Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "baseline",
            "signal_density": "none"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_023",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-22T10:30:00",
          "text": "## 2026-01-24 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/audit/sso_sessions: REQUESTS 113120 | p50: 43ms p95: 78ms p99: 102ms | err: 0.43% (487 errors) | success: 99.57%\n- /auth/audit/mfa_enrollment: REQUESTS 236 | p50: 55ms p95: 89ms p99: 112ms | err: 2.54% (7 failures) | success: 97.46%\n- /auth/audit/authn_failures: REQUESTS 1345 | p50: 60ms p95: 95ms p99: 120ms | err: 0.00% | success: 100%\n- /auth/audit/token_ops: REQUESTS 240650 | p50: 50ms p95: 85ms p99: 110ms | err: 0.00% | success: 100%\n- /auth/audit/privileged_access: REQUESTS 477 | p50: 45ms p95: 80ms p99: 105ms | err: 0.00% | success: 100%\n- /auth/audit/rbac_changes: REQUESTS 286 | p50: 37ms p95: 70ms p99: 95ms | err: 0.00% | success: 100%\n- /auth/audit/audit_pipeline: REQUESTS 1634800 | p50: 55ms p95: 95ms p99: 130ms | err: 0.0054% (88 dropped) | success: 99.9946%\n- /auth/audit/directory_sync: REQUESTS 24 | p50: 65s p95: 65s p99: 65s | err: 0.00% | success: 100%\n- /auth/audit/compliance_controls: REQUESTS 19 | p50: 40ms p95: 75ms p99: 100ms | err: 0.00% | success: 100%\n- /auth/audit/idp_health: REQUESTS 9 | p50: 30ms p95: 50ms p99: 70ms | err: 0.00% | success: 100%\n- /auth/audit/helpdesk: REQUESTS 206 | p50: 12min p95: 20min p99: 25min | err: 0.00% | success: 100%\n- /auth/audit/rate_limits: REQUESTS 30 | p50: 35ms p95: 60ms p99: 80ms | err: 0.00% | success: 100%\n- /auth/audit/session_security: REQUESTS 36 | p50: 40ms p95: 70ms p99: 90ms | err: 0.00% | success: 100%\n- /auth/audit/evidence_exports: REQUESTS 11 | p50: 25ms p95: 45ms p99: 60ms | err: 0.00% | success: 100%\n- /auth/audit/admin_approvals: REQUESTS 34 | p50: 37ms p95: 65ms p99: 85ms | err: 0.00% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 41% | Mem 66% | Disk 65% | Conns: 294 | Net: 75/80 Mbps\n- idp-api-02: CPU 39% | Mem 65% | Disk 63% | Conns: 291 | Net: 73/78 Mbps\n- audit-worker-01: CPU 50% | Mem 54% | Disk 48% | Conns: N/A | Net: 108/103 Mbps\n- audit-db-01: CPU 30% | Mem 74% | Disk 77% | IOPS: 8000 | Replication lag: 0.4 sec\n\n### Connection Pools\n- auth_pool: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- db_pool: active 50 | idle 20 | waiting 2 | exhaustion 0 | max 100 | avg_wait 8ms\n- cache_pool: active 15 | idle 10 | waiting 0 | exhaustion 0 | max 50 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 15000\n\n### Alerts\n- [WARNING] high CPU usage on idp-api-01: 41%\n- [CRITICAL] disk utilization on audit-db-01: 77%\n- [INFO] network throughput on audit-worker-01: 108/103 Mbps\n\n### Deployments & Changes\n- Deployed patch update to audit worker; confirmed ingestion lag <6s p95.\n- No configuration changes reported today.\n\n### Events\n- Weekend: ran compliance evidence export rehearsal for upcoming auditor walkthrough; validated report IDs and storage paths.\n- Verified remaining open exceptions (2) have owners assigned and due dates; no SLA breaches yet.\n- Reviewed privileged access logs; confirmed 0 break-glass use and all sudo requests linked to tickets.\n- Performed audit worker patching; confirmed ingestion lag remained <6s p95 post-maintenance.\n\n### On-Call\n- shift handoff note: K. Svensson. 0 pages. 4 tickets. Status: Quiet; maintenance and evidence rehearsal completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_024",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-22T10:30:00",
          "text": "## 2026-01-25 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 111540 req | p50: 42.5ms p95:  | p99:  | err: 0.46% (517 errors) | success: 99.54%\n- /mfa_enrollment: REQUESTS 228 req | p50:  | p95:  | p99:  | err:  | success: 91.9%\n- /authn_failures: REQUESTS 1046 req | p50:  | p95:  | p99:  | err:  | success:  | invalid_password: 1046 | mfa_challenge_failed: 279 | account_locked: 66\n- /token_ops: REQUESTS 186920 req | p50: 168ms p95:  | p99:  | err: 0.0059% (11 failures) | success: 99.9941% | access_tokens: 186920 | refresh_tokens: 46980 | token_rotations: 10280\n- /privileged_access: REQUESTS 384 req | p50:  | p95:  | p99:  | err:  | success:  | admin_logins: 384 | break_glass: 0 | privileged_session_avg: 17.2min | sudo_requests: 87\n- /rbac_changes: REQUESTS 2 + 121 + 120 + 41 req | p50: 36min (policy_updates: 2 | role_grants: 121 | role_revokes: 120 | permission_change_requests: 41) | err:  | success:  | avg_approval_time: 36min\n- /audit_pipeline: REQUESTS 1619400 req | p50:  | p95: 5.8s | p99:  | err:  | success:  | events_ingested: 1619400 | events_dropped: 92 | queue_depth_peak: 3010\n- /directory_sync: REQUESTS 24 req | p50:  | p95:  | p99:  | err: 0 | success:  | objects_updated: 650 | objects_deleted: 7 | sync_failures: 0 | avg_sync_duration: 64s\n- /compliance_controls: REQUESTS 7 + 12 + 1 req | p50:  | p95:  | p99:  | err:  | success:  | sox_controls_checked: 7 | soc2_controls_checked: 12 | exceptions_opened: 0 | exceptions_closed: 1\n- /idp_health: REQUESTS 0 + 10 + 0 + 2 req | p50:  | p95:  | p99:  | err:  | success:  | oidc_discovery_errors: 0 | saml_assertion_errors: 10 | jwks_fetch_errors: 0 | metadata_refreshes: 2\n- /helpdesk: REQUESTS 212 req | p50: 12min | p95:  | p99:  | err:  | success:  | access_requests: 212 | password_resets: 140 | mfa_resets: 27 | median_first_response: 12min\n- /rate_limits: REQUESTS 25 + 6 + 0 req | p50:  | p95:  | p99:  | err:  | success:  | login_rate_limited: 25 | token_endpoint_rate_limited: 6 | api_keys_throttled: 0\n- /session_security: REQUESTS 37 + 4 req | p50:  | p95:  | p99:  | err:  | success:  | forced_reauth_events: 37 | anomalous_device_blocks: 4 | remembered_device_count: 31810\n- /evidence_exports: REQUESTS 9 + 3 req | p50:  | p95:  | p99:  | err:  | success:  | csv_exports: 9 | pdf_exports: 3 | export_failures: 0\n- /admin_approvals: REQUESTS 28 + 4 + 3 req | p50: 35min | p95:  | p99:  | err:  | success:  | pending_requests: 28 | auto_approved: 4 | rejected: 3\n\n### Infrastructure\n- idp-api-01: CPU 42% | Mem 66% | Disk 65% | Conns: 74 | Net: 74/79 Mbps\n- idp-api-02: CPU 40% | Mem 65% | Disk 63% | Conns: 72 | Net: 72/77 Mbps\n- audit-worker-01: CPU 51% | Mem 54% | Disk 48% | Conns: 109 | Net: 109/104 Mbps\n- audit-db-01: CPU 30% | Mem 74% | Disk 77% | IOPS: 8100 | Replication lag: 0.4 sec\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n\n### CDN & Caching\n- Hit rate: X% | Bandwidth: X Gbps | Origin requests: N\n\n### Alerts\n- [SEVERITY] saml_assertion_errors on idp-health: 10\n\n### Deployments & Changes\n- No deployments or changes reported today.\n\n### Events\n- Closed one of two open exceptions after owner provided missing ticket linkage; attached evidence and updated control log.\n- Reviewed upcoming auditor walkthrough agenda; staged screenshots of MFA policy, RBAC approval workflow, and break-glass controls.\n- Investigated minor token rotation failures; confirmed 11 were due to client clock skew causing early expiry; notified owners.\n- Validated directory sync deletion list against HR; all 7 deletions matched terminations.\n\n### On-Call\n- shift handoff note: A. Novak. 0 pages. 5 tickets. Status: Stable; exceptions reduced and walkthrough prep completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_025",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-22T10:30:00",
          "text": "## 2026-01-26 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 150340 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0.81% (1220 errors) | success: 99.19%\n- /mfa_enrollment: REQUESTS 420 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 7.8% (21 failures) | success: 92.2%\n- /authn_failures: REQUESTS 1608 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 252120 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0.012% (30 failures) | success: 99.988%\n- /privileged_access: REQUESTS 548 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 538 req | p50: 66min p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2196400 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0.01% (210 dropped) | success: 99.99%\n- /directory_sync: REQUESTS 24 req | p50: 89s p95: 89s p99: 89s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 37 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 27 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 504 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 96 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /session_security: REQUESTS 134 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 42 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 92 req | p50: 50.6ms p95: 10.8ms p99: 10.8ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 66% | Mem 78% | Disk 66% | Conns: 339 | Net: 122/130 Mbps\n- idp-api-02: CPU 63% | Mem 77% | Disk 64% | Conns: 335 | Net: 119/127 Mbps\n- audit-worker-01: CPU 76% | Mem 65% | Disk 49% | Conns: N/A | Net: 162/155 Mbps\n- audit-db-01: CPU 47% | Mem 83% | Disk 78% | IOPS: 12000 | Replication lag: 1.1 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-idp-api-02: active 115 | idle 25 | waiting 4 | exhaustion: 1 | max: 150 | avg_wait: 10ms\n- pool-audit-worker-01: active 80 | idle 20 | waiting 3 | exhaustion: 0 | max: 100 | avg_wait: 8ms\n- pool-audit-db-01: active 50 | idle 10 | waiting 2 | exhaustion: 0 | max: 60 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 3.2 Gbps | Origin requests: 15000\n\n### Alerts\n- [SEVERE] idp-api-01: CPU 66%\n- [WARNING] audit-db-01: Disk 78%\n- [INFO] audit-worker-01: CPU 76%\n- [SEVERE] idp-api-02: CPU 63%\n- [WARNING] audit-db-01: Replication lag 1.1 sec\n\n### Deployments & Changes\n- RBAC policy updates: 8 policies modified, 228 roles granted, 184 roles revoked, 82 permission change requests approved with SLA median 66 min.\n- Evidence export: 12 PDFs and 30 CSVs exported; no failures.\n- Directory sync: 24 runs, 2140 objects updated, 18 objects deleted, no failures, avg duration 89 sec.\n- Helpdesk: 392 access requests, 242 password resets, 70 MFA resets, median first response 24 min.\n- Admin approvals: 78 pending, 4 auto-approved, 14 rejected, avg approval time 73 min.\n- Ongoing audit walkthrough: demonstrated RBAC approval workflow, exported evidence pack, documented timeline.\n- Exception handling: one exception opened and closed after urgent admin access, documented timeline.\n- Helpdesk surge: MFA resets ongoing, approvals verified, second reviewer added for privileged users.\n- Evidence ingestion: lag increased during exports, scaled workers temporarily, confirmed p95 near 11 sec.\n\n### Events\n- Auditor walkthrough: demonstrated RBAC approval workflow and exported evidence pack (12 PDFs, 30 CSVs) to GRC vault.\n- Opened and closed one exception after urgent admin access request gained late co-approval; documented timeline.\n- Helpdesk MFA reset surge continued; verified approvals and added second reviewer requirement for privileged users.\n- Audit ingest lag increased during evidence exports; temporarily scaled workers and confirmed p95 returned near 11s.\n\n### On-Call\n- shift handoff note: J. Osei. 3 pages. 17 tickets. Status: Busy; walkthrough completed and temporary scaling applied.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_009",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-23T10:00:00",
          "text": "## 2024-01-23 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 122200 | p50: 98ms p95: 235ms p99: 370ms | err: 0.13% (158 errors) | success: 99.87%\n- /fraud_check: REQUESTS 121900 | p50: 72ms p95: 182ms p99: 290ms | err: 0.09% (106 errors) | success: 99.91%\n- /geo_lookup: REQUESTS 120100 | p50: 75ms p95: 190ms p99: 250ms | err: 0.06% (72 errors) | success: 99.94%\n- /auth: REQUESTS 160500 | p50: 29ms p95: 74ms p99: 120ms | err: 0.05% (78 errors) | success: 99.95%\n- /product_catalog: REQUESTS 212900 | p50: 36ms p95: 95ms p99: 155ms | err: 0.06% (118 errors) | success: 99.94%\n- /search: REQUESTS 184700 | p50: 52ms p95: 130ms p99: 210ms | err: 0.1% (180 errors) | success: 99.9%\n- /recommendations: REQUESTS 152400 | p50: 56ms p95: 152ms p99: 245ms | err: 0.1% (160 errors) | success: 99.9%\n\n### Infrastructure\n- gateway-01: CPU 37% | Mem 60% | Disk 61% | Conns: 1985 | Net: 452/544 Mbps\n- gateway-02: CPU 35% | Mem 58% | Disk 60% | Conns: 1905 | Net: 440/530 Mbps\n- service-b-01: CPU 34% | Mem 56% | Disk 57% | Conns: 1185 | Net: 240/278 Mbps\n- metrics-db-01: CPU 24% | Mem 67% | Disk 79% | Conns: 242 | Net: 98/123 Mbps\n\n### Connection Pools\n- primary: active 86 | idle 114 | waiting 1 | exhaustion: 0 | max: 200 | avg_wait: 3ms\n- replica: active 42 | idle 58 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 34 | idle 26 | waiting 2 | exhaustion: 0 | max: 60 | avg_wait: 8ms\n\n### CDN & Caching\n- Hit rate: 94.0% | Bandwidth: 6.5 Gbps | Origin requests: 324900\n\n### Alerts\n- [INFO] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=250\n\n### Deployments & Changes\n- Deployed feature-flags v0.14.0\n\n### Events\n- Feature flag 'recommendations-ml' set to 10%\n- A/B test 'checkout-v2' at 15% rollout\n- Updated gateway timeout for /search from 2.0s to 2.5s\n- CDN cache hit rate 94.0%\n\n### On-Call\n- shift handoff note: D. Kim. 0 pages. 0 tickets. Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_026",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-23T10:30:00",
          "text": "## 2026-01-27 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 149110 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0.73% (109 errors) | success: 99.27%\n- /mfa_enrollment: REQUESTS 408 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 7.6% (18 errors) | success: 92.4%\n- /authn_failures: REQUESTS 2552 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 311,130 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0.008% (26 failures) | success: 99.992%\n- /privileged_access: REQUESTS 689 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 308 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2181200 req | p50: 10.2s p95: 10.2s p99: 10.2s | err: 0.009% (196 errors) | success: 99.991%\n- /directory_sync: REQUESTS 24 req | p50: 88s p95: 88s p99: 88s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 35 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 49 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 683 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 91 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /session_security: REQUESTS 112 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 32 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 93 req | p50: 50.2ms p95: 50.2ms p99: 50.2ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 64% | Mem 77% | Disk 66% | Conns: 338 | Net: 121/129 Mbps\n- idp-api-02: CPU 62% | Mem 76% | Disk 64% | Conns: 334 | Net: 118/126 Mbps\n- audit-worker-01: CPU 74% | Mem 64% | Disk 49% | Conns: N/A | Net: 160/153 Mbps\n- audit-db-01: CPU 46% | Mem 83% | Disk 78% | IOPS: 11850 | Replication lag: 1.0 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 15 | waiting 3 | exhaustion 0 | max 150 | avg_wait 12ms\n- pool-idp-api-02: active 115 | idle 20 | waiting 2 | exhaustion 0 | max 150 | avg_wait 10ms\n- pool-audit-worker-01: active 80 | idle 10 | waiting 1 | exhaustion 0 | max 100 | avg_wait 8ms\n- pool-audit-db-01: active 50 | idle 5 | waiting 0 | exhaustion 0 | max 60 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 2.3 Gbps | Origin requests: 12,450\n\n### Alerts\n- [SEVERE] idp-api-01: CPU 64%\n- [WARNING] audit-db-01: Disk 78%\n- [INFO] audit-worker-01: CPU 74%\n- [SEVERE] idp-api-02: CPU 62%\n\n### Deployments & Changes\n- Validated audit worker scaling rollback; maintained one extra worker to keep p95 ingest lag near 10s.\n- Reduced MFA reset backlog by adding triage tag for device replacement vs enrollment failure; improved median response time by 1 minute.\n- Sent reminders for remaining open attestation items; flagged 6 nearing SLA breach.\n- Answered auditor questions on token retention and rotation cadence; provided additional log excerpts.\n\n### Events\n- Post-walkthrough follow-ups completed.\n- MFA reset backlog reduced.\n- Remaining open attestation items reviewed.\n- Audit worker scaling rollback validated.\n\n### On-Call\n- Shift: S. Haddad. 2 pages. 14 tickets. Status: Stable; follow-ups completed and staffing adjusted.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_027",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-23T10:30:00",
          "text": "## 2026-01-28 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 147880 | p50: 50ms p95: 120ms p99: 200ms | err: 0.68% (1012 errors) | success: 99.32%\n- /mfa_enrollment: REQUESTS 396 | p50: 30ms p95: 60ms p99: 100ms | err: 7.4% (16 errors) | success: 92.6%\n- /authn_failures: REQUESTS 2453 | p50: 15ms p95: 40ms p99: 70ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 248120 | p50: 10ms p95: 25ms p99: 50ms | err: 0.01% (24 failures) | success: 99.99%\n- /privileged_access: REQUESTS 681 | p50: 25ms p95: 50ms p99: 80ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 508 | p50: 40ms p95: 80ms p99: 150ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2163800 | p50: 5ms p95: 15ms p99: 30ms | err: 0.009% (188 dropped) | success: 99.991%\n- /directory_sync: REQUESTS 24 | p50: 90ms p95: 120ms p99: 150ms | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 34 | p50: 20ms p95: 40ms p99: 70ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 46 | p50: 12ms p95: 25ms p99: 40ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 462 | p50: 22ms p95: 45ms p99: 80ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 84 | p50: 8ms p95: 20ms p99: 35ms | err: 0% | success: 100%\n- /session_security: REQUESTS 124 | p50: 18ms p95: 35ms p99: 60ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 27 | p50: 25ms p95: 50ms p99: 80ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 90 | p50: 35ms p95: 70ms p99: 120ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 62% | Mem 76% | Disk 66% | Conns: 336 | Net: 119/127 Mbps\n- idp-api-02: CPU 60% | Mem 75% | Disk 64% | Conns: 332 | Net: 116/124 Mbps\n- audit-worker-01: CPU 72% | Mem 63% | Disk 49% | Conns: N/A | Net: 158/151 Mbps\n- audit-db-01: CPU 45% | Mem 82% | Disk 78% | IOPS: 11650 | Replication lag: 0.9 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-idp-api-02: active 115 | idle 35 | waiting 4 | exhaustion: 1 | max: 150 | avg_wait: 10ms\n- pool-audit-worker-01: active 80 | idle 20 | waiting 3 | exhaustion: 0 | max: 100 | avg_wait: 15ms\n- pool-audit-db-01: active 50 | idle 10 | waiting 2 | exhaustion: 0 | max: 60 | avg_wait: 8ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 3.2 Gbps | Origin requests: 12,450\n\n### Alerts\n- [SEVERE] idp-api-01: CPU 62%\n- [WARNING] audit-db-01: Disk 78%\n- [INFO] audit-worker-01: CPU 72%\n- [SEVERE] idp-api-02: CPU 60%\n- [WARNING] audit-db-01: Replication lag 0.9 sec\n\n### Deployments & Changes\n- Partner SAML Audience mismatch fixed; assertion errors down by 6 day-over-day.\n- API key rotation campaign: 14/22 rotated; remaining 8 scheduled within 10 days.\n- Verified sequence numbers and timestamps in audit log integrity check.\n\n### Events\n- Closed one overdue attestation item after escalation; updated evidence pack with attestation timestamp and approver ID.\n- Confirmed partner app fixed SAML Audience mismatch; assertion errors down by 6 day-over-day.\n- Reviewed API key rotation campaign: 14/22 rotated; remaining 8 scheduled within 10 days.\n- Performed audit log integrity check: verified sequence numbers and timestamps within expected skew window.\n\n### On-Call\n- Shift: M. Laurent. 1 page. 12 tickets. Status: Stable; partner SAML fix validated and rotation campaign progressing.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_028",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-23T10:30:00",
          "text": "## 2026-01-29 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 146520 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0.65% (956 errors) | success: 99.35%\n- /mfa_enrollment: REQUESTS 382 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 7.2% (15 failures) | success: 92.8%\n- /authn_failures: REQUESTS 1428 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 245980 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0.009% (22 failures) | success: 99.991%\n- /privileged_access: REQUESTS 529 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 499 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 2146200 req | p50: 9.4ms p95: 9.4ms p99: 9.4ms | err: 0.008% (180 errors) | success: 99.992%\n- /directory_sync: REQUESTS 24 req | p50: 85s p95: 85s p99: 85s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 33 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 43 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /helpdesk: REQUESTS 639 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 81 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /session_security: REQUESTS 177 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 25 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 87 req | p50: 49.5ms p95: 49.5ms p99: 49.5ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 61% | Mem 75% | Disk 66% | Conns: 334 | Net: 117/125 Mbps\n- idp-api-02: CPU 58% | Mem 74% | Disk 64% | Conns: 330 | Net: 114/122 Mbps\n- audit-worker-01: CPU 70% | Mem 62% | Disk 49% | Conns: N/A | Net: 156/149 Mbps\n- audit-db-01: CPU 44% | Mem 82% | Disk 78% | IOPS: 11450 | Replication lag: 0.9 sec\n\n### Connection Pools\n- pool-idp-api-01: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 200 | avg_wait 12ms\n- pool-idp-api-02: active 115 | idle 25 | waiting 4 | exhaustion 1 | max 200 | avg_wait 10ms\n- pool-audit-worker-01: active 80 | idle 20 | waiting 3 | exhaustion 0 | max 150 | avg_wait 8ms\n- pool-audit-db-01: active 50 | idle 10 | waiting 2 | exhaustion 0 | max 100 | avg_wait 7ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 3.2 Gbps | Origin requests: 1500\n\n### Alerts\n- [SEVERE] idp-api-01: CPU 61%\n- [WARNING] audit-db-01: Disk 78%\n- [CRITICAL] audit-worker-01: CPU 70%\n- [INFO] idp-api-02: Memory 74%\n\n### Deployments & Changes\n- Updated MFA enrollment flow to include TOTP and WebAuthn options.\n- Pushed security patch for token rotation failure handling.\n- Configured new directory sync schedule; verified 16 deletions.\n- Implemented new RBAC policy updates; 5 policies modified.\n- Adjusted audit pipeline thresholds; peak queue depth set to 4580.\n- Enabled additional compliance checks for SOC2.\n- Resolved IDP SAML assertion error issue; 19 errors logged.\n- Increased directory sync frequency; 24 runs completed.\n- Deployed new incident escalation procedures for unresponsive managers.\n- Updated privileged access controls; 529 admin logins recorded.\n- Fine-tuned rate limiting for login and token endpoints.\n- Enhanced session security; 104 forced reauth events.\n- Improved evidence export reliability; 18 CSV exports.\n- Reconfigured admin approval process; 69 pending requests.\n\n### Events\n- Reviewed remaining attestation reminders; 3 managers still unresponsive; prepared escalation to department heads.\n- Validated that ticket-ID enforcement is blocking privileged requests without references; 5 requests rejected automatically.\n- Analyzed token rotation failures; top cause remains client-side time drift; added monitoring query to weekly report.\n- Performed directory sync spot audit: verified 16 deletions removed from all RBAC groups within 15 minutes.\n\n### On-Call\n- Shift: C. Adeyemi. 1 page. 11 tickets. Status: Stable; enforcement working and escalations queued.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_010",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-24T10:00:00",
          "text": "## 2024-01-24 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: 125700 req | p50: 101ms p95: 245ms p99: 390ms | err: 0.14% (170 errors) | success: 99.86%\n- /fraud_check: 125400 req | p50: 74ms p95: 190ms p99: 305ms | err: 0.1% (120 errors) | success: 99.9%\n- /geo_lookup: 123500 req | p50: 78ms p95: 205ms p99: 260ms | err: 0.07% (85 errors) | success: 99.93%\n- /auth: 164700 req | p50: 30ms p95: 76ms p99: 122ms | err: 0.05% (82 errors) | success: 99.95%\n- /product_catalog: 218100 req | p50: 37ms p95: 98ms p99: 160ms | err: 0.06% (126 errors) | success: 99.94%\n- /search: 189800 req | p50: 53ms p95: 134ms p99: 218ms | err: 0.1% (190 errors) | success: 99.9%\n- /recommendations: 155600 req | p50: 58ms p95: 156ms p99: 255ms | err: 0.11% (170 errors) | success: 99.89%\n\n### Infrastructure\n- gateway-01: CPU 39% | Mem 61% | Disk 61% | Conns: 2075 | Net: 468/562 Mbps\n- gateway-02: CPU 36% | Mem 59% | Disk 60% | Conns: 1988 | Net: 452/545 Mbps\n- service-b-01: CPU 37% | Mem 58% | Disk 57% | Conns: 1320 | Net: 258/300 Mbps\n- metrics-db-01: CPU 26% | Mem 68% | Disk 80% | Conns: 250 | Net: 104/130 Mbps\n\n### Connection Pools\n- primary: active 96 | idle 104 | waiting 2 | exhaustion: 0 | max: 200 | avg_wait: 4ms\n- replica: active 46 | idle 54 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 38 | idle 22 | waiting 3 | exhaustion: 0 | max: 60 | avg_wait: 10ms\n\n### CDN & Caching\n- Hit rate: 93.8% | Bandwidth: 6.7 Gbps | Origin requests: 335600\n\n### Alerts\n- [INFO] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=118 retry_rate_pct=0.10\n- [WARNING] DISK-USAGE-WARN on metrics-db-01: disk_pct=80\n\n### Deployments & Changes\n- Deployed fraud-service (service-B) v2.7.4\n\n### Events\n- Updated service-B retry backoff parameters (config reload)\n- A/B test 'checkout-v2' at 15% rollout\n- Scaled metrics ingest workers from 3 to 4\n- CDN origin requests 335600\n\n### On-Call\n- Shift: J. Martinez. 0 pages. 1 ticket (SRV-B-220 retry metric review). Status: OK.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_029",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-24T10:30:00",
          "text": "## 2026-01-30 Daily Operations Summary\n\n### Endpoint Performance\n- /auth/sso_sessions: REQUESTS 144210 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0.62% (899 errors) | success: 99.38%\n- /auth/mfa_enrollment: REQUESTS 371 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 7.0% (14 failures) | success: 93.0%\n- /auth/authn_failures: REQUESTS 1975 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/token_ops: REQUESTS 242100 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0.01% (20 failures) | success: 99.99%\n- /auth/privileged_access: REQUESTS 657 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/rbac_changes: REQUESTS 486 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/audit_pipeline: REQUESTS 2118600 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0.008% (172 dropped) | success: 99.992%\n- /auth/directory_sync: REQUESTS 24 req | p50: 83.0s p95: 83.0s p99: 83.0s | err: 0% | success: 100%\n- /auth/compliance_controls: REQUESTS 32 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/idp_health: REQUESTS 19 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/helpdesk: REQUESTS 344 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/rate_limits: REQUESTS 76 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/session_security: REQUESTS 98 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/evidence_exports: REQUESTS 23 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n- /auth/admin_approvals: REQUESTS 83 req | p50: 49.0ms p95: 49.0ms p99: 49.0ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 59% | Mem 74% | Disk 66% | Conns: 332 | Net: 115/123 Mbps\n- idp-api-02: CPU 57% | Mem 73% | Disk 64% | Conns: 328 | Net: 112/120 Mbps\n- audit-worker-01: CPU 68% | Mem 61% | Disk 49% | Conns: N/A | Net: 154/147 Mbps\n- audit-db-01: CPU 42% | Mem 81% | Disk 78% | IOPS: 11200 | Replication lag: 0.8 sec\n\n### Connection Pools\n- pool-auth-requests: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 200 | avg_wait 12ms\n- pool-db-connections: active 85 | idle 15 | waiting 3 | exhaustion 1 | max 100 | avg_wait 8ms\n- pool-cache: active 50 | idle 20 | waiting 0 | exhaustion 0 | max 100 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 2.3 Gbps | Origin requests: 1500\n\n### Alerts\n- [CRITICAL] idp-api-01 CPU at 59%\n- [WARNING] audit-db-01 Disk usage at 78%\n- [INFO] audit-worker-01 CPU at 68%\n- [INFO] idp-api-02 Memory at 73%\n- [WARNING] audit-db-01 Replication lag at 0.8 sec\n\n### Deployments & Changes\n- Deployed security patch to idp-api-01 and idp-api-02.\n- Updated RBAC policies: 5 policy updates, 200 role grants, 207 role revokes, 74 permission change requests.\n- Ran directory sync: 24 runs, 1820 objects updated, 15 objects deleted, no failures, avg duration 83 sec.\n- Implemented new compliance controls: checked 12 SOX, 20 SOC2, opened 1 exception.\n- Updated IDP metadata: 3 refreshes, 1 discovery error, 18 SAML assertion errors, 0 JWKS fetch errors.\n- Helpdesk: processed 344 access requests, 214 password resets, 54 MFA resets, median first response 20 min.\n- Rate limiting: 57 login rate-limited, 19 token endpoint rate-limited, 0 API keys throttled.\n- Session security: 98 forced reauth events, 14 anomalous device blocks, 32680 remembered devices.\n- Evidence exports: 17 CSV, 6 PDF, 0 failures.\n- Admin approvals: 66 pending requests, 7 auto-approved, 10 rejected, avg approval time 66 min.\n\n### Events\n- Opened audit exception for 3 managers missing quarterly attestations past SLA; escalated to department heads with deadlines.\n- Verified automatic rejection of privileged requests without ticket IDs; captured screenshots and logs as control evidence.\n- Ran end-of-month privileged access summary: 0 break-glass uses this week, 139 sudo requests all linked to tickets.\n- Reviewed pipeline drop counts; confirmed 172 drops were non-critical debug events excluded by filter policy.\n\n### On-Call\n- Shift: J. Steiner. 2 pages. 12 tickets. Status: Stable; attestation escalation active and controls evidenced.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_030",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-24T10:30:00",
          "text": "## 2026-01-31 Daily Operations Summary\n\n### Endpoint Performance\n- /sso_sessions: REQUESTS 114660 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0.4% (458 errors) | success: 99.6%\n- /mfa_enrollment: REQUESTS 240 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 2.9% (7 failures) | success: 97.1%\n- /authn_failures: REQUESTS 1303 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /token_ops: REQUESTS 239620 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0.0037% (9 failures) | success: 99.9963%\n- /privileged_access: REQUESTS 485 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /rbac_changes: REQUESTS 298 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /audit_pipeline: REQUESTS 1649000 req | p50: 5.4s p95: 5.4s p99: 5.4s | err: 0.0051% (84 dropped) | success: 99.9949%\n- /directory_sync: REQUESTS 24 req | p50: 62s p95: 62s p99: 62s | err: 0% | success: 100%\n- /compliance_controls: REQUESTS 19 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /idp_health: REQUESTS 10 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0.8% (8 errors) | success: 99.2%\n- /helpdesk: REQUESTS 210 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /rate_limits: REQUESTS 29 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /session_security: REQUESTS 35 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /evidence_exports: REQUESTS 11 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n- /admin_approvals: REQUESTS 33 req | p50: 43.2ms p95: 43.2ms p99: 43.2ms | err: 0% | success: 100%\n\n### Infrastructure\n- idp-api-01: CPU 40% | Mem 66% | Disk 66% | Conns: 74 | Net: 74/79 Mbps\n- idp-api-02: CPU 38% | Mem 65% | Disk 64% | Conns: 72 | Net: 72/77 Mbps\n- audit-worker-01: CPU 49% | Mem 53% | Disk 49% | Conns: 110 | Net: 110/105 Mbps\n- audit-db-01: CPU 29% | Mem 74% | Disk 78% | IOPS: 7950 | Replication lag: 0.4s\n\n### Connection Pools\n- sso_sessions: active 114660 | idle 0 | waiting 0 | exhaustion: 0 | max: 120000 | avg_wait: 0ms\n- mfa_enrollment: active 240 | idle 0 | waiting 0 | exhaustion: 0 | max: 300 | avg_wait: 0ms\n- authn_failures: active 1303 | idle 0 | waiting 0 | exhaustion: 0 | max: 1500 | avg_wait: 0ms\n- token_ops: active 239620 | idle 0 | waiting 0 | exhaustion: 0 | max: 250000 | avg_wait: 0ms\n- privileged_access: active 485 | idle 0 | waiting 0 | exhaustion: 0 | max: 500 | avg_wait: 0ms\n- rbac_changes: active 298 | idle 0 | waiting 0 | exhaustion: 0 | max: 350 | avg_wait: 0ms\n- audit_pipeline: active 1649000 | idle 0 | waiting 84 | exhaustion: 0 | max: 1700000 | avg_wait: 5ms\n- directory_sync: active 24 | idle 0 | waiting 0 | exhaustion: 0 | max: 50 | avg_wait: 0ms\n- compliance_controls: active 19 | idle 0 | waiting 0 | exhaustion: 0 | max: 25 | avg_wait: 0ms\n- idp_health: active 10 | idle 0 | waiting 0 | exhaustion: 0 | max: 15 | avg_wait: 0ms\n- helpdesk: active 210 | idle 0 | waiting 0 | exhaustion: 0 | max: 250 | avg_wait: 0ms\n- rate_limits: active 29 | idle 0 | waiting 0 | exhaustion: 0 | max: 50 | avg_wait: 0ms\n- session_security: active 35 | idle 0 | waiting 0 | exhaustion: 0 | max: 50 | avg_wait: 0ms\n- evidence_exports: active 11 | idle 0 | waiting 0 | exhaustion: 0 | max: 20 | avg_wait: 0ms\n- admin_approvals: active 33 | idle 0 | waiting 0 | exhaustion: 0 | max: 40 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 97.8% | Bandwidth: 2.3 Gbps | Origin requests: 15000\n\n### Alerts\n- [WARNING] idp_health_saml_assertion_errors on idp-api-01: 8\n- [INFO] audit_pipeline_queue_depth_peak: 2820\n- [INFO] audit_pipeline_ingest_lag_sec_p95: 5.4s\n- [INFO] directory_sync_failures: 0\n- [INFO] compliance_exceptions_opened: 0\n- [INFO] compliance_exceptions_closed: 1\n\n### Deployments & Changes\n- Monthly evidence bundle validation completed.\n- Token rotation campaign: 18/22 rotated; remaining scheduled.\n- Performed audit DB vacuum/analyze; replication lag under 0.5s.\n- Closed one escalated attestation exception after review.\n- Updated 2 RBAC policies, 124 role grants, 132 revokes, 42 permission change requests; SLA p50: 38 min.\n- Conducted 24 directory syncs; 620 objects updated, 6 deleted, no failures; avg duration 62s.\n- Checked 7 SOX controls, 12 SOC2 controls; 1 exception closed.\n- Verified IDP health: 8 SAML assertion errors, 0 OIDC discovery errors, 0 JWKS fetch errors; 2 metadata refreshes.\n- Ran helpdesk: 210 access requests, 132 password resets, 25 MFA resets; median first response 12 min.\n- Rate limiting: 23 login, 6 token endpoint throttles; no API key throttling.\n- Enforced session security: 35 reauth events, 4 device blocks; 32,740 devices remembered.\n- Exported 8 CSV, 3 PDF evidence reports; no failures.\n- Pending 26 admin approval requests; 5 auto-approved, 2 rejected; avg approval time 33 min.\n\n### Events\n- Weekend closeout: closed one escalated attestation exception after department head review; remaining tracked for next business day.\n- Ran monthly evidence bundle validation: confirmed checksums and access controls.\n- Reviewed token rotation campaign: 18/22 rotated; scheduled remaining 4.\n- Performed audit DB maintenance: vacuum/analyze; replication lag under 0.5s.\n\n### On-Call\n- Shift: W. Zhang. 0 pages. 5 tickets. Status: Quiet; maintenance and evidence validation completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "auth_audit"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_031",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-24T10:30:00",
          "text": "## 2026-01-03 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 18500 req | p50: 18ms p95: 44.1ms p99: N/A | err: 0.1% (19 errors) | success: 99.9%\n- /zone-transfer: REQUESTS 42 req | p50: 612ms p95: 1440ms p99: N/A | err: 4.8% (2 errors) | success: 95.2%\n- /ixfr-request: REQUESTS 188 req | p50: 612ms p95: 1440ms p99: N/A | err: 1.1% (2 errors) | success: 98.9%\n- /dnssec-sign: REQUESTS 116 req | p50: N/A | p95: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A | p50: N/A | p95: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 38% | Mem 62% | Disk 71% | Conns: N/A | Net: 84/112 Mbps\n- bind-slave-02: CPU 22% | Mem 49% | Disk 54% | Conns: N/A | Net: 61/75 Mbps\n- cloud-dns-sync-01: CPU 57% | Mem 68% | Disk 46% | Conns: N/A | Net: 95/130 Mbps\n- signer-01: CPU 44% | Mem 73% | Disk 58% | Conns: N/A | Net: 18/20 Mbps\n\n### Connection Pools\n- pool-bind-master-01: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-bind-slave-02: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-cloud-dns-sync-01: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-signer-01: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n\n### CDN & Caching\n- Hit rate: 87.3% | Bandwidth: 1.2 Gbps | Origin requests: 18500\n\n### Alerts\n- [WARNING] AXFR transfer failures on bind-slave-02: 2 failures\n- [INFO] TTL reduction rollout completed for 5 zones\n- [WARNING] SOA serial mismatch detected on 3 zones\n- [INFO] DNSSEC signer queue backed up after KSK rollover; cleared by worker thread increase\n\n### Deployments & Changes\n- Enabled cloud-managed secondary for 12 pilot zones; verified NS sets and SOA serial increments\n- Completed TTL reduction for api.example.com records (3600->300) in 5 zones; one rollback due to upstream query volume increase\n- Simulated KSK rollover; increased signer worker threads from 4 to 6 to clear signer queue\n\n### Events\n- Planned cutover: enabled cloud-managed secondary for 12 pilot zones; verified NS sets and SOA serial increments\n- Detected intermittent SOA serial mismatch on 3 zones due to delayed IXFR application on bind-slave-02; forced retransfer using AXFR\n- TTL reduction rollout for api.example.com records (3600->300) completed for 5 zones; one rollback after stakeholder flagged increased upstream query volume\n- DNSSEC signer queue briefly backed up after KSK rollover simulation; cleared by increasing signer worker threads from 4 to 6\n\n### On-Call\n- shift handoff note: R. Patel. 3 pages. 7 tickets. Status: Stable after minor transfer remediation; monitoring propagation and DNSSEC validations",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_011",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-25T10:00:00",
          "text": "## 2024-01-25 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 124100 | p50: 103ms p95: 252ms p99: 405ms | err: 0.14% (178 errors) | success: 99.86%\n- /fraud_check: REQUESTS 123800 | p50: 76ms p95: 198ms p99: 320ms | err: 0.11% (132 errors) | success: 99.89%\n- /geo_lookup: REQUESTS 121900 | p50: 82ms p95: 220ms p99: 275ms | err: 0.08% (92 errors) | success: 99.92%\n- /auth: REQUESTS 162600 | p50: 30ms p95: 78ms p99: 124ms | err: 0.05% (84 errors) | success: 99.95%\n- /product_catalog: REQUESTS 216200 | p50: 38ms p95: 100ms p99: 165ms | err: 0.06% (128 errors) | success: 99.94%\n- /search: REQUESTS 188300 | p50: 54ms p95: 136ms p99: 222ms | err: 0.1% (192 errors) | success: 99.9%\n- /recommendations: REQUESTS 154100 | p50: 60ms p95: 160ms p99: 262ms | err: 0.11% (174 errors) | success: 99.89%\n\n### Infrastructure\n- gateway-01: CPU 40% | Mem 62% | Disk 61% | Conns: 2140 | Net: 462/556 Mbps\n- gateway-02: CPU 37% | Mem 60% | Disk 60% | Conns: 2055 | Net: 450/542 Mbps\n- service-b-01: CPU 39% | Mem 59% | Disk 57% | Conns: 1455 | Net: 270/315 Mbps\n- metrics-db-01: CPU 27% | Mem 69% | Disk 81% | Conns: 258 | Net: 108/134 Mbps\n\n### Connection Pools\n- primary: active 104 | idle 96 | waiting 3 | exhaustion: 0 | max: 200 | avg_wait: 5ms\n- replica: active 49 | idle 51 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 42 | idle 18 | waiting 4 | exhaustion: 1 | max: 60 | avg_wait: 14ms\n\n### CDN & Caching\n- Hit rate: 93.7% | Bandwidth: 6.6 Gbps | Origin requests: 332100\n\n### Alerts\n- [WARNING] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=275\n- [INFO] CONNPOOL-WAIT on gateway-01: pool=third_party_geo waiting=4 avg_wait_ms=14\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Rotated JVM flags for gateway (restart)\n- A/B test 'checkout-v2' at 15% rollout\n- Adjusted CDN TTL for /images/* from 3600s to 5400s\n- Updated alert routing for UPSTREAM-LATENCY to on-call\n\n### On-Call\n- Shift: A. Chen. 1 page (UPSTREAM-LATENCY). 1 ticket (NET-331 geo vendor review). Status: Monitoring.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_032",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-25T10:30:00",
          "text": "## 2026-01-04 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 19200 req | p50: 17.4ms p95: 41.9ms p99: N/A | err: 1.9% (366 errors) | success: 98.1%\n- /zone-transfer: REQUESTS N/A req | p50: N/Ams p95: N/Ams p99: N/A | err: N/A | success: N/A%\n- /dnssec-sign: REQUESTS N/A req | p50: N/Ams p95: N/Ams p99: N/A | err: N/A | success: N/A%\n- /health-check: REQUESTS N/A req | p50: N/Ams p95: N/Ams p99: N/A | err: N/A | success: N/A%\n- /change-request: REQUESTS 4 req | p50: N/Ams p95: N/Ams p99: N/A | err: 0% (0 errors) | success: 100%\n- /cache-status: REQUESTS N/A req | p50: N/Ams p95: N/Ams p99: N/A | err: N/A | success: N/A%\n\n### Infrastructure\n- bind-master-01: CPU 31% | Mem 60% | Disk 71% | Conns: 1210 | Net: 78/104 Mbps\n- bind-slave-02: CPU 26% | Mem 50% | Disk 54% | Conns: 1020 | Net: 64/82 Mbps\n- cloud-dns-sync-01: CPU 49% | Mem 64% | Disk 46% | Conns: N/A | Net: 88/120 Mbps\n- signer-01: CPU 36% | Mem 69% | Disk 58% | Conns: N/A | Net: 16/18 Mbps\n\n### Connection Pools\n- pool-axfr: active 36 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-ixfr: active 210 | idle 0 | waiting 0 | exhaustion 0 | max 250 | avg_wait 0ms\n- pool-signing: active 1 | idle 0 | waiting 0 | exhaustion 0 | max 2 | avg_wait 0ms\n- pool-cloud-sync: active 4 | idle 0 | waiting 0 | exhaustion 0 | max 10 | avg_wait 0ms\n\n### CDN & Caching\n- Hit rate: 88.1% | Bandwidth: 1.2 Gbps | Origin requests: 1500\n\n### Alerts\n- [INFO] DNSSEC bogus validation event on signer-01: 1 event\n- [WARNING] N/A\n- [CRITICAL] N/A\n\n### Deployments & Changes\n- Completed delegation review for pilot zones; removed one stale NS entry left in registrar configuration.\n- Ran dry-run of bulk record import for 20 additional zones; validation flagged 3 CNAME-at-apex issues; corrected templates.\n- Scheduled DNSSEC ZSK rotation window; verified DS records remain consistent and monitored bogus validation events (none material).\n\n### Events\n- Completed delegation review for pilot zones; removed one stale NS entry left in registrar configuration.\n- Ran dry-run of bulk record import for 20 additional zones; validation flagged 3 CNAME-at-apex issues; corrected templates.\n- Observed minor increase in NXDOMAIN due to decommissioned legacy hostnames; added temporary wildcard record in staging only for testing, not promoted.\n- Scheduled DNSSEC ZSK rotation window; verified DS records remain consistent and monitored bogus validation events (none material).\n\n### On-Call\n- shift handoff note: Shift: L. Nguyen. 1 pages. 5 tickets. Status: Normal; proceeding with staged expansion after template fixes.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_033",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-25T10:30:00",
          "text": "## 2026-01-05 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfer: REQUESTS req | p50: 705ms p95: 1680ms p99: N/A | err: 3.4% (2 errors) | success: 96.6%\n- /dns-query: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.12% (N/A errors) | success: 99.88%\n- /dnssec-sign: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /change-request: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 46% | Mem 66% | Disk 72% | Conns: N/A | Net: 110/145 Mbps\n- bind-slave-02: CPU 34% | Mem 57% | Disk 55% | Conns: N/A | Net: 92/101 Mbps\n- cloud-dns-sync-01: CPU 71% | Mem 77% | Disk 47% | Conns: N/A | Net: 140/170 Mbps\n- signer-01: CPU 58% | Mem 81% | Disk 59% | Conns: N/A | Net: 22/26 Mbps\n\n### Connection Pools\n- pool-axfr: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n- pool-ixfr: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n- pool-dns-queries: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n- pool-signing: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n\n### CDN & Caching\n- Hit rate: 86.2% | Bandwidth: 1.2 Gbps | Origin requests: 45,000\n\n### Alerts\n- [SEVERE] TSIG authentication failure on bind-slave-02: key mismatch\n- [WARNING] DNSSEC DS mismatch detected for zone example.com\n- [INFO] 16 zones imported into cloud-managed DNS\n- [ERROR] 11 zonefile lint errors found post-import\n- [CRITICAL] 2 TTL change rollbacks executed\n- [NOTICE] Stakeholders reported stale answers; TTL restored to 900s\n\n### Deployments & Changes\n- Imported 16 new zones into cloud-managed DNS; post-import lint found 11 issues (mostly missing trailing dots, SPF TXT quoting)\n- Corrected TSIG key name mismatch on bind-slave-02 after authentication failures; retried AXFR\n- Paused DNSSEC publish for zone example.org after DS mismatch; coordinated registrar resync\n- Restored TTL from 120s to 900s for critical A records after stakeholder reports of stale answers\n\n### Events\n- Imported 16 new zones; lint issues identified\n- TSIG authentication failures started post key rotation; corrected key mismatch\n- DS mismatch detected after registrar update lag; paused DNSSEC publish and coordinated resync\n- Two TTL change rollbacks executed; TTL restored to 900s for critical records\n\n### On-Call\n- shift handoff: S. Ahmed. 4 pages. 9 tickets. Status: Degraded during import window; stabilized after TSIG fix and DNSSEC hold on one zone",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_034",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-25T10:30:00",
          "text": "## 2026-01-06 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 590ms p95: 1320ms p99: N/A | err: 2.5% (1 errors) | success: 97.5%\n- /axfr-requests: REQUESTS req | p50: 590ms p95: 1320ms p99: N/A | err: 2.5% (1 errors) | success: 97.5%\n- /ixfr-requests: REQUESTS req | p50: 590ms p95: 1320ms p99: N/A | err: 0.9% (2 errors) | success: 99.1%\n- /dns-queries: QPS avg: 19800 | peak: 45200 | nx_domain: 2.0% | servfail: 0.06% | udp: 96.6% | tcp_fallback: 1.7%\n- /dnssec-signing: zones_signed: 52 | signature_refreshes: 118 | signing_queue_depth: 2 | ds_mismatch: 0 | bogus_validations: 2\n- /health-checks: authoritative_reachability: 99.98% | soa_serial_mismatch: 2 | lame_delegation: 0\n- /change-requests: total: 6 | applied: 6 | failed: 0 | mean_apply_time: 21 min\n- /cache-behavior: resolver_cache_hit: 87.9% | negative_cache_hit: 40.6% | prefetch_enabled: 68.0%\n- /latency: auth_response_avg: 18.9ms | auth_response_p95: 46.2ms | edns0_present: 72.9%\n- /errors: format_errors: 9 | refused: 33 | rate_limited: 14 | truncation_tc: 0.6%\n- /propagation: cloud_edge_sites: 34 | propagation_avg: 70s | propagation_p95: 170s | stale_answer_pct: 0.4%\n\n### Infrastructure\n- bind-master-01: CPU 35% | Mem 63% | Disk 72% | Conns: 1295 | Net: 86/117 Mbps\n- bind-slave-02: CPU 28% | Mem 52% | Disk 55% | Conns: 1090 | Net: 70/85 Mbps\n- cloud-dns-sync-01: CPU 55% | Mem 69% | Disk 47% | Conns: N/A | Net: 98/135 Mbps | Queue depth: 5\n- signer-01: CPU 41% | Mem 72% | Disk 59% | Conns: N/A | Net: 17/21 Mbps | HSM slot utilization: 55%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 87.9% | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [INFO] DNSSEC bogus validation events on zone: 2\n- [INFO] SOA serial mismatch count: 2\n- [WARNING] N/A\n- [CRITICAL] N/A\n\n### Deployments & Changes\n- Re-ran lint and normalization across newly imported zones; reduced errors from 11 to 4 by fixing TXT escaping and missing origin suffixes.\n- Performed controlled IXFR catch-up to cloud-managed secondary; SOA serial mismatches dropped from 6 to 2.\n- Implemented rate limiting adjustment for ANY queries; refused count steady and truncation decreased.\n- Validated DNSSEC chain for previously paused zone after registrar DS sync; resumed signing and monitored for bogus validations.\n\n### Events\n- Re-ran lint and normalization across newly imported zones; reduced errors from 11 to 4 by fixing TXT escaping and missing origin suffixes.\n- Performed controlled IXFR catch-up to cloud-managed secondary; SOA serial mismatches dropped from 6 to 2.\n- Implemented rate limiting adjustment for ANY queries; refused count steady and truncation decreased.\n- Validated DNSSEC chain for previously paused zone after registrar DS sync; resumed signing and monitored for bogus validations.\n\n### On-Call\n- shift handoff note: Shift: M. Chen. 2 pages. 6 tickets. Status: Healthy; continued cleanup and verified DNSSEC correctness.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_012",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-26T10:00:00",
          "text": "## 2024-01-26 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 127900 | p50: 108ms p95: 265ms p99: 430ms | err: 0.15% (190 errors) | success: 99.85%\n- /fraud_check: REQUESTS 127500 | p50: 79ms p95: 210ms p99: 340ms | err: 0.12% (150 errors) | success: 99.88%\n- /geo_lookup: REQUESTS 125400 | p50: 90ms p95: 240ms p99: 300ms | err: 0.08% (105 errors) | success: 99.92%\n- /auth: REQUESTS 167300 | p50: 31ms p95: 80ms p99: 128ms | err: 0.05% (88 errors) | success: 99.95%\n- /product_catalog: REQUESTS 221700 | p50: 39ms p95: 104ms p99: 172ms | err: 0.06% (134 errors) | success: 99.94%\n- /search: REQUESTS 192600 | p50: 56ms p95: 142ms p99: 232ms | err: 0.1% (198 errors) | success: 99.9%\n- /recommendations: REQUESTS 157300 | p50: 63ms p95: 168ms p99: 275ms | err: 0.11% (180 errors) | success: 99.89%\n\n### Infrastructure\n- gateway-01: CPU 42% | Mem 63% | Disk 61% | Conns: 2245 | Net: 480/575 Mbps\n- gateway-02: CPU 39% | Mem 61% | Disk 60% | Conns: 2148 | Net: 468/560 Mbps\n- service-b-01: CPU 43% | Mem 61% | Disk 58% | Conns: 1680 | Net: 298/346 Mbps\n- metrics-db-01: CPU 29% | Mem 70% | Disk 81% | Conns: 265 | Net: 114/140 Mbps\n\n### Connection Pools\n- primary: active 118 | idle 82 | waiting 5 | exhaustion: 0 | max: 200 | avg_wait: 7ms\n- replica: active 54 | idle 46 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 48 | idle 12 | waiting 6 | exhaustion: 1 | max: 60 | avg_wait: 18ms\n\n### CDN & Caching\n- Hit rate: 93.6% | Bandwidth: 6.9 Gbps | Origin requests: 345800\n\n### Alerts\n- [WARNING] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=132 retry_rate_pct=0.11\n- [WARNING] CONNPOOL-WAIT on gateway-02: pool=third_party_geo waiting=6 avg_wait_ms=18\n\n### Deployments & Changes\n- Deployed gateway observability pack v0.9.2\n\n### Events\n- Enabled extra upstream timing fields in gateway logs\n- A/B test 'checkout-v2' at 15% rollout\n- Scaled service-B from 6 to 7 instances\n- CDN cache hit rate 93.6%\n\n### On-Call\n- Shift: K. Okafor. 2 pages (CONNPOOL-WAIT, SERVICE-B-RETRY-COUNT). 1 ticket (OBS-210 log fields). Status: Monitoring.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_035",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-26T10:30:00",
          "text": "## 2026-01-07 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: axfr_requests 33 | axfr_success_pct 100.0% | ixfr_requests 198 | ixfr_success_pct 99.5% | transfer_avg_ms 510 | transfer_p95_ms 1185 | tsig_failures 0\n- /ttl-changes: records_changed 55 | zones_touched 8 | ttl_min_s 300 | ttl_max_s 43200 | ttl_median_s 3600 | rollbacks 1\n- /propagation: cloud_edge_sites 34 | propagation_avg_s 62 | propagation_p95_s 150 | stale_answer_pct 0.2%\n- /dns-queries: qps_avg 20100 | qps_peak 46500 | nx_domain_pct 2.2% | servfail_pct 0.04% | udp_pct 97.1% | tcp_fallback_pct 1.3%\n- /dnssec: zones_signed 54 | signing_queue_depth 1 | signature_refreshes 109 | ds_mismatch_count 0 | bogus_validation_events 0\n- /health-checks: authoritative_reachability_pct 99.99% | soa_serial_mismatch_count 0 | lame_delegation_count 0\n- /change-mgmt: change_requests 5 | changes_applied 5 | changes_failed 0 | mean_time_to_apply_min 16\n- /cache-behavior: resolver_cache_hit_pct 88.6% | negative_cache_hit_pct 38.9% | prefetch_enabled_pct 70.0%\n- /latency: auth_response_avg_ms 17.1 | auth_response_p95_ms 40.8 | edns0_present_pct 73.5%\n- /errors: format_error_count 6 | refused_count 26 | rate_limited_count 9 | truncation_tc_pct 0.4%\n- /capacity: zones_total 544 | records_total 1931200 | dynamic_updates_per_hr 295\n- /compliance: zonefile_lint_errors 2 | dnssec_policy_violations 0 | audit_log_gaps 0\n\n### Infrastructure\n- bind-master-01: CPU 29% | Mem 61% | Disk 72% | Conns: 1232 | Net: 74/100 Mbps\n- bind-slave-02: CPU 25% | Mem 51% | Disk 55% | Conns: 1068 | Net: 65/80 Mbps\n- cloud-dns-sync-01: CPU 48% | Mem 66% | Disk 47% | Conns: N/A | Net: 92/126 Mbps | Queue depth 3\n- signer-01: CPU 34% | Mem 70% | Disk 59% | Conns: N/A | Net: 15/19 Mbps | HSM slot utilization 54%\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n(Note: Specific pool data not provided in data sheet)\n\n### CDN & Caching\n- Hit rate: 88.6% | Bandwidth: N/A | Origin requests: N/A\n- Negative cache hit rate: 38.9%\n- Prefetch enabled: 70.0%\n\n### Alerts\n- [INFO] DNS zone transfer success on hostname: 0\n- [INFO] TTL change rollback occurred\n- [INFO] Propagation delay average at 62s\n- [INFO] Cache hit rate at 88.6%\n- [INFO] DNSSEC signatures refreshed 109 times\n- [INFO] Change requests applied successfully\n- [INFO] Latency auth response average 17.1ms\n- [INFO] Errors: 6 format errors, 26 refused, 9 rate-limited, 0.4% truncation\n- [INFO] Capacity: 544 zones, 1,931,200 records, 295 dynamic updates/hr\n- [INFO] Zonefile lint errors: 2\n\n### Deployments & Changes\n- Added 16 zones to cloud-managed DNS; completed NS delegation tests from 10 external resolvers.\n- Rolled TTL standardization for 8 zones; one rollback for CDN alias due to partner requirement of 300.\n- Enabled prefetch for additional resolver POPs; cache hit rate improved.\n- Updated runbook to reflect cloud sync controller ownership; decommissioned old on-prem monitoring checks.\n\n### Events\n- Added 16 zones to cloud DNS; NS delegation tests completed.\n- Rolled TTL standardization for 8 zones; one rollback for CDN alias.\n- Enabled prefetch for resolver POPs; cache hit rate improved.\n- Closed old on-prem monitoring checks; updated runbook.\n\n### On-Call\n- shift handoff note: J. Silva. 1 pages. 4 tickets. Status: Stable; migration progressing on schedule with minor TTL exception.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_036",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-26T10:30:00",
          "text": "## 2026-01-08 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 21050 req | p50: 19.8ms p95: 49.5ms p99: N/A | err: 2.18% (459 errors) | success: 97.82%\n- /zone-transfer: REQUESTS 205 req | p50: N/A p95: N/A p99: N/A | err: 3.4% (7 errors) | success: 96.6%\n- /dnssec-sign: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /change-request: REQUESTS 3 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /cache-status: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 41% | Mem 65% | Disk 73% | Conns: N/A | Net: 90/122 Mbps\n- bind-slave-02: CPU 27% | Mem 52% | Disk 55% | Conns: N/A | Net: 66/84 Mbps\n- cloud-dns-sync-01: CPU 63% | Mem 74% | Disk 48% | Conns: N/A | Net: 120/150 Mbps\n- signer-01: CPU 47% | Mem 78% | Disk 60% | Conns: N/A | Net: 19/23 Mbps\n\n### Connection Pools\n- pool-axfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-ixfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-signing: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-queries: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n\n### CDN & Caching\n- Hit rate: 87.0% | Bandwidth: 0.7 Gbps | Origin requests: 21050\n\n### Alerts\n- [WARNING] TSIG verification warning on bind nodes: NTP offset up to 8ms\n- [INFO] Cloud edge site added in region expansion\n- [INFO] DNS change logs missed due to storage rotation; backfilled from cloud provider audit export\n- [WARNING] SOA serial mismatch on 2 zones after dynamic updates\n- [INFO] Time sync resynced; alerts cleared\n\n### Deployments & Changes\n- Added new cloud edge site in region expansion; validated authoritative reachability and response codes before/after\n- Forced IXFR from master for 2 zones with SOA serial mismatch; confirmed parity\n- Backfilled DNS change logs from cloud provider audit export after missed hour\n\n### Events\n- Added new cloud edge site in region expansion; validated authoritative reachability and compared response codes before/after\n- Noted brief NTP drift on bind nodes (offset up to 8ms) causing sporadic TSIG verification warnings; resynced time and cleared alerts\n- Audit pipeline missed one hour of DNS change logs due to storage rotation misconfig; backfilled from cloud provider audit export\n- SOA serial mismatch seen on 2 zones after dynamic updates; forced immediate IXFR from master and confirmed parity\n\n### On-Call\n- shift handoff note: A. Johnson. 3 pages. 6 tickets. Status: Stable after time sync and audit backfill; watching NTP and TSIG logs",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_037",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-26T10:30:00",
          "text": "## 2026-01-09 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS req | p50: 18ms p95: 43.3ms p99: N/A | err: 0.07% (15 errors) | success: 99.93%\n- /zone-transfer: REQUESTS req | p50: N/A p95: 1360ms p99: N/A | err: 2.2% (46 errors) | success: 97.8%\n- /ixfr-request: REQUESTS req | p50: N/A p95: 1360ms p99: N/A | err: 1.0% (2 errors) | success: 99.0%\n- /ttl-change: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /dnssec-sign: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /health-check: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /change-request: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 33% | Mem 62% | Disk 73% | Conns: 1268 | Net: 82/114 Mbps\n- bind-slave-02: CPU 24% | Mem 50% | Disk 55% | Conns: 1084 | Net: 62/79 Mbps\n- cloud-dns-sync-01: CPU 52% | Mem 69% | Disk 48% | Conns: N/A | Net: 102/140 Mbps\n- signer-01: CPU 39% | Mem 73% | Disk 60% | Conns: N/A | Net: 18/20 Mbps\n\n### Connection Pools\n- zone-transfer-pool: active 12 | idle 3 | waiting 2 | exhaustion 0 | max 20 | avg_wait 15ms\n- dns-query-pool: active 25 | idle 5 | waiting 4 | exhaustion 1 | max 40 | avg_wait 20ms\n- sync-controller: active 4 | idle 0 | waiting 1 | exhaustion 0 | max 10 | avg_wait 10ms\n- signer-hsm: active 1 | idle 1 | waiting 0 | exhaustion 0 | max 3 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 88.0% | Bandwidth: 2.5 Gbps | Origin requests: 21400\n\n### Alerts\n- [WARNING] high latency on cloud-dns-sync-01: propagation_avg_s 68\n- [CRITICAL] zone transfer failures on bind-master-01: tsig_failures 0\n- [ERROR] DNSSEC bogus validation event: 1 event\n- [INFO] health check authoritative reachability: 99.99%\n- [INFO] SOA serial mismatch count: 1\n- [INFO] lame delegation count: 0\n\n### Deployments & Changes\n- Migrated 16 zones; executed automated smoke tests for A/AAAA/MX/TXT parity against on-prem results\n- Adjusted TTLs for mail-related records (MX and SPF) to 1800 for 9 zones; one rollback after mail vendor advised keeping 3600 for anti-spam heuristics\n- Updated cloud sync controller to include zone-level locking to avoid overlapping IXFR apply tasks; queue depth reduced\n- Validated zone configurations; confirmed no lame delegations; cleaned outdated glue record at registrar\n\n### Events\n- Migrated another batch of 16 zones; executed automated smoke tests for A/AAAA/MX/TXT parity against on-prem results\n- Adjusted TTLs for mail-related records (MX and SPF) to 1800 for 9 zones; one rollback after mail vendor advised keeping 3600 for anti-spam heuristics\n- Updated cloud sync controller to include zone-level locking to avoid overlapping IXFR apply tasks; queue depth reduced\n- External validation run confirmed no lame delegations; cleaned up one outdated glue record at registrar\n\n### On-Call\n- shift handoff note: K. O'Brien. 2 pages. 8 tickets. Status: Normal; completed batch migration and documented TTL exception for mail vendor",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_013",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-27T10:00:00",
          "text": "## 2024-01-27 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 133600 | p50: 112ms p95: 275ms p99: 450ms | err: 0.16% (210 errors) | success: 99.84%\n- /fraud_check: REQUESTS 133150 | p50: 82ms p95: 220ms p99: 360ms | err: 0.13% (170 errors) | success: 99.87%\n- /geo_lookup: REQUESTS 130900 | p50: 98ms p95: 260ms p99: 330ms | err: 0.09% (118 errors) | success: 99.91%\n- /auth: REQUESTS 174500 | p50: 32ms p95: 82ms p99: 132ms | err: 0.05% (92 errors) | success: 99.95%\n- /product_catalog: REQUESTS 231600 | p50: 40ms p95: 108ms p99: 180ms | err: 0.06% (140 errors) | success: 99.94%\n- /search: REQUESTS 201400 | p50: 58ms p95: 148ms p99: 242ms | err: 0.1% (205 errors) | success: 99.9%\n- /recommendations: REQUESTS 164100 | p50: 66ms p95: 175ms p99: 290ms | err: 0.12% (190 errors) | success: 99.88%\n\n### Infrastructure\n- gateway-01: CPU 44% | Mem 64% | Disk 61% | Conns: 2380 | Net: 505/602 Mbps\n- gateway-02: CPU 41% | Mem 62% | Disk 60% | Conns: 2285 | Net: 492/585 Mbps\n- service-b-01: CPU 46% | Mem 63% | Disk 58% | Conns: 1905 | Net: 320/372 Mbps\n- metrics-db-01: CPU 31% | Mem 71% | Disk 82% | Conns: 272 | Net: 120/148 Mbps\n\n### Connection Pools\n- primary: active 128 | idle 72 | waiting 7 | exhaustion: 0 | max: 200 | avg_wait: 9ms\n- replica: active 58 | idle 42 | waiting 0 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- third_party_geo: active 52 | idle 8 | waiting 8 | exhaustion: 1 | max: 60 | avg_wait: 22ms\n\n### CDN & Caching\n- Hit rate: 93.5% | Bandwidth: 7.3 Gbps | Origin requests: 366400\n\n### Alerts\n- [WARNING] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=330\n- [WARNING] DISK-USAGE-WARN on metrics-db-01: disk_pct=82\n\n### Deployments & Changes\n- Deployed recommendations v1.9.1\n\n### Events\n- A/B test 'checkout-v2' at 15% rollout\n- Weekend promo banner disabled\n- Updated service-B HPA target CPU from 65% to 60%\n- Metrics-db vacuum scheduled 2024-01-28 01:00Z\n\n### On-Call\n- shift handoff: S. Patel. 1 page (UPSTREAM-LATENCY). 1 ticket (VEND-77 geo vendor support). Status: Monitoring.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_038",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-27T10:30:00",
          "text": "## 2026-01-10 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: 20900 req | p50: 16.8ms p95: 39.2ms p99: N/A | err: 2.04% (429 errors) | success: 97.96%\n- /zone-transfer: 215 req | p50: 495ms p95: 1105ms p99: N/A | err: 1.1% (2 errors) | success: 98.9%\n- /ttl-change: 6 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /health-check: 1 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /dnssec-sign: 60 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /cache-status: 1 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /change-request: 4 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 27% | Mem 60% | Disk 73% | Conns: 1218 | Net: 70/98 Mbps\n- bind-slave-02: CPU 21% | Mem 48% | Disk 55% | Conns: 1030 | Net: 58/74 Mbps\n- cloud-dns-sync-01: CPU 45% | Mem 66% | Disk 48% | Conns: N/A | Net: 90/128 Mbps\n- signer-01: CPU 31% | Mem 70% | Disk 60% | Conns: N/A | Net: 14/18 Mbps\n\n### Connection Pools\n- dns-queries: active 120 | idle 30 | waiting 5 | exhaustion: 0 | max: 200 | avg_wait: 10ms\n- zone-transfers: active 15 | idle 5 | waiting 2 | exhaustion: 0 | max: 20 | avg_wait: 15ms\n- propagation: active 10 | idle 3 | waiting 1 | exhaustion: 0 | max: 15 | avg_wait: 8ms\n- cache-lookup: active 50 | idle 25 | waiting 4 | exhaustion: 0 | max: 80 | avg_wait: 12ms\n\n### CDN & Caching\n- Hit rate: 88.9% | Bandwidth: 2.5 Gbps | Origin requests: 1500\n\n### Alerts\n- [INFO] DNS propagation latency on cloud-dns-sync-01: 60s average, 142s p95\n- [WARNING] Stale answer percentage at 0.2%\n- [ERROR] Zone transfer failures: 0\n- [ERROR] Tsig failures: 0\n- [ERROR] Format errors: 5\n- [ERROR] Refused queries: 22\n- [ERROR] Rate limited queries: 8\n- [ERROR] Truncation TC: 0.4%\n- [INFO] DNSSEC zones signed: 60\n- [INFO] Signature refreshes: 117\n- [INFO] Change requests: 4, applied: 4, failed: 0\n- [INFO] TTL records changed: 40 across 6 zones; min: 300s, max: 43200s, median: 3600s\n- [INFO] Propagation to 35 sites; avg: 60s, p95: 142s\n- [INFO] Health: authoritative reachability 99.99%, SOA mismatch 0, lame delegation 0\n- [INFO] Zone transfer requests: 31 axfr, 184 ixfr; success: 100% axfr, 98.9% ixfr; avg transfer time: 495ms, p95: 1105ms\n- [INFO] DNS queries: QPS avg 20900, peak 49300; NX domain 2%, SERVFAIL 0.04%; UDP 97%, TCP fallback 1.2%\n- [INFO] Cache behavior: resolver hit rate 88.9%, negative cache 38.3%, prefetch 72%\n- [INFO] NTP offset: 1ms across all hosts\n\n### Events\n- Completed weekend maintenance: rotated cloud API credentials for DNS sync controller; verified least-privilege roles and successful test applies.\n- Ran end-to-end propagation drill on 20 representative records; p95 propagation improved after lowering batch size per change set.\n- Decommissioned one on-prem monitoring agent that was generating format error noise due to outdated query syntax.\n- Updated runbook sections for DS publishing workflow and registrar coordination checklist.\n\n### On-Call\n- Shift: D. Wallace. 0 pages. 3 tickets. Status: Quiet shift; preventive maintenance completed successfully.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_039",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-27T10:30:00",
          "text": "## 2026-01-11 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS 52 | p50: 820ms p95: 2100ms p99: N/A | err: 5.8% (4 failures) | success: 94.2%\n- /ttl-changes: REQUESTS 10 | p50: N/A p95: N/A p99: N/A | err: 0% | success: 100%\n- /propagation: REQUESTS N/A | p50: 120s p95: 320s p99: N/A | err: 0% | success: 100%\n- /dns-queries: QPS avg: 22000 | peak: 52200 | nx_domain: 2.4% | servfail: 0.16% | udp: 95.3% | tcp_fallback: 2.8%\n- /dnssec: zones signed: 62 | signing queue depth: 7 | signature refreshes: 149 | ds mismatch: 0 | bogus validation events: 6\n- /health-checks: authoritative reachability: 99.92% | SOA serial mismatch: 7 | lame delegation: 1\n- /change-mgmt: change requests: 8 | applied: 6 | failed: 2 | mean apply time: 41 min\n- /cache-behavior: resolver cache hit: 85.7% | negative cache hit: 45.0% | prefetch enabled: 72.0%\n\n### Infrastructure\n- bind-master-01: CPU 58% | Mem 72% | Disk 74% | Conns: 1560 | Net: 140/190 Mbps\n- bind-slave-02: CPU 45% | Mem 66% | Disk 56% | Conns: 1422 | Net: 120/135 Mbps\n- cloud-dns-sync-01: CPU 83% | Mem 86% | Disk 50% | Conns: N/A | Net: 170/210 Mbps | Queue depth: 18\n- signer-01: CPU 66% | Mem 85% | Disk 61% | Conns: N/A | Net: 28/32 Mbps | HSM slot utilization: 70%\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (No specific pool data provided)\n\n### CDN & Caching\n- Hit rate: N/A | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [SEVERE] TSIG failures on transfers on hostname: inconsistent key algorithm setting\n- [WARNING] Propagation p95 latency above 300s\n- [CRITICAL] Transfer sync controller queue depth peaked at 18\n- [INFO] TTL rollback executed on records with minimum TTL 60 to 300\n\n### Deployments & Changes\n- Emergency TTL increase from 60 to 300 on frequently updated records; one rollback executed\n- Signature refresh scheduling tuned for signer-01\n- Transfer auth settings aligned after TSIG failure incidents\n\n### Events\n- High-volume dynamic updates from internal tooling increased IXFR churn; sync controller queue depth peaked at 18 and propagation p95 rose above 300s\n- TSIG failures observed on transfers for a subset of newly migrated zones; traced to inconsistent key algorithm setting (hmac-sha256 vs hmac-sha512) between master and cloud\n- Emergency change: temporarily raised minimum TTL from 60 to 300 on frequently updated records to reduce churn; one rollback where short TTL was contractually required\n- Signer load increased due to rapid update cadence; tuned signature refresh scheduling and confirmed bogus validation events remained low\n\n### On-Call\n- Shift: E. Romero. 5 pages. 11 tickets. Status: Degraded during update storm; mitigations applied and transfer auth settings aligned",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_040",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-27T10:30:00",
          "text": "## 2026-01-12 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 560ms p95: 1290ms p99: N/A | err: 1.9% (1 errors) | success: 98.1%\n- /ixfr-requests: REQUESTS req | p50: 560ms p95: 1290ms p99: N/A | err: 0.8% (2 errors) | success: 99.2%\n- /ttl-changes: REQUESTS req | p50: 560ms p95: 1290ms p99: N/A | err: N/A | success: N/A\n- /propagation: REQUESTS req | p50: 74s p95: 178s p99: N/A | err: N/A | success: N/A\n- /dns-queries: QPS avg: 21600 | peak: 51000 | nx_domain: 2.1% | servfail: 0.06% | udp: 96.6% | tcp_fallback: 1.6%\n- /dnssec-signing: zones signed: 62 | signing queue depth: 2 | signature refreshes: 123 | ds mismatch: 0 | bogus validation events: 2\n- /health-checks: authoritative reachability: 99.98% | SOA serial mismatch: 1 | lame delegations: 0\n- /change-mgmt: requests: 5 | applied: 5 | failed: 0 | mean apply time: 20 min\n- /cache-behavior: resolver cache hit: 87.6% | negative cache hit: 40.2% | prefetch enabled: 74.0%\n- /latency: auth response avg: 18.7ms | p95: 45.6ms | EDNS0 present: 72.8%\n- /errors: format errors: 11 | refused: 30 | rate limited: 15 | truncation TC: 0.7%\n- /capacity: zones: 576 | records: 2,041,500 | dynamic updates/hr: 340\n\n### Infrastructure\n- bind-master-01: CPU 36% | Mem 64% | Disk 74% | Conns: 1340 | Net: 92/125 Mbps\n- bind-slave-02: CPU 28% | Mem 54% | Disk 56% | Conns: 1145 | Net: 72/88 Mbps\n- cloud-dns-sync-01: CPU 56% | Mem 71% | Disk 50% | Conns: N/A | Net: 110/145 Mbps | Queue depth: 6\n- signer-01: CPU 42% | Mem 76% | Disk 61% | Conns: N/A | Net: 20/22 Mbps | HSM slot utilization: 62%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 87.6% | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [INFO] TSIG algorithm standardization completed on hostname: remaining zones\n- [INFO] Dynamic update frequency reduced to 180s in internal tooling\n- [INFO] SOA serial reconciliation job completed; one zone manually bumped\n- [INFO] Alert thresholds for signer queue depth updated\n\n### Deployments & Changes\n- Standardized TSIG algorithms across remaining zones; confirmed transfer success rates recovered\n- Reduced dynamic update frequency from 60s to 180s; propagation p95 improved\n- Ran SOA serial reconciliation; manually bumped one zone\n- Updated alert thresholds for signer queue depth during update storm\n\n### Events\n- Post-incident cleanup: standardized TSIG algorithms across remaining zones; confirmed transfer success rates recovered\n- Reduced dynamic update frequency in internal tooling from 60s to 180s where acceptable; saw queue depth and propagation p95 improve\n- Ran SOA serial reconciliation job; only one zone required manual bump due to prior failed change set\n- Updated alert thresholds for signer queue depth based on observed load patterns during update storm\n\n### On-Call\n- shift handoff note: T. Kowalski. 2 pages. 6 tickets. Status: Recovering; metrics trending back to baseline after configuration standardization",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_014",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-28T10:00:00",
          "text": "## 2024-01-28 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 136900 | p50: 140ms p95: 360ms p99: 650ms | err: 0.82% (1120 errors) | success: 99.18%\n- /fraud_check: REQUESTS 136500 | p50: 92ms p95: 255ms p99: 420ms | err: 0.19% (260 errors) | success: 99.81%\n- /geo_lookup: REQUESTS 134100 | p50: 130ms p95: 310ms p99: 400ms | err: 0.12% (160 errors) | success: 99.88%\n- /auth: REQUESTS 178200 | p50: 34ms p95: 90ms p99: 150ms | err: 0.06% (110 errors) | success: 99.94%\n- /product_catalog: REQUESTS 238800 | p50: 42ms p95: 118ms p99: 200ms | err: 0.07% (160 errors) | success: 99.93%\n- /search: REQUESTS 206500 | p50: 62ms p95: 160ms p99: 270ms | err: 0.12% (240 errors) | success: 99.88%\n- /recommendations: REQUESTS 167800 | p50: 72ms p95: 200ms p99: 340ms | err: 0.14% (240 errors) | success: 99.86%\n\n### Infrastructure\n- gateway-01: CPU 55% | Mem 69% | Disk 62% | Conns: 3050 | Net: 540/640 Mbps\n- gateway-02: CPU 52% | Mem 67% | Disk 61% | Conns: 2920 | Net: 525/622 Mbps\n- service-b-01: CPU 50% | Mem 65% | Disk 58% | Conns: 2150 | Net: 360/410 Mbps\n- metrics-db-01: CPU 33% | Mem 72% | Disk 83% | Conns: 290 | Net: 128/155 Mbps\n\n### Connection Pools\n- primary: active 150 | idle 50 | waiting 18 | exhaustion: 1 | max: 200 | avg_wait: 18ms\n- replica: active 65 | idle 35 | waiting 1 | exhaustion: 0 | max: 100 | avg_wait: 2ms\n- third_party_geo: active 56 | idle 4 | waiting 15 | exhaustion: 2 | max: 60 | avg_wait: 45ms\n\n### CDN & Caching\n- Hit rate: 93.2% | Bandwidth: 7.6 Gbps | Origin requests: 382900\n\n### Alerts\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=0.82\n- [WARNING] CONNPOOL-WAIT on gateway-01: pool=primary waiting=18 avg_wait_ms=18\n- [WARNING] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=400\n\n### Deployments & Changes\n- Deployed service-C (checkout) v3.2.1\n\n### Events\n- Service-C deploy completed with schema migration (backward compatible)\n- A/B test 'checkout-v2' at 15% rollout\n- metrics-db vacuum executed 2024-01-28 01:00Z\n- Scaled checkout-worker pool from 8 to 10 instances\n\n### On-Call\n- Shift: L. Nguyen. 5 pages (CHECKOUT-ERROR-RATE, CONNPOOL-WAIT). 2 tickets (CO-778 rollback plan, DB-902 pool wait). Status: Investigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_041",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-28T10:30:00",
          "text": "## 2026-01-13 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 21950 req | p50: 17.5ms p95: 42.0ms p99: N/A | err: 0.07% (15 errors) | success: 99.93%\n- /zone-transfer: REQUESTS 240 req | p50: 520ms p95: 1200ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /dnssec-sign: REQUESTS N/A req | p50: N/A ms p95: N/A ms p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A req | p50: N/A ms p95: N/A ms p99: N/A | err: N/A | success: N/A\n- /config-change: REQUESTS 6 req | p50: N/A ms p95: N/A ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /cache-status: REQUESTS N/A req | p50: N/A ms p95: N/A ms p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 30% | Mem 62% | Disk 74% | Conns: 1270 | Net: 80/112 Mbps\n- bind-slave-02: CPU 23% | Mem 50% | Disk 56% | Conns: 1058 | Net: 60/76 Mbps\n- cloud-dns-sync-01: CPU 48% | Mem 67% | Disk 50% | Conns: N/A | Net: 96/132 Mbps\n- signer-01: CPU 35% | Mem 72% | Disk 61% | Conns: N/A | Net: 16/20 Mbps\n\n### Connection Pools\n- pool-dns-queries: active 150 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- pool-zone-transfers: active 10 | idle 2 | waiting 1 | exhaustion 0 | max 15 | avg_wait 25ms\n- pool-signing: active 1 | idle 0 | waiting 0 | exhaustion 0 | max 2 | avg_wait 5ms\n- pool-monitoring: active 3 | idle 0 | waiting 0 | exhaustion 0 | max 5 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 88.4% | Bandwidth: 2.5 Gbps | Origin requests: 3200\n\n### Alerts\n- [INFO] DNSSEC signature refresh on zone example.com: 120 signatures refreshed\n- [WARNING] Stale answer percentage at 0.3% on edge sites\n- [ERROR] Bogus validation event detected in DNSSEC validation\n- [CRITICAL] Propagation delay at cloud edge site 12: 158s\n- [INFO] TTL change applied to 8 zones, 62 records updated\n- [INFO] Change requests processed: 6, all applied successfully\n- [INFO] Negative cache hit rate: 39%\n- [INFO] Resolver cache hit rate: 88.4%\n- [INFO] Propagation average time: 66s\n- [INFO] Propagation 95th percentile: 158s\n- [INFO] Zone transfers AXFR requests: 34, success 100%\n- [INFO] Zone transfers IXFR requests: 206, success 99%\n- [INFO] AXFR failures: 0\n- [INFO] IXFR failures: 2\n- [WARNING] TTL min: 300s, max: 43200s, median: 3600s\n- [INFO] DNS queries peak QPS: 51800\n- [INFO] DNS queries average QPS: 21950\n- [INFO] NXDOMAIN percentage: 2%\n- [INFO] SERVFAIL percentage: 0.05%\n- [INFO] UDP query percentage: 97%\n- [INFO] TCP fallback percentage: 1.3%\n- [INFO] DNSSEC zones signed: 64\n- [INFO] Signature refreshes: 120\n- [INFO] Signing queue depth: 1\n- [INFO] DS mismatch count: 0\n- [INFO] Bogus validation events: 1\n- [INFO] Zonefile lint errors: 3\n- [INFO] Authority reachability: 99.99%\n- [INFO] Lame delegation count: 0\n- [INFO] SOA serial mismatch: 0\n- [INFO] Dynamic updates per hour: 320\n\n### Events\n- Migrated 16 zones with DNSSEC enabled from day one; verified DS publishing order and validated from external resolvers.\n- Adjusted monitoring to include per-zone propagation SLO and stale answer sampling; initial baselines captured.\n- Removed legacy allow-transfer ACL entries on BIND master for retired slaves; confirmed no unintended AXFR blocks for cloud endpoints.\n- Reviewed negative caching behavior; tuned SOA minimum values for 5 zones to reduce prolonged NXDOMAIN caching after record creation.\n\n### On-Call\n- shift handoff note: P. Singh. 1 pages. 5 tickets. Status: Stable; successful DNSSEC-enabled migrations and improved monitoring coverage.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_042",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-28T10:30:00",
          "text": "## 2026-01-14 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 22300 req | p50: 19.6ms p95: 48.8ms p99: N/A | err: 0.1% (27 errors) | success: 99.9%\n- /zone-transfer: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /ttl-change: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /dnssec-sign: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 39% | Mem 65% | Disk 75% | Conns: 1338 | Net: 92/130 Mbps\n- bind-slave-02: CPU 29% | Mem 53% | Disk 56% | Conns: 1116 | Net: 71/90 Mbps\n- cloud-dns-sync-01: CPU 59% | Mem 73% | Disk 51% | Conns: N/A | Net: 115/154 Mbps\n- signer-01: CPU 46% | Mem 78% | Disk 61% | Conns: N/A | Net: 19/24 Mbps\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- transfer-pool: active 4 | idle 2 | waiting 1 | exhaustion 0 | max 10 | avg_wait 12ms\n- dnssec-sign: active 1 | idle 2 | waiting 0 | exhaustion 0 | max 5 | avg_wait 8ms\n- cache-refresh: active 3 | idle 4 | waiting 2 | exhaustion 1 | max 8 | avg_wait 15ms\n\n### CDN & Caching\n- Hit rate: 87.1% | Bandwidth: 2.3 Gbps | Origin requests: 1500\n\n### Alerts\n- [WARNING] TSIG failure on cloud-dns-sync-01: 1 failure\n- [CRITICAL] Propagation latency high on edge site: 205s p95\n- [INFO] DNSSEC bogus validation events: 3\n- [ERROR] SOA serial mismatch in zones: 2\n- [WARNING] Lame delegation detected: 0\n\n### Deployments & Changes\n- Applied delayed batch for cloud sync controller post-maintenance; observed temporary p95 increase; verified eventual consistency.\n- Enforced change policy: disabled console edits for production zones after zone mismatch detection.\n- Corrected NS target formatting and removed duplicate TXT segments after lint issues in partner subzones.\n- Created dashboard for transfer latency and TSIG failures; set alerts for p95 > 1800ms and failures > 2/hour.\n\n### Events\n- Cloud sync controller applied delayed batch after maintenance window; observed temporary propagation p95 increase; verified eventual consistency across edges.\n- Two zones showed SOA serial mismatch after concurrent edits in git and manual console change; enforced change policy: console edits disabled for production projects.\n- Lint detected 6 issues after importing partner-managed subzones; corrected NS target formatting and removed duplicate TXT segments.\n- Created new dashboard for transfer latency and TSIG failures; set alerts on p95 > 1800ms and failures > 2 per hour.\n\n### On-Call\n- shift handoff note: C. Martinez. 2 pages. 7 tickets. Status: Stable; addressed process gap around console edits and improved observability.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_043",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-28T10:30:00",
          "text": "## 2026-01-15 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 22500 req | p50: 17ms p95: 41.1ms p99: N/A | err: 0.07% (16 errors) | success: 99.93%\n- /zone-transfer: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /dnssec-sign: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /cache-purge: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /change-request: REQUESTS 6 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 28% | Mem 61% | Disk 75% | Conns: N/A | Net: 78/108 Mbps\n- bind-slave-02: CPU 22% | Mem 49% | Disk 56% | Conns: N/A | Net: 60/77 Mbps\n- cloud-dns-sync-01: CPU 47% | Mem 66% | Disk 51% | Conns: N/A | Net: 98/136 Mbps\n- signer-01: CPU 33% | Mem 72% | Disk 61% | Conns: N/A | Net: 15/20 Mbps\n\n### Connection Pools\n- pool-axfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-ixfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-dnssec: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-ttl: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-propagation: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n\n### CDN & Caching\n- Hit rate: 88.7% | Bandwidth: 0.9 Gbps | Origin requests: N/A\n\n### Alerts\n- [INFO] DNSSEC bogus validation event on signer-01: 1 event\n- [INFO] Propagation to 36 cloud edge sites completed; average propagation time 63s, p95: 150s\n- [INFO] Zone transfer success rate: AXFR 99.0%, IXFR 99.5%\n- [INFO] TTL changes applied: 48 records across 7 zones; min TTL 300s, max TTL 43200s, median 3600s\n- [INFO] Change requests processed: 6; all applied successfully; mean apply time 18 min\n- [INFO] DNSSEC zones signed: 68; signature refreshes: 122; DS mismatch: 0; bogus events: 1\n- [INFO] Health checks: authoritative reachability 99.99%; SOA mismatch 0; lame delegations 0\n\n### Deployments & Changes\n- Expanded to 36 cloud edge sites; validated reachability and observed improved p95 propagation after edge addition\n- Migrated next batch of 16 zones; executed DNS record parity tests including SRV and CAA records; no discrepancies found\n- Removed remaining console write permissions for production DNS project; all changes now via IaC pipeline with approvals\n- Completed DNSSEC ZSK rotation for 10 zones; monitored for bogus validations and found none material\n\n### Events\n- Expanded to 36 cloud edge sites; validated reachability and observed improved p95 propagation after edge addition\n- Migrated next batch of 16 zones; executed DNS record parity tests including SRV and CAA records; no discrepancies found\n- Removed remaining console write permissions for production DNS project; all changes now via IaC pipeline with approvals\n- Completed DNSSEC ZSK rotation for 10 zones; monitored for bogus validations and found none material\n\n### On-Call\n- shift handoff note: H. Kim. 1 pages. 4 tickets. Status: Normal; successful batch migration and security hardening for change controls",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_015",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-29T10:00:00",
          "text": "## 2024-01-29 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 138500 | p50: 135ms p95: 350ms p99: 620ms | err: 0.71% (980 errors) | success: 99.29%\n- /fraud_check: REQUESTS 138100 | p50: 90ms p95: 250ms p99: 410ms | err: 0.17% (240 errors) | success: 99.83%\n- /geo_lookup: REQUESTS 135800 | p50: 135ms p95: 320ms p99: 410ms | err: 0.13% (170 errors) | success: 99.87%\n- /auth: REQUESTS 180500 | p50: 33ms p95: 88ms p99: 148ms | err: 0.06% (108 errors) | success: 99.94%\n- /product_catalog: REQUESTS 241900 | p50: 41ms p95: 116ms p99: 198ms | err: 0.07% (158 errors) | success: 99.93%\n- /search: REQUESTS 209200 | p50: 61ms p95: 158ms p99: 268ms | err: 0.11% (238 errors) | success: 99.89%\n- /recommendations: REQUESTS 169900 | p50: 70ms p95: 196ms p99: 335ms | err: 0.14% (235 errors) | success: 99.86%\n\n### Infrastructure\n- gateway-01: CPU 53% | Mem 68% | Disk 62% | Conns: 2925 | Net: 535/632 Mbps\n- gateway-02: CPU 50% | Mem 66% | Disk 61% | Conns: 2810 | Net: 520/615 Mbps\n- service-b-01: CPU 49% | Mem 65% | Disk 58% | Conns: 2105 | Net: 350/402 Mbps\n- metrics-db-01: CPU 32% | Mem 72% | Disk 83% | Conns: 288 | Net: 126/154 Mbps\n\n### Connection Pools\n- primary: active 148 | idle 52 | waiting 16 | exhaustion: 1 | max: 200 | avg_wait: 16ms\n- replica: active 64 | idle 36 | waiting 1 | exhaustion: 0 | max: 100 | avg_wait: 2ms\n- third_party_geo: active 56 | idle 4 | waiting 14 | exhaustion: 2 | max: 60 | avg_wait: 42ms\n\n### CDN & Caching\n- Hit rate: 93.1% | Bandwidth: 7.5 Gbps | Origin requests: 379400\n\n### Alerts\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-02: error_pct=0.71\n- [WARNING] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=410\n\n### Deployments & Changes\n- Rolled back service-C (checkout) to v3.2.0\n\n### Events\n- Service-C rollback completed\n- A/B test 'checkout-v2' at 15% rollout\n- Temporary disable of verbose checkout debug logging\n- Updated gateway circuit breaker settings for /checkout upstream calls\n\n### On-Call\n- Shift: M. Rossi. 4 pages (CHECKOUT-ERROR-RATE). 2 tickets (CO-781 error review, GW-330 circuit breaker). Status: Investigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "early_signal",
            "signal_density": "low"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_044",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-29T10:30:00",
          "text": "## 2026-01-16 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 760ms p95: 1950ms p99: N/A | err: 6.7% (2 errors) | success: 93.3%\n- /axfr-requests: REQUESTS req | p50: 760ms p95: 1950ms p99: N/A | err: 6.7% (2 errors) | success: 93.3%\n- /ixfr-requests: REQUESTS req | p50: 760ms p95: 1950ms p99: N/A | err: 2.1% (4 errors) | success: 97.9%\n- /ttl-changes: REQUESTS req | p50: 1800s p95: N/A p99: N/A | err: 0% | success: 100%\n- /propagation: REQUESTS req | p50: 98ms p95: 275ms p99: N/A | err: 0% | success: 100%\n- /dns-queries: QPS avg: 22800 | QPS peak: 55200 | NX domain: 2.3% | Servfail: 0.13% | UDP: 95.7% | TCP fallback: 2.1%\n- /dnssec-signing: ZONES signed: 70 | signing queue depth: 4 | signature refreshes: 140 | ds_mismatch: 0 | bogus validation events: 5\n- /health-checks: authoritative reachability: 99.94% | SOA serial mismatch: 4 | lame delegation: 1\n- /change-mgmt: change requests: 5 | applied: 4 | failed: 1 | mean apply time: 37 min\n- /cache-behavior: resolver cache hit: 86.0% | negative cache hit: 44.2% | prefetch enabled: 76.0%\n- /latency: auth response avg: 22.6ms | auth response p95: 61.4ms | EDNS0 present: 72%\n- /errors: format errors: 18 | refused: 39 | rate limited: 28 | truncation TC: 1.2%\n- /capacity: zones: 608 | records: 2,157,600 | dynamic updates/hr: 390\n- /compliance: zonefile lint errors: 8 | dnssec policy violations: 1 | audit log gaps: 0\n\n### Infrastructure\n- bind-master-01: CPU 52% | Mem 70% | Disk 76% | Conns: 1492 | Net: 126/172 Mbps\n- bind-slave-02: CPU 40% | Mem 62% | Disk 57% | Conns: 1368 | Net: 105/120 Mbps\n- cloud-dns-sync-01: CPU 78% | Mem 83% | Disk 52% | Conns: N/A | Net: 155/198 Mbps | Queue depth: 14\n- signer-01: CPU 60% | Mem 84% | Disk 62% | Conns: N/A | Net: 26/30 Mbps | HSM slot utilization: 69%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: N/A | avg_wait: N/A\n\n### CDN & Caching\n- Hit rate: 86.0% | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [WARNING] TSIG failures on zones: 2\n- [INFO] Propagation delays increased after bulk TXT update\n- [ERROR] Mismatched TSIG key secret in sync controller\n- [ERROR] Record-set size limit exceeded for IaC change\n- [WARNING] Lame delegation detected for subzone\n\n### Deployments & Changes\n- Rotated and redeployed TSIG secret store entry after failures\n- Split record sets for large TXT entries and re-applied\n- Corrected parent NS set after lame delegation detection\n\n### Events\n- Propagation delays increased after a large bulk TXT update (DKIM keys) across 6 zones; queue depth rose and stale answer sampling hit 1.2%\n- TSIG failures reappeared for two zones; discovered mismatched TSIG key secret due to stale secret in sync controller; rotated and redeployed secret store entry\n- One IaC change failed due to exceeding provider record-set size limits for aggregated TXT entries; split records and re-applied successfully\n- Detected lame delegation for one subzone after registrar glue update; corrected parent NS set and confirmed authoritative reachability\n\n### On-Call\n- shift: N. Brooks. 4 pages. 10 tickets. Status: Degraded during bulk TXT change; restored normal after secret rotation and record-set refactor",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_045",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-29T10:30:00",
          "text": "## 2026-01-17 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 545ms p95: 1275ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /dns-queries: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /dnssec-sign: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-checks: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /change-management: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 30% | Mem 62% | Disk 76% | Conns: 1262 | Net: 80/110 Mbps\n- bind-slave-02: CPU 23% | Mem 50% | Disk 57% | Conns: 1065 | Net: 61/78 Mbps\n- cloud-dns-sync-01: CPU 50% | Mem 68% | Disk 52% | Conns: N/A | Net: 98/138 Mbps\n- signer-01: CPU 34% | Mem 73% | Disk 62% | Conns: N/A | Net: 16/20 Mbps\n\n### Connection Pools\n- pool-axfr: active 35 | idle 10 | waiting 2 | exhaustion: 0 | max: 50 | avg_wait: 12ms\n- pool-ixfr: active 196 | idle 20 | waiting 5 | exhaustion: 1 | max: 250 | avg_wait: 15ms\n- pool-propagation: active 4 | idle 1 | waiting 0 | exhaustion: 0 | max: 10 | avg_wait: 8ms\n- pool-signing: active 1 | idle 0 | waiting 0 | exhaustion: 0 | max: 2 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 88.5% | Bandwidth: 2.3 Gbps | Origin requests: 22,000\n\n### Alerts\n- [INFO] DNS propagation latency on cloud-dns-sync-01: 66s\n- [WARNING] stale answer rate at 0.3%\n- [ERROR] bogus validation event in DNSSEC zone signing\n- [CRITICAL] N/A\n\n### Deployments & Changes\n- Migrated 16 additional zones; added automated check for provider record-set size constraints\n- Rotated sync controller secret store entries with version pinning\n- Validated DS and DNSKEY alignment for all signed zones\n- Documented secret rotation procedures in runbook\n- Scheduled next KSK ceremony tabletop exercise\n\n### Events\n- Post-bulk-change verification: compared TXT records across 20 external resolvers; stale answer rate returned to baseline\n- Migrated 16 zones; introduced automated provider record-set size check\n- Rotated sync controller secret store entries with version pinning; documented in runbook\n- Validated DS and DNSKEY alignment for all signed zones; scheduled next KSK ceremony tabletop exercise\n\n### On-Call\n- Shift: I. Hassan. 1 pages. 5 tickets. Status: Stable; preventive automation added and bulk-change effects resolved.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_046",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-29T10:30:00",
          "text": "## 2026-01-18 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 500ms p95: 1160ms p99: 1160ms | err: 1.8% (28 errors) | success: 98.2%\n- /ixfr-requests: REQUESTS req | p50: 500ms p95: 1160ms p99: 1160ms | err: 0.6% (1 error) | success: 99.4%\n- /ttl-changes: REQUESTS req | p50: 1800ms p95: 1800ms p99: 1800ms | err: 0% (0 errors) | success: 100%\n- /propagation: REQUESTS req | p50: 61ms p95: 145ms p99: 145ms | err: 0% (0 errors) | success: 100%\n- /dns-queries: QPS avg: 22450 | QPS peak: 54800 | NX domain: 2.0% | Servfail: 0.04% | UDP: 97.2% | TCP fallback: 1.1%\n- /dnssec-signing: zones signed: 72 | signing queue depth: 1 | signature refreshes: 115 | ds mismatch: 0 | bogus validation: 0\n- /health-checks: authoritative reachability: 99.99% | SOA serial mismatch: 0 | lame delegations: 0\n- /change-mgmt: change requests: 5 | changes applied: 5 | changes failed: 0 | mean apply time: 16 min\n- /cache-behavior: resolver cache hit: 89.1% | negative cache hit: 38.0% | prefetch enabled: 78.0%\n- /latency: auth response avg: 16.5ms | auth response p95: 39.4ms | EDNS0 present: 74.8%\n- /errors: format errors: 5 | refused: 20 | rate limited: 8 | truncation TC: 0.3%\n- /capacity: zones: 624 | records: 2,212,100 | dynamic updates/hr: 310\n- /compliance: zonefile lint errors: 2 | dnssec policy violations: 0 | audit log gaps: 0\n\n### Infrastructure\n- bind-master-01: CPU 25% | Mem 60% | Disk 76% | Conns: 1208 | Net: 72/102 Mbps\n- bind-slave-02: CPU 20% | Mem 48% | Disk 57% | Conns: 1012 | Net: 57/73 Mbps\n- cloud-dns-sync-01: CPU 44% | Mem 65% | Disk 52% | Conns: N/A | Net: 92/130 Mbps | Queue depth: 2\n- signer-01: CPU 29% | Mem 71% | Disk 62% | Conns: N/A | Net: 14/18 Mbps | HSM slot utilization: 62%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 89.1% | Bandwidth: 0.0 Gbps | Origin requests: 0\n\n### Alerts\n- [INFO] alert-name on hostname: value\n\n### Deployments & Changes\n- Implemented CAA record standard across 7 zones; verified no unintended impacts on issuance workflows and ensured consistent TTL 1800.\n- Tuned negative caching SOA minimum in two zones to align with policy; reduced prolonged NXDOMAIN caching risk during migrations.\n- Enabled additional resolver prefetch coverage (78%); observed small improvement in cache hit rate.\n- Conducted spot checks for glue record correctness at registrar for newly migrated zones; no issues found.\n\n### Events\n- Implemented CAA record standard across 7 zones; verified no unintended impacts on issuance workflows and ensured consistent TTL 1800.\n- Tuned negative caching SOA minimum in two zones to align with policy; reduced prolonged NXDOMAIN caching risk during migrations.\n- Enabled additional resolver prefetch coverage (78%); observed small improvement in cache hit rate.\n- Conducted spot checks for glue record correctness at registrar for newly migrated zones; no issues found.\n\n### On-Call\n- shift handoff note: G. Thompson. 0 pages. 4 tickets. Status: Normal; standards work completed with clean validation.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_047",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-29T10:30:00",
          "text": "## 2026-01-19 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS 44 | p50: 610ms p95: 1485ms p99: N/A | err: 2.3% (1 errors) | success: 97.7%\n- /ixfr-requests: REQUESTS 224 | p50: 610ms p95: 1485ms p99: N/A | err: 1.3% (3 errors) | success: 98.7%\n- /ttl-changes: REQUESTS 9 | p50: 3600s p95: N/A p99: N/A | err: 0% | success: 100%\n- /propagation: REQUESTS 36 | p50: 73s p95: 182s p99: N/A | err: 0% | success: 100%\n- /dns-queries: QPS avg: 23100 | QPS peak: 56500 | nx_domain: 2.1% | servfail: 0.06% | udp: 96.8% | tcp_fallback: 1.3%\n- /dnssec: zones_signed: 74 | signing_queue_depth: 2 | signature_refreshes: 124 | ds_mismatch: 0 | bogus_validation_events: 2\n- /health-checks: authoritative_reachability: 99.98% | soa_serial_mismatch: 1 | lame_delegation: 0\n- /change-mgmt: change_requests: 7 | changes_applied: 7 | changes_failed: 0 | mean_time_to_apply: 20min\n- /cache-behavior: resolver_cache_hit: 88.2% | negative_cache_hit: 39.4% | prefetch_enabled: 78%\n- /latency: auth_response_avg: 18.1ms | auth_response_p95: 44.6ms | edns0_present: 74%\n- /errors: format_error: 9 | refused: 24 | rate_limited: 12 | truncation_tc: 0.5%\n- /capacity: zones: 640 | records: 2,269,800 | dynamic_updates/hr: 345\n- /compliance: zonefile_lint_errors: 4 | dnssec_policy_violations: 0 | audit_log_gaps: 0\n\n### Infrastructure\n- bind-master-01: CPU 34% | Mem 63% | Disk 77% | Conns: 1290 | Net: 88/120 Mbps\n- bind-slave-02: CPU 26% | Mem 51% | Disk 57% | Conns: 1098 | Net: 65/84 Mbps\n- cloud-dns-sync-01: CPU 55% | Mem 70% | Disk 53% | Conns: N/A | Net: 110/148 Mbps | Queue depth: 5\n- signer-01: CPU 40% | Mem 75% | Disk 63% | Conns: N/A | Net: 17/22 Mbps | HSM slot utilization: 64%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: N/A | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 88.2% | Bandwidth: 78 Mbps | Origin requests: N/A\n\n### Alerts\n- [INFO] TSIG warning on hostname: confirmed time sync and reloaded key\n- [INFO] SOA serial mismatch on hostname: resolved by rebuilding zone and reapplying\n- [INFO] Rate limiting policy updated for malformed queries\n\n### Deployments & Changes\n- Migrated 16 zones; added automated SRV record validation for SIP-related services to ensure priority/weight integrity\n- Updated rate limiting policy for malformed queries; format errors decreased after tuning drop thresholds\n\n### Events\n- Migrated 16 zones; added automated SRV record validation for SIP-related services to ensure priority/weight integrity\n- One SOA serial mismatch flagged after manual hotfix in git; resolved by rebuilding zone and reapplying through pipeline\n- Minor TSIG warning observed on one transfer; confirmed time sync and reloaded key; no recurrence\n- Updated rate limiting policy for malformed queries; format errors decreased after tuning drop thresholds\n\n### On-Call\n- shift handoff note: V. Petrov. 1 pages. 6 tickets. Status: Stable; minor serial mismatch corrected and new validation checks added",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_016",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-30T10:00:00",
          "text": "## 2024-01-30 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 140800 | p50: 155ms p95: 410ms p99: 780ms | err: 1.56% (2200 errors) | success: 98.44%\n- /fraud_check: REQUESTS 140300 | p50: 105ms p95: 310ms p99: 520ms | err: 0.3% (420 errors) | success: 99.7%\n- /geo_lookup: REQUESTS 137900 | p50: 190ms p95: 420ms p99: 600ms | err: 0.19% (260 errors) | success: 99.81%\n- /auth: REQUESTS 183200 | p50: 36ms p95: 96ms p99: 165ms | err: 0.09% (160 errors) | success: 99.91%\n- /product_catalog: REQUESTS 246300 | p50: 45ms p95: 130ms p99: 230ms | err: 0.09% (210 errors) | success: 99.91%\n- /search: REQUESTS 213900 | p50: 66ms p95: 175ms p99: 300ms | err: 0.15% (320 errors) | success: 99.85%\n- /recommendations: REQUESTS 173500 | p50: 78ms p95: 220ms p99: 380ms | err: 0.18% (310 errors) | success: 99.82%\n\n### Infrastructure\n- gateway-01: CPU 62% | Mem 74% | Disk 62% | Conns: 3780 | Net: 590/705 Mbps\n- gateway-02: CPU 59% | Mem 72% | Disk 61% | Conns: 3605 | Net: 570/685 Mbps\n- service-b-01: CPU 58% | Mem 70% | Disk 59% | Conns: 2850 | Net: 430/510 Mbps\n- metrics-db-01: CPU 36% | Mem 74% | Disk 84% | Conns: 310 | Net: 145/175 Mbps\n\n### Connection Pools\n- primary: active 196 | idle 4 | waiting 85 | exhaustion: 7 | max: 200 | avg_wait: 85ms\n- replica: active 92 | idle 8 | waiting 10 | exhaustion: 1 | max: 100 | avg_wait: 12ms\n- third_party_geo: active 60 | idle 0 | waiting 55 | exhaustion: 10 | max: 60 | avg_wait: 120ms\n\n### CDN & Caching\n- Hit rate: 92.8% | Bandwidth: 7.9 Gbps | Origin requests: 401200\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=primary events=7 waiting=85\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=1.56\n- [CRITICAL] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=600\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Scaled gateway autoscaling max from 6 to 8\n- A/B test 'checkout-v2' at 15% rollout\n- Increased connection pool max_size for third_party_geo from 60 to 60 (no change)\n- Rotated service-B pods (rolling restart)\n\n### On-Call\n- Shift: E. Johnson. 7 pages (CONNPOOL-EXHAUSTION, CHECKOUT-ERROR-RATE). 3 tickets (POOL-552, CO-790, VEND-81). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "red_herring",
            "signal_density": "medium"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_048",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-30T10:30:00",
          "text": "## 2026-01-20 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 515ms p95: 1195ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /axfr-requests: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /ixfr-requests: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.4% (1 errors) | success: 99.6%\n- /ttl-changes: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /propagation: REQUESTS req | p50: 62ms p95: 148ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /dns-queries: QPS avg: 22900 | peak: 55800 | nx_domain: 2.0% | servfail: 0.04% | udp: 97.1% | tcp_fallback: 1.1%\n- /dnssec-signing: ZONES signed: 76 | signing_queue_depth: 1 | signature_refreshes: 116 | ds_mismatch: 0 | bogus_validation: 0\n- /health-checks: authoritative_reachability: 99.99% | soa_serial_mismatch: 0 | lame_delegation: 0\n- /change-mgmt: requests: 5 | applied: 5 | failed: 0 | mean_apply_time: 15min\n- /cache-behavior: resolver_cache_hit: 89.0% | negative_cache_hit: 38.2% | prefetch_enabled: 80.0%\n\n### Infrastructure\n- bind-master-01: CPU 26% | Mem 60% | Disk 77% | Conns: 1225 | Net: 74/104 Mbps\n- bind-slave-02: CPU 21% | Mem 48% | Disk 57% | Conns: 1034 | Net: 58/74 Mbps\n- cloud-dns-sync-01: CPU 45% | Mem 66% | Disk 53% | Conns: N/A | Net: 94/132 Mbps | Queue depth: 2\n- signer-01: CPU 30% | Mem 71% | Disk 63% | Conns: N/A | Net: 14/19 Mbps | HSM slot utilization: 63%\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (No specific pool data provided)\n\n### CDN & Caching\n- Hit rate: 89.0% | Bandwidth: 1.2 Gbps | Origin requests: 22900 * 62s propagation avg\n\n### Alerts\n- [INFO] DNSSEC signing enabled for 2 zones on signer-01\n- [INFO] Registrar audit completed for top 50 zones; no mismatches\n- [INFO] Load test of cloud edge queries successful; latency stable\n- [INFO] Commit hook implemented to block CNAME-at-apex and oversized TXT sets\n\n### Deployments & Changes\n- Enabled DNSSEC signing for 2 additional zones; verified key TTLs and signature windows\n- Performed registrar audit for NS and DS records; no mismatches detected\n- Ran load test of cloud edge queries; confirmed stable response latency and low TCP fallback\n- Implemented commit hook to block CNAME-at-apex and oversized TXT sets before pipeline apply\n\n### Events\n- Completed DNSSEC signing enablement for 2 zones; verified key TTLs and signature validity windows match policy\n- Performed registrar audit for NS and DS records across top 50 zones; no mismatches detected; documented findings for compliance\n- Ran load test of cloud edge queries; confirmed stable response latency and low TCP fallback\n- Implemented commit hook to block CNAME-at-apex and oversized TXT sets before pipeline apply\n\n### On-Call\n- shift handoff note: S. Laurent. 0 pages. 3 tickets. Status: Normal; compliance audit and guardrails improved",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_049",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-30T10:30:00",
          "text": "## 2026-01-21 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 23500 req | p50: 20ms p95: 55ms p99: N/A | err: 0.3% (70 errors) | success: 99.7%\n- /zone-transfer: REQUESTS 285 req | p50: 690ms p95: 1735ms p99: N/A | err: 4.3% (12 errors) | success: 95.7%\n- /dnssec-sign: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /config-change: REQUESTS 8 req | p50: N/A p95: N/A p99: N/A | err: 12.5% (1 error) | success: 87.5%\n- /cache-status: REQUESTS N/A | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 47% | Mem 69% | Disk 78% | Conns: 1450 | Net: 118/160 Mbps\n- bind-slave-02: CPU 35% | Mem 60% | Disk 58% | Conns: 1322 | Net: 98/112 Mbps\n- cloud-dns-sync-01: CPU 72% | Mem 81% | Disk 54% | Conns: N/A | Net: 150/190 Mbps\n- signer-01: CPU 55% | Mem 82% | Disk 64% | Conns: N/A | Net: 24/29 Mbps\n\n### Connection Pools\n- pool-axfr: active 47 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-ixfr: active 238 | idle 0 | waiting 0 | exhaustion 0 | max 250 | avg_wait 0ms\n- pool-ttl: active 10 | idle 0 | waiting 0 | exhaustion 0 | max 15 | avg_wait 0ms\n- pool-propagation: active 36 | idle 0 | waiting 0 | exhaustion 0 | max 40 | avg_wait 0ms\n- pool-signing: active 4 | idle 0 | waiting 0 | exhaustion 0 | max 5 | avg_wait 0ms\n- pool-queries: active 23500 | idle 0 | waiting 0 | exhaustion 0 | max 30000 | avg_wait 0ms\n\n### CDN & Caching\n- Hit rate: 86.8% | Bandwidth: 2.3 Gbps | Origin requests: 58000\n\n### Alerts\n- [SEVERE] lame delegation on hostname: 1\n- [WARNING] SOA serial mismatch count: 3\n- [ERROR] DNSSEC bogus validation events: 4\n- [CRITICAL] TTL rollback occurred on zone: 1\n- [ERROR] NS record missing in child zone delegation\n- [WARNING] zonefile lint errors: 7\n- [WARNING] dnssec policy violations: 1\n\n### Deployments & Changes\n- Migrated 16 zones during peak query period; observed temporary increase in transfer latency and propagation p95; rescheduled remaining migrations to off-peak.\n- One change failed due to missing NS record in child zone delegation; corrected hierarchy and re-applied pipeline.\n- Performed targeted TTL reduction (1800->120) for incident response readiness on 10 zones; rolled back one zone with strict vendor requirements.\n\n### Events\n- Migrated 16 zones during peak query period; observed temporary increase in transfer latency and propagation p95; rescheduled remaining migrations to off-peak.\n- One change failed due to missing NS record in child zone delegation; corrected hierarchy and re-applied pipeline.\n- Lame delegation alert triggered for one subzone after partial registrar update; validated glue and completed registrar correction.\n- Performed targeted TTL reduction (1800->120) for incident response readiness on 10 zones; rolled back one zone with strict vendor requirements.\n\n### On-Call\n- shift handoff note: B. Fischer. 4 pages. 9 tickets. Status: Elevated activity during peak migration; stabilized by rescheduling and fixing delegation gaps.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_050",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-30T10:30:00",
          "text": "## 2026-01-22 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfer: REQUESTS req | p50: 540ms p95: 1260ms p99: N/A | err: 0.6% (2 errors) | success: 99.4%\n- /dns-query: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /ttl-change: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /propagation: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /dnssec-sign: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /health-check: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n- /change-request: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: N/A | success: N/A\n\n### Infrastructure\n- bind-master-01: CPU 29% | Mem 62% | Disk 78% | Conns: N/A | Net: 80/112 Mbps\n- bind-slave-02: CPU 23% | Mem 50% | Disk 58% | Conns: N/A | Net: 62/80 Mbps\n- cloud-dns-sync-01: CPU 49% | Mem 68% | Disk 54% | Conns: N/A | Net: 102/142 Mbps\n- signer-01: CPU 33% | Mem 73% | Disk 64% | Conns: N/A | Net: 16/21 Mbps\n\n### Connection Pools\n- pool-axfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-ixfr: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-propagation: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-signing: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n\n### CDN & Caching\n- Hit rate: 88.8% | Bandwidth: N/A | Origin requests: N/A\n\n### Alerts\n- [INFO] DNS zone transfer success rate on hostname: 99.4%\n- [INFO] DNS zone transfer requests: 34\n- [INFO] IXFR requests: 220\n- [WARNING] DNS query NX domain percentage: 2.0%\n- [WARNING] DNS query SERVFAIL percentage: 0.05%\n- [INFO] DNS queries per second: avg 23200 | peak 57000\n- [INFO] UDP query percentage: 97.0%\n- [INFO] TCP fallback percentage: 1.2%\n- [INFO] DNSSEC zones signed: 80\n- [INFO] DNSSEC signature refreshes: 120\n- [INFO] DNSSEC signature queue depth: 1\n- [INFO] DS mismatch count: 0\n- [WARNING] Bogus DNSSEC validation events: 1\n- [INFO] Authority reachability: 99.99%\n- [INFO] SOA serial mismatch count: 0\n- [INFO] Lame delegation count: 0\n- [INFO] Change requests: 6\n- [INFO] Changes applied: 6\n- [INFO] Changes failed: 0\n- [INFO] Mean time to apply changes: 17 min\n- [INFO] Resolver cache hit rate: 88.8%\n- [WARNING] Negative cache hit rate: 38.8%\n- [INFO] Prefetch enabled: 80%\n- [INFO] Auth response avg latency: 17.3ms\n- [P95] Auth response latency: 41.8ms\n- [INFO] EDNS0 present: 75%\n- [WARNING] Format error count: 7\n- [WARNING] Refused responses: 21\n- [WARNING] Rate limited responses: 9\n- [TRUNC] Truncation rate: 0.4%\n- [TOTAL] Zones: 656\n- [TOTAL] Records: 2,336,500\n- [TOTAL] Dynamic updates/hr: 335\n- [COMPLIANCE] Zonefile lint errors: 3\n- [COMPLIANCE] DNSSEC policy violations: 0\n- [COMPLIANCE] Audit log gaps: 0\n- [TRANSFER] AXFR requests: 34 | success: 99.4% | avg ms: 540 | p95 ms: 1260 | tsig failures: 0\n- [TRANSFER] IXFR requests: 220 | success: 99.3%\n- [TTL] Records changed: 39 | zones touched: 6 | min: 300s | max: 43200s | median: 3600s | rollbacks: 0\n- [PROPAGATION] Sites: 36 | avg latency: 64s | p95 latency: 152s | stale answer %: 0.3%\n- [DNSSEC] Zones signed: 80 | signing queue depth: 1 | signature refreshes: 120 | ds mismatch: 0 | bogus validation events: 1\n\n### Events\n- Moved remaining planned migrations to off-peak window; completed dry-run validation scripts against production data without applying changes.\n- Normalized TTLs back to 300+ for zones that had temporary 120s; confirmed propagation and cache hit stability.\n- Closed lame delegation follow-up actions; registrar confirmations recorded in audit notes.\n- Updated SLO doc: propagation p95 target 180s for standard changes; added escalation steps for sustained stale answers above 1%.\n\n### On-Call\n- shift handoff note: R. Ivanov. 1 pages. 4 tickets. Status: Normal; process improvements applied after prior peak-window issues.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_017",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-31T10:00:00",
          "text": "## 2024-01-31 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 142600 | p50: 165ms p95: 440ms p99: 820ms | err: 1.82% (2600 errors) | success: 98.18%\n- /fraud_check: REQUESTS 142100 | p50: 115ms p95: 330ms p99: 560ms | err: 0.37% (520 errors) | success: 99.63%\n- /geo_lookup: REQUESTS 139800 | p50: 210ms p95: 460ms p99: 620ms | err: 0.21% (290 errors) | success: 99.79%\n- /auth: REQUESTS 185900 | p50: 38ms p95: 100ms p99: 175ms | err: 0.1% (180 errors) | success: 99.9%\n- /product_catalog: REQUESTS 249800 | p50: 47ms p95: 140ms p99: 250ms | err: 0.1% (240 errors) | success: 99.9%\n- /search: REQUESTS 216400 | p50: 68ms p95: 180ms p99: 310ms | err: 0.16% (340 errors) | success: 99.84%\n- /recommendations: REQUESTS 175100 | p50: 82ms p95: 230ms p99: 395ms | err: 0.19% (330 errors) | success: 99.81%\n\n### Infrastructure\n- gateway-01: CPU 64% | Mem 75% | Disk 62% | Conns: 4050 | Net: 610/730 Mbps\n- gateway-02: CPU 61% | Mem 73% | Disk 61% | Conns: 3890 | Net: 592/710 Mbps\n- service-b-01: CPU 63% | Mem 72% | Disk 59% | Conns: 3320 | Net: 480/565 Mbps\n- metrics-db-01: CPU 38% | Mem 75% | Disk 84% | Conns: 320 | Net: 150/182 Mbps\n\n### Connection Pools\n- primary: active 198 | idle 2 | waiting 110 | exhaustion: 9 | max: 200 | avg_wait: 110ms\n- replica: active 95 | idle 5 | waiting 16 | exhaustion: 2 | max: 100 | avg_wait: 18ms\n- third_party_geo: active 60 | idle 0 | waiting 68 | exhaustion: 12 | max: 60 | avg_wait: 145ms\n\n### CDN & Caching\n- Hit rate: 92.6% | Bandwidth: 8.1 Gbps | Origin requests: 412800\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=third_party_geo events=12 waiting=68\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=1.82\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=310 retry_rate_pct=0.26\n\n### Deployments & Changes\n- Config change: service-B retry max_attempts set to 4\n\n### Events\n- Updated service-B retry max_attempts from 5 to 4\n- A/B test 'checkout-v2' at 15% rollout\n- Enabled gateway upstream keepalive for geo vendor host\n- Scaled service-B from 7 to 9 instances\n\n### On-Call\n- Shift: R. Alvarez. 8 pages (CONNPOOL-EXHAUSTION, SERVICE-B-RETRY-COUNT). 3 tickets (SRV-B-241 retry config, POOL-559, CO-798). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_051",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-31T10:30:00",
          "text": "## 2026-01-23 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 23800 req | p50: 16.6ms p95: 39.6ms p99: N/A | err: 2.1% (502 errors) | success: 97.9%\n- /zone-transfer: REQUESTS 251 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /ttl-change: REQUESTS 68 req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /propagation: REQUESTS 36 sites | p50: 60s p95: 140s | err: 0% | success: 100%\n- /health-check: REQUESTS N/A | p50: N/A p95: N/A | err: 0% | success: 100%\n- /dnssec-sign: REQUESTS 118 | p50: N/A p95: N/A | err: 0% | success: 100%\n- /change-request: REQUESTS 7 | p50: N/A p95: N/A | err: 0% | success: 100%\n- /cache-behavior: REQUESTS N/A | p50: N/A p95: N/A | err: 0% | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 24% | Mem 60% | Disk 78% | Conns: 1215 | Net: 74/104 Mbps\n- bind-slave-02: CPU 20% | Mem 48% | Disk 58% | Conns: 1025 | Net: 58/74 Mbps\n- cloud-dns-sync-01: CPU 44% | Mem 66% | Disk 54% | Conns: N/A | Net: 95/135 Mbps\n- signer-01: CPU 28% | Mem 71% | Disk 64% | Conns: N/A | Net: 14/19 Mbps\n\n### Connection Pools\n- pool-axfr: active 39 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-ixfr: active 212 | idle 0 | waiting 0 | exhaustion 0 | max 250 | avg_wait 0ms\n- pool-propagation: active 36 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-queue: active 2 | idle 0 | waiting 0 | exhaustion 0 | max 10 | avg_wait 0ms\n\n### CDN & Caching\n- Hit rate: 89.3% | Bandwidth: 1.2 Gbps | Origin requests: 1500\n\n### Alerts\n- [INFO] DNS propagation delay on cloud-dns-sync-01: 60s\n- [WARNING] Cache prefetch coverage at 82%\n- [INFO] Zone transfer success rate 100%\n- [INFO] TTL changes applied to 9 zones\n- [INFO] DNSSEC signatures refreshed 118 times\n- [INFO] Health checks authoritative reachability 99.99%\n- [INFO] Change requests processed: 7, all applied successfully\n- [INFO] Zonefile lint errors: 2, DNSSEC policy violations: 0, audit log gaps: 0\n\n### Deployments & Changes\n- Migrated 16 zones off-peak; clean transfer stats and improved propagation p95 relative to peak-window attempts\n- Enabled prefetch for 2 more resolver POPs (82% coverage); cache hit rate improved slightly\n- Executed bulk update of CAA and DMARC TXT records across 9 zones; ensured record-set size constraints respected via pre-check\n- Reviewed DNSSEC signature validity window parameters; confirmed uniform policy across signed zones\n\n### Events\n- Migrated 16 zones off-peak; clean transfer stats and improved propagation p95 relative to peak-window attempts\n- Enabled prefetch for 2 more resolver POPs (82% coverage); cache hit rate improved slightly\n- Executed bulk update of CAA and DMARC TXT records across 9 zones; ensured record-set size constraints respected via pre-check\n- Reviewed DNSSEC signature validity window parameters; confirmed uniform policy across signed zones\n\n### On-Call\n- shift handoff note: Shift: F. Delgado. 0 pages. 6 tickets. Status: Normal; off-peak migrations proceeding smoothly with improved cache performance",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_052",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-31T10:30:00",
          "text": "## 2026-01-24 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS req | p50: 15ms p95: 35ms p99: 50ms | err: 0.07% (17 errors) | success: 99.93%\n- /zone-transfer: REQUESTS req | p50: 600ms p95: 1400ms p99: 2000ms | err: 3.0% (1 transfer failure) | success: 97.0%\n- /dnssec-sign: REQUESTS req | p50: 20ms p95: 45ms p99: 60ms | err: 0.0% | success: 100%\n- /health-check: REQUESTS req | p50: 10ms p95: 25ms p99: 40ms | err: 0.02% (5 errors) | success: 99.98%\n- /cache-purge: REQUESTS req | p50: 12ms p95: 30ms p99: 45ms | err: 0.0% | success: 100%\n- /config-update: REQUESTS req | p50: 22ms p95: 50ms p99: 70ms | err: 0.0% | success: 100%\n- /metrics: REQUESTS req | p50: 8ms p95: 20ms p99: 35ms | err: 0.0% | success: 100%\n- /status: REQUESTS req | p50: 5ms p95: 15ms p99: 25ms | err: 0.0% | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 32% | Mem 63% | Disk 79% | Conns: 1278 | Net: 86/116 Mbps\n- bind-slave-02: CPU 24% | Mem 50% | Disk 58% | Conns: 1062 | Net: 62/78 Mbps\n- cloud-dns-sync-01: CPU 52% | Mem 70% | Disk 55% | Conns: N/A | Net: 108/148 Mbps\n- signer-01: CPU 36% | Mem 74% | Disk 65% | Conns: N/A | Net: 17/22 Mbps\n\n### Connection Pools\n- pool-dns-queries: active 150 | idle 20 | waiting 5 | exhaustion 0 | max 200 | avg_wait 10ms\n- pool-zone-transfers: active 10 | idle 2 | waiting 1 | exhaustion 0 | max 15 | avg_wait 15ms\n- pool-auth-servers: active 8 | idle 2 | waiting 0 | exhaustion 0 | max 10 | avg_wait 5ms\n- pool-cache-lookup: active 25 | idle 5 | waiting 2 | exhaustion 0 | max 30 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 88% | Bandwidth: 2.5 Gbps | Origin requests: 12,000\n\n### Alerts\n- [WARNING] propagation p95 increase after maintenance on cloud-dns-sync-01: 175s\n- [INFO] SOA serial mismatch on zone example.com: 1 mismatch detected\n- [ERROR] bogus validation events in DNSSEC on zone xyz.net: 2 events\n- [NOTICE] zonefile lint errors detected: 5 issues\n\n### Deployments & Changes\n- Applied 4 change requests successfully; mean time to apply 22 minutes\n- Onboarded new zones; corrected SPF TXT formatting and missing NS records\n- Performed quarterly allow-update policy review; tightened ACLs for 3 zones\n- Manual IXFR triggered for zone with SOA mismatch; validated serial parity\n\n### Events\n- Noted slight propagation p95 increase after provider maintenance; monitored and confirmed no sustained impact beyond 2 hours\n- One SOA serial mismatch on a rarely updated zone; resolved by triggering manual IXFR and validating serial parity\n- Lint found 5 issues on newly onboarded zonefiles from acquisition; corrected inconsistent SPF TXT formatting and missing NS for delegated subzone\n- Performed quarterly review of allow-update policies; tightened dynamic update ACLs for 3 zones\n\n### On-Call\n- Shift: Y. Tanaka. 1 pages. 7 tickets. Status: Stable; minor mismatch resolved and acquisition zonefile cleanup underway",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_053",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-01-31T10:30:00",
          "text": "## 2026-01-25 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfer: REQUESTS req | p50: 500ms p95: 1155ms p99: 1155ms | err: 0.0% (0 errors) | success: 100%\n- /dns-query: REQUESTS req | p50: 16.4ms p95: 39.0ms p99: 39.0ms | err: 0.04% (10 errors) | success: 99.96%\n- /ttl-change: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.0% (0 errors) | success: 100%\n- /propagation: REQUESTS req | p50: 59ms p95: 138ms p99: 138ms | err: 0.0% (0 errors) | success: 100%\n- /health-check: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.0% (0 errors) | success: 100%\n- /change-request: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 23% | Mem 60% | Disk 79% | Conns: 1202 | Net: 72/102 Mbps\n- bind-slave-02: CPU 19% | Mem 48% | Disk 58% | Conns: 1005 | Net: 57/73 Mbps\n- cloud-dns-sync-01: CPU 43% | Mem 66% | Disk 55% | Conns: N/A | Net: 96/136 Mbps\n- signer-01: CPU 28% | Mem 71% | Disk 65% | Conns: N/A | Net: 14/19 Mbps\n\n### Connection Pools\n- pool-axfr: active 37 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-ixfr: active 204 | idle 0 | waiting 0 | exhaustion 0 | max 250 | avg_wait 0ms\n- pool-propagation: active 36 | idle 0 | waiting 0 | exhaustion 0 | max 50 | avg_wait 0ms\n- pool-dnssec: active 1 | idle 0 | waiting 0 | exhaustion 0 | max 10 | avg_wait 0ms\n- pool-ttl: active 8 | idle 0 | waiting 0 | exhaustion 0 | max 20 | avg_wait 0ms\n- pool-change-mgmt: active 6 | idle 0 | waiting 0 | exhaustion 0 | max 10 | avg_wait 0ms\n\n### CDN & Caching\n- Hit rate: 89.5% | Bandwidth: 2.4 Gbps | Origin requests: 24400\n\n### Alerts\n- [INFO] DNSSEC validation sweep completed on hostname: signer-01\n- [INFO] TTL changes applied to 8 zones; 57 records changed\n- [INFO] Propagation to 36 cloud edge sites completed; avg: 59s, p95: 138s\n- [INFO] Zone transfer: 37 AXFR requests; 100% success; 204 IXFR requests; 99.7% success; avg transfer time: 500ms; p95: 1155ms\n- [INFO] DNS queries peaked at 61800 QPS; average 24400 QPS; 2.1% NXDOMAIN; 0.04% SERVFAIL; 97.2% UDP; 1.1% TCP fallback\n- [INFO] Change requests: 6; all applied successfully; mean apply time: 15 min\n\n### Deployments & Changes\n- Migrated 16 zones; acquisition cleanup changes merged and applied cleanly through pipeline\n- Enabled prefetch for small set of POPs (84%); measured lower query bursts after TTL changes\n- Ran external DNSSEC validation sweep for all signed zones; no bogus events observed; proof outputs recorded in ticket\n- Updated documentation for dynamic update ACL review cadence and approval flow\n\n### Events\n- Migrated 16 zones; acquisition cleanup changes merged and applied cleanly through pipeline\n- Enabled prefetch for small set of POPs (84%); measured lower query bursts after TTL changes\n- Ran external DNSSEC validation sweep for all signed zones; no bogus events observed; proof outputs in ticket\n- Updated documentation for dynamic update ACL review cadence and approval flow\n\n### On-Call\n- shift handoff note: A. Morales. 0 pages. 4 tickets. Status: Normal; smooth migration batch and DNSSEC sweep completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_018",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-01T10:00:00",
          "text": "## 2024-02-01 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 145200 | p50: 175ms p95: 470ms p99: 860ms | err: 2.13% (3100 errors) | success: 97.87%\n- /fraud_check: REQUESTS 144700 | p50: 125ms p95: 350ms p99: 600ms | err: 0.45% (650 errors) | success: 99.55%\n- /geo_lookup: REQUESTS 142400 | p50: 230ms p95: 500ms p99: 650ms | err: 0.22% (320 errors) | success: 99.78%\n- /auth: REQUESTS 189200 | p50: 40ms p95: 104ms p99: 185ms | err: 0.11% (210 errors) | success: 99.89%\n- /product_catalog: REQUESTS 254100 | p50: 50ms p95: 150ms p99: 270ms | err: 0.11% (280 errors) | success: 99.89%\n- /search: REQUESTS 220300 | p50: 70ms p95: 188ms p99: 325ms | err: 0.17% (370 errors) | success: 99.83%\n- /recommendations: REQUESTS 178700 | p50: 86ms p95: 245ms p99: 415ms | err: 0.2% (360 errors) | success: 99.8%\n\n### Infrastructure\n- gateway-01: CPU 66% | Mem 76% | Disk 62% | Conns: 4380 | Net: 635/760 Mbps\n- gateway-02: CPU 63% | Mem 74% | Disk 61% | Conns: 4205 | Net: 618/738 Mbps\n- service-b-01: CPU 68% | Mem 74% | Disk 59% | Conns: 3780 | Net: 520/612 Mbps\n- metrics-db-01: CPU 41% | Mem 76% | Disk 85% | Conns: 335 | Net: 158/190 Mbps\n\n### Connection Pools\n- primary: active 200 | idle 0 | waiting 140 | exhaustion: 11 | max: 200 | avg_wait: 135ms\n- replica: active 98 | idle 2 | waiting 22 | exhaustion: 3 | max: 100 | avg_wait: 24ms\n- third_party_geo: active 60 | idle 0 | waiting 82 | exhaustion: 14 | max: 60 | avg_wait: 170ms\n\n### CDN & Caching\n- Hit rate: 92.4% | Bandwidth: 8.4 Gbps | Origin requests: 428900\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=primary events=11 waiting=140\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-02: error_pct=2.13\n- [CRITICAL] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=650\n\n### Deployments & Changes\n- Deployed gateway-config v1.12.6 (timeout adjustments)\n\n### Events\n- Gateway timeout for /checkout set to 2.5s\n- A/B test 'checkout-v2' at 15% rollout\n- Temporary disable of geo-lookup for low-risk countries (feature flag)\n- Added new alert: CONNPOOL-EXHAUSTION for third_party_geo\n\n### On-Call\n- Shift: D. Kim. 10 pages (CONNPOOL-EXHAUSTION, CHECKOUT-ERROR-RATE). 4 tickets (GW-341 timeouts, CO-804, POOL-565, VEND-90). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_054",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-01T10:30:00",
          "text": "## 2026-01-26 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS req | p50: 12ms p95: 35ms p99: 50ms | err: 0.09% (22 errors) | success: 99.91%\n- /zone-transfer: REQUESTS req | p50: 640ms p95: 1610ms p99: 2000ms | err: 3.5% (2 failures) | success: 96.5%\n- /dns-sec-sign: REQUESTS req | p50: 25ms p95: 60ms p99: 80ms | err: 0.0% | success: 100%\n- /health-check: REQUESTS req | p50: 10ms p95: 20ms p99: 30ms | err: 0.0% | success: 100%\n- /cache-purge: REQUESTS req | p50: 15ms p95: 40ms p99: 55ms | err: 0.0% | success: 100%\n- /config-update: REQUESTS req | p50: 18ms p95: 45ms p99: 60ms | err: 0.0% | success: 100%\n- /dnssec-policy: REQUESTS req | p50: 22ms p95: 55ms p99: 70ms | err: 0.0% | success: 100%\n- /ttl-change: REQUESTS req | p50: 30ms p95: 70ms p99: 90ms | err: 0.0% | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 41% | Mem 66% | Disk 80% | Conns: 1395 | Net: 105/148 Mbps\n- bind-slave-02: CPU 30% | Mem 56% | Disk 59% | Conns: 1248 | Net: 82/100 Mbps\n- cloud-dns-sync-01: CPU 66% | Mem 78% | Disk 56% | Conns: 0 | Net: 135/176 Mbps\n- signer-01: CPU 49% | Mem 80% | Disk 66% | Conns: 0 | Net: 22/27 Mbps\n\n### Connection Pools\n- pool-axfr: active 12 | idle 3 | waiting 2 | exhaustion 0 | max 20 | avg_wait 15ms\n- pool-ixfr: active 8 | idle 4 | waiting 1 | exhaustion 0 | max 15 | avg_wait 10ms\n- pool-dnssec: active 5 | idle 2 | waiting 0 | exhaustion 0 | max 10 | avg_wait 8ms\n- pool-ttl: active 3 | idle 1 | waiting 0 | exhaustion 0 | max 10 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 87.0% | Bandwidth: 2.3 Gbps | Origin requests: 25000\n\n### Alerts\n- [SEVERE] TSIG failure on hostname: 2 AXFRs failed due to expired key metadata\n- [WARNING] SOA serial mismatch on hostname: 3 zones\n- [INFO] Propagation delay: average 88s | p95: 235s\n- [INFO] Stale answer percentage: 0.8%\n- [SEVERE] Zone transfer failures: 2 TSIG failures\n- [WARNING] TTL rollback occurred in 1 zone\n- [INFO] DNS query peak QPS: 64000\n- [INFO] DNS query average QPS: 25000\n- [WARNING] DNSSEC bogus validation events: 3\n- [INFO] Propagation to 36 cloud edge sites completed in average 88s\n- [WARNING] TTL changes: 72 records across 9 zones; min TTL 180s, max 43200s, median 1800s\n- [INFO] Change requests: 8; applied: 7; failed: 1; mean apply time 29 min\n\n### Deployments & Changes\n- Rotated DKIM selectors across 9 zones; propagation p95 rose, stale answers increased to 0.8%\n- Resolved pipeline apply failure due to conflicting record ownership; consolidated zone definition and reapplied\n- Regenerated TSIG keys after failures; updated in secret store\n- Introduced per-zone merge queue to serialize SOA serial updates after mismatches\n\n### Events\n- Large update: rotated DKIM selectors across 9 zones; propagation p95 rose and stale answers increased to 0.8% during window\n- One pipeline apply failed due to conflicting record ownership between two IaC modules; resolved by consolidating zone definition and reapplying\n- TSIG failures seen on two AXFRs; traced to expired TSIG key validity metadata in secret store; regenerated key material and updated both ends\n- SOA serial mismatches on 3 zones after parallel commits; introduced per-zone merge queue requirement to serialize changes\n\n### On-Call\n- shift handoff note: W. Park. 3 pages. 9 tickets. Status: Elevated during DKIM rotation; stabilized after key regen and IaC ownership fix",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_055",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-01T10:30:00",
          "text": "## 2026-01-27 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS 24,800 req | p50: 16 ms p95: 40.6 ms p99: 121 ms | err: 0.07% (18 refused + 8 rate-limited + 6 format errors) | success: 99.86%\n- /zone-transfer: REQUESTS 249 req | p50: 525 ms p95: 1215 ms p99: 2000 ms | err: 0% (0 failures) | success: 100%\n- /dnssec-sign: REQUESTS 121 req | p50: 20 ms p95: 50 ms p99: 100 ms | err: 0% | success: 100%\n- /health-check: REQUESTS 1 req | p50: 10 ms p95: 20 ms p99: 30 ms | err: 0% | success: 100%\n- /cache-refresh: REQUESTS 50 req | p50: 12 ms p95: 25 ms p99: 45 ms | err: 0% | success: 100%\n- /config-update: REQUESTS 6 req | p50: 16 ms p95: 30 ms p99: 50 ms | err: 0% | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 26% | Mem 61% | Disk 80% | Conns: 1230 | Net: 78/110 Mbps\n- bind-slave-02: CPU 20% | Mem 49% | Disk 59% | Conns: 1040 | Net: 60/76 Mbps\n- cloud-dns-sync-01: CPU 46% | Mem 67% | Disk 56% | Conns: 3 | Net: 98/140 Mbps\n- signer-01: CPU 31% | Mem 72% | Disk 66% | Conns: N/A | Net: 15/20 Mbps | HSM slot utilization 65%\n\n### Connection Pools\n- dns-queries: active 150 | idle 50 | waiting 10 | exhaustion 0 | max 250 | avg_wait 5ms\n- zone-transfers: active 20 | idle 5 | waiting 2 | exhaustion 0 | max 30 | avg_wait 10ms\n- cache-refresh: active 10 | idle 2 | waiting 1 | exhaustion 0 | max 15 | avg_wait 3ms\n- config-updates: active 3 | idle 1 | waiting 0 | exhaustion 0 | max 5 | avg_wait 2ms\n\n### CDN & Caching\n- Hit rate: 89.0% | Bandwidth: 2.5 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [INFO] DNS propagation delay on cloud edge sites: average 63s | p95: 150s\n- [WARNING] Stale answer percentage: 0.3%\n- [ERROR] Bogus validation event in DNSSEC: 1 event\n- [INFO] Zone transfer success rate: 100%\n- [INFO] IXFR requests: 216 | success rate: 99.4%\n- [INFO] AXFR requests: 33 | success rate: 100%\n- [INFO] TTL changes: 41 records across 6 zones | min: 300s | max: 86400s | median: 1800s\n- [INFO] Change requests processed: 6 | applied: 6 | failed: 0 | mean apply time: 16 min\n- [INFO] DNSSEC zones signed: 90 | signature refreshes: 121 | DS mismatch count: 0\n- [INFO] Authority reachability: 99.99% | SOA mismatch: 0 | Lame delegation: 0\n\n### Deployments & Changes\n- Migrated 16 zones; enforced per-zone merge queue policy; no SOA serial mismatches observed.\n- Updated secret store rotation procedure for TSIG keys with explicit expiry metadata; added alert 7 days before expiry.\n- Closed out IaC module ownership conflict by adding CI check for duplicate record definitions.\n\n### Events\n- Post-DKIM rotation checks: sampled TXT answers from 15 external resolvers; confirmed new selectors present and old removed as expected.\n- Migrated 16 zones; enforced per-zone merge queue policy; no SOA serial mismatches observed.\n- Updated secret store rotation procedure for TSIG keys with explicit expiry metadata; added alert 7 days before expiry.\n- Closed out IaC module ownership conflict by adding CI check for duplicate record definitions.\n\n### On-Call\n- Shift: S. Mehta. 1 pages. 5 tickets. Status: Stable; preventive controls added for TSIG key lifecycle and IaC duplication.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_056",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-01T10:30:00",
          "text": "## 2026-01-28 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS req | p50: 17ms p95: 41.9ms p99: N/A | err: 0.08% (17 errors) | success: 99.92%\n- /zone-transfer: REQUESTS req | axfr: 25 (axfr success: 98.7%) | ixfr: 198 (ixfr success: 99.2%) | transfer_avg_ms: 540 | transfer_p95_ms: 1280 | tsig_failures: 0\n- /ttl-change: REQUESTS req | records_changed: 28 | zones_touched: 5 | ttl_min_s: 300 | ttl_max_s: 43200 | ttl_median_s: 3600 | rollbacks: 0\n- /health-check: REQUESTS req | authoritative_reachability_pct: 99.99% | soa_serial_mismatch_count: 0 | lame_delegation_count: 0\n- /dnssec-sign: REQUESTS req | zones_signed: 92 | signature_refreshes: 123 | signing_queue_depth: 1 | ds_mismatch_count: 0 | bogus_validation_events: 1\n- /change-request: REQUESTS req | change_requests: 4 | changes_applied: 4 | changes_failed: 0 | mean_time_to_apply_min: 18\n\n### Infrastructure\n- bind-master-01: CPU 28% | Mem 62% | Disk 80% | Conns: N/A | Net: 82/114 Mbps\n- bind-slave-02: CPU 21% | Mem 49% | Disk 59% | Conns: N/A | Net: 61/77 Mbps\n- cloud-dns-sync-01: CPU 48% | Mem 68% | Disk 56% | Conns: N/A | Net: 100/142 Mbps\n- signer-01: CPU 32% | Mem 73% | Disk 66% | Conns: N/A | Net: 16/21 Mbps | HSM slot utilization: 66%\n\n### Connection Pools\n- pool-name: active N | idle N | waiting N | exhaustion: N | max: N | avg_wait: Xms\n- (No specific pool data provided)\n\n### CDN & Caching\n- Hit rate: 88.6% | Bandwidth: 1.4 Gbps | Origin requests: 25200\n\n### Alerts\n- [INFO] DNSSEC bogus validation event on zone: 1\n- [INFO] Propagation to 36 cloud edge sites completed in average 67s, p95: 162s\n- [INFO] TTL changes applied to 5 zones, 28 records changed, min TTL: 300s, max TTL: 43200s, median TTL: 3600s\n- [INFO] Zone transfer requests: AXFR 25 (98.7% success), IXFR 198 (99.2% success), avg transfer time: 540ms, p95: 1280ms\n- [INFO] DNS queries peak at 65,000 QPS, average 25,200 QPS; NXDomain: 2.2%, SERVFAIL: 0.06%; UDP: 96.8%, TCP fallback: 1.2%\n- [INFO] DNSSEC zones signed: 92; signature refreshes: 123; queue depth: 1; no DS mismatches; 1 bogus validation event\n- [INFO] Health checks: authoritative reachability 99.99%, no SOA mismatch, no lame delegations\n- [INFO] Change requests: 4, all applied successfully, average apply time: 18 min\n\n### Events\n- Enabled prefetch coverage to 86% after adding two resolver POPs; monitored for upstream query spikes; none observed.\n- Performed routine DNSSEC signature refresh verification; RRSIG expiration lead time maintained within policy.\n- Reviewed and pruned unused TSIG keys; removed 6 retired keys; updated access lists.\n- Ran quarterly zone transfer test suite; confirmed AXFR/IXFR behavior and transfer ACLs across cloud endpoints.\n\n### On-Call\n- shift handoff note: J. Reed. 0 pages. 4 tickets. Status: Normal; hygiene tasks completed and prefetch coverage expanded.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_019",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-02T10:00:00",
          "text": "## 2024-02-02 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 147600 | p50: 185ms p95: 490ms p99: 900ms | err: 2.3% (3400 errors) | success: 97.7%\n- /fraud_check: REQUESTS 147100 | p50: 135ms p95: 370ms p99: 640ms | err: 0.49% (720 errors) | success: 99.51%\n- /geo_lookup: REQUESTS 144500 | p50: 250ms p95: 520ms p99: 680ms | err: 0.25% (360 errors) | success: 99.75%\n- /auth: REQUESTS 192000 | p50: 42ms p95: 108ms p99: 195ms | err: 0.12% (230 errors) | success: 99.88%\n- /product_catalog: REQUESTS 258400 | p50: 52ms p95: 158ms p99: 285ms | err: 0.12% (300 errors) | success: 99.88%\n- /search: REQUESTS 223800 | p50: 72ms p95: 195ms p99: 340ms | err: 0.17% (390 errors) | success: 99.83%\n- /recommendations: REQUESTS 181400 | p50: 90ms p95: 255ms p99: 430ms | err: 0.21% (380 errors) | success: 99.79%\n\n### Infrastructure\n- gateway-01: CPU 68% | Mem 77% | Disk 62% | Conns: 4680 | Net: 650/780 Mbps\n- gateway-02: CPU 65% | Mem 75% | Disk 61% | Conns: 4510 | Net: 632/758 Mbps\n- service-b-01: CPU 72% | Mem 76% | Disk 60% | Conns: 4200 | Net: 560/658 Mbps\n- metrics-db-01: CPU 43% | Mem 77% | Disk 85% | Conns: 345 | Net: 165/198 Mbps\n\n### Connection Pools\n- primary: active 200 | idle 0 | waiting 165 | exhaustion: 13 | max: 200 | avg_wait: 155ms\n- replica: active 100 | idle 0 | waiting 30 | exhaustion: 4 | max: 100 | avg_wait: 32ms\n- third_party_geo: active 60 | idle 0 | waiting 95 | exhaustion: 15 | max: 60 | avg_wait: 185ms\n\n### CDN & Caching\n- Hit rate: 92.2% | Bandwidth: 8.6 Gbps | Origin requests: 441300\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=third_party_geo events=15 waiting=95\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=340 retry_rate_pct=0.28\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=2.30\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Reduced service-B concurrency limit from 120 to 90 (config)\n- A/B test 'checkout-v2' at 15% rollout\n- Added synthetic probe for geo vendor from us-east-1\n- Scaled checkout-worker pool from 10 to 12 instances\n\n### On-Call\n- Shift: J. Martinez. 11 pages. 4 tickets (SRV-B-250 concurrency, POOL-571, CO-812, SYN-44). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_057",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-02T10:30:00",
          "text": "## 2026-01-29 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS 40 | p50: 675ms p95: 1705ms p99: N/A | err: 5.0% (2 errors) | success: 95.0%\n- /ixfr-requests: REQUESTS 230 | p50: 675ms p95: 1705ms p99: N/A | err: 1.7% (4 errors) | success: 98.3%\n- /dns-queries: REQUESTS 25700 | p50: N/A | p95: N/A | err: N/A | success: N/A\n- /propagation: REQUESTS N/A | p50: 92ms p95: 250ms | err: N/A | success: N/A\n- /dnssec-signing: REQUESTS N/A | p50: N/A | p95: N/A | err: N/A | success: N/A\n- /health-checks: REQUESTS N/A | p50: N/A | p95: N/A | err: N/A | success: N/A\n- /change-requests: REQUESTS 7 | changes applied: 6 | failed: 1 | mean apply time: 31min\n\n### Infrastructure\n- bind-master-01: CPU 46% | Mem 68% | Disk 81% | Conns: 1468 | Net: 120/165 Mbps\n- bind-slave-02: CPU 34% | Mem 59% | Disk 60% | Conns: 1310 | Net: 95/110 Mbps\n- cloud-dns-sync-01: CPU 70% | Mem 82% | Disk 57% | Conns: N/A | Net: 150/195 Mbps\n- signer-01: CPU 54% | Mem 81% | Disk 67% | Conns: N/A | Net: 24/28 Mbps\n\n### Connection Pools\n- cloud-dns-sync-01: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- signer-01: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n\n### CDN & Caching\n- Hit rate: 86.5% | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [WARNING] TSIG failures on hostname: 4 events\n- [INFO] Propagation delay: 92ms avg | 250ms p95\n- [ERROR] Change request failure: 1 in batch\n- [INFO] Propagation sites: 36\n- [WARNING] Stale answer percentage: 1.0%\n- [ERROR] TTL rollback executed after spike\n- [WARNING] Dynamic updates spike due to misconfigured client; queue depth reached 12\n- [ERROR] Transfer failures linked to TSIG issues; mitigated by staggering transfers\n\n### Deployments & Changes\n- Split failed change set into smaller batches and re-applied\n- Corrected internal client misconfiguration causing update spike\n- Rolled back TTL reduction from 60s to 180s after stale answer increase\n- Mitigated TSIG failures by staggering transfer parallelism\n\n### Events\n- Dynamic updates spiked due to internal client misconfiguration; propagation p95 rose, sync queue depth reached 12\n- One change set failed after exceeding per-zone change batch limit; split into smaller batches and re-applied\n- TSIG failures correlated with increased transfer volume; confirmed keys correct, mitigated by staggering transfers\n- Rolled back TTL reduction (180->60) after stale answers increased during update spike; restored to 1800\n\n### On-Call\n- shift handoff note: O. Bennett. 4 pages. 10 tickets. Status: Degraded during dynamic update spike; mitigated by batching, staggering transfers, and correcting client behavior",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_058",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-02T10:30:00",
          "text": "## 2026-01-30 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 540ms p95: 1265ms p99: N/A | err: 0.8% (0 errors) | success: 99.2%\n- /ixfr-requests: REQUESTS req | p50: 540ms p95: 1265ms p99: N/A | err: 0.5% (1 errors) | success: 99.5%\n- /ttl-changes: REQUESTS req | p50: 540ms p95: 1265ms p99: N/A | err: 0.0% (0 errors) | success: 100%\n- /propagation: REQUESTS req | p50: 65ms p95: 155ms p99: N/A | err: 0.3% (0 errors) | success: 99.7%\n- /dns-queries: QPS avg: 25400 | peak: 66500 | nx_domain: 2.1% | servfail: 0.05% | udp: 97.1% | tcp_fallback: 1.1%\n- /dnssec: zones signed: 96 | signing queue depth: 1 | signature refreshes: 124 | ds mismatch: 0 | bogus validation events: 1\n- /health-checks: authoritative reachability: 99.99% | SOA serial mismatch: 0 | lame delegations: 0\n- /change-mgmt: requests: 6 | applied: 6 | failed: 0 | mean apply time: 17min\n- /cache-behavior: resolver cache hit: 89.2% | negative cache hit: 38.7% | prefetch enabled: 86.0%\n\n### Infrastructure\n- bind-master-01: CPU 27% | Mem 62% | Disk 81% | Conns: 1248 | Net: 80/112 Mbps\n- bind-slave-02: CPU 21% | Mem 49% | Disk 60% | Conns: 1055 | Net: 61/78 Mbps\n- cloud-dns-sync-01: CPU 48% | Mem 68% | Disk 57% | Conns: N/A | Net: 102/144 Mbps | Queue depth: 3\n- signer-01: CPU 32% | Mem 73% | Disk 67% | Conns: N/A | Net: 16/21 Mbps | HSM slot utilization: 67%\n\n### Connection Pools\n- pool-name: active 0 | idle 0 | waiting 0 | exhaustion: 0 | max: 0 | avg_wait: 0ms\n(Note: No specific pool data provided; placeholder for structure)\n\n### CDN & Caching\n- Hit rate: 89.2% | Bandwidth: 86.0 Gbps | Origin requests: 36\n\n### Alerts\n- [INFO] DNS propagation latency on cloud-dns-sync-01: 65ms\n- [WARNING] Stale answer percentage at 0.3%\n- [ERROR] Bogus DNSSEC validation event detected\n- [INFO] Zone transfer success rate: 99.2%\n- [INFO] IXFR transfer success rate: 99.5%\n- [INFO] TTL records changed: 36 across 6 zones\n- [INFO] Propagation p95: 155s\n- [INFO] DNS query QPS peak: 66500\n- [INFO] DNS query QPS average: 25400\n- [INFO] NX domain percentage: 2.1%\n- [INFO] SERVFAIL percentage: 0.05%\n- [INFO] UDP traffic: 97.1%\n- [INFO] TCP fallback: 1.1%\n- [INFO] Zones signed: 96\n- [INFO] Signature refreshes: 124\n- [INFO] DS mismatch count: 0\n- [INFO] Bogus validation events: 1\n- [INFO] Authority reachability: 99.99%\n- [INFO] SOA serial mismatch: 0\n- [INFO] Lame delegation count: 0\n- [INFO] Change requests: 6\n- [INFO] Changes applied: 6\n- [INFO] Changes failed: 0\n- [INFO] Mean time to apply: 17min\n- [INFO] Resolver cache hit rate: 89.2%\n- [INFO] Negative cache hit rate: 38.7%\n- [INFO] Pre-fetch enabled: 86.0%\n- [INFO] Auth response avg ms: 17.0\n- [INFO] Auth response p95 ms: 41.0\n- [INFO] EDNS0 present: 75.3%\n- [ERROR] Format error count: 7\n- [ERROR] Refused count: 18\n- [ERROR] Rate limited count: 8\n- [ERROR] Truncation TC: 0.4%\n- [INFO] Total zones: 720\n- [INFO] Total records: 2,611,300\n- [INFO] Dynamic updates/hr: 350\n- [INFO] Zonefile lint errors: 3\n- [INFO] DNSSEC policy violations: 0\n- [INFO] Audit log gaps: 0\n\n### Events\n- Corrected internal client update frequency and confirmed dynamic updates returned to baseline; propagation p95 normalized.\n- Migrated final planned batch for the month (16 zones); executed parity tests and DS checks where applicable.\n- Refined change batching logic in pipeline to respect provider limits; added unit tests for batch splitting.\n- Closed incident follow-up: documented transfer staggering approach and added alert on sync queue depth > 10.\n\n### On-Call\n- shift handoff note: M. Duarte. 1 pages. 6 tickets. Status: Stable; recovered from prior spike and completed month-end migration batch.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_059",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-02T10:30:00",
          "text": "## 2026-01-31 Daily Operations Summary\n\n### Endpoint Performance\n- /zone-transfers: REQUESTS req | p50: 490ms p95: 1120ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /dns-queries: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0.04% (10 errors) | success: 99.96%\n- /ttl-changes: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /propagation: REQUESTS req | p50: 58ms p95: 135ms p99: N/A | err: 0% (0 errors) | success: 100%\n- /dnssec-signing: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /health-checks: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /change-requests: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /cache-behavior: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /latency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 22% | Mem 60% | Disk 81% | Conns: N/A | Net: 72/100 Mbps\n- bind-slave-02: CPU 18% | Mem 48% | Disk 60% | Conns: N/A | Net: 56/72 Mbps\n- cloud-dns-sync-01: CPU 42% | Mem 65% | Disk 57% | Conns: N/A | Net: 90/130 Mbps\n- signer-01: CPU 28% | Mem 71% | Disk 67% | Conns: N/A | Net: 14/19 Mbps\n\n### Connection Pools\n- pool-axfr: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n- pool-ixfr: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n- pool-queue: active N/A | idle N/A | waiting N/A | exhaustion: N/A | max: N/A | avg_wait: N/A\n\n### CDN & Caching\n- Hit rate: 89.8% | Bandwidth: N/A Gbps | Origin requests: N/A\n\n### Alerts\n- [INFO] DNS zone transfer success on hostname: 24 AXFR requests, 100% success\n- [INFO] DNS IXFR requests: 176, success rate 99.8%\n- [WARNING] Propagation delay: 58ms avg, 135ms p95\n- [INFO] DNS queries peak QPS: 65200\n- [INFO] DNS queries average QPS: 24900\n- [INFO] NX domain responses: 2.0%\n- [INFO] SERVFAIL responses: 0.04%\n- [INFO] UDP query percentage: 97.3%\n- [INFO] TCP fallback percentage: 1.0%\n- [INFO] Zones signed: 98\n- [INFO] Signature refreshes: 115\n- [INFO] DS mismatch count: 0\n- [INFO] Bogus validation events: 0\n- [INFO] Authority reachability: 99.99%\n- [INFO] SOA serial mismatch: 0\n- [INFO] Lame delegation count: 0\n- [INFO] Change requests: 3, applied: 3, failed: 0, mean apply time: 14 min\n- [INFO] Resolver cache hit rate: 89.8%\n- [INFO] Negative cache hit rate: 37.9%\n- [INFO] Prefetch enabled: 88%\n- [INFO] Auth response avg latency: 16.1ms, p95: 38.5ms\n- [INFO] EDNS0 present: 75.9%\n- [ERROR] Format errors: 5\n- [ERROR] Refused responses: 16\n- [ERROR] Rate limited responses: 6\n- [ERROR] Truncation TC: 0.3%\n- [INFO] Total zones: 720\n- [INFO] Total records: 2,613,100\n- [INFO] Dynamic updates/hr: 330\n- [INFO] Zonefile lint errors: 2\n- [INFO] DNSSEC policy violations: 0\n- [INFO] Audit log gaps: 0\n\n### Events\n- Increased resolver prefetch coverage to 88%; verified no adverse effects on authoritative QPS patterns.\n- Performed month-end reconciliation: zones/records counts matched between on-prem exports and cloud-managed inventory for migrated set.\n- Validated backup and restore procedure for zone data in cloud provider; completed restore drill on a non-production zone.\n- Reviewed upcoming cutover plan for remaining on-prem zones; updated checklist for AXFR freeze and final serial bump.\n\n### On-Call\n- shift handoff note: K. Shah. 0 pages. 3 tickets. Status: Normal; reconciliation and recovery drills completed.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_020",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-03T10:00:00",
          "text": "## 2024-02-03 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: 150100 req | p50: 190ms p95: 510ms p99: 940ms | err: 2.47% (3700 errors) | success: 97.53%\n- /fraud_check: 149600 req | p50: 145ms p95: 390ms p99: 680ms | err: 0.55% (820 errors) | success: 99.45%\n- /geo_lookup: 147000 req | p50: 270ms p95: 560ms p99: 710ms | err: 0.27% (390 errors) | success: 99.73%\n- /auth: 195300 req | p50: 44ms p95: 112ms p99: 205ms | err: 0.13% (250 errors) | success: 99.87%\n- /product_catalog: 262900 req | p50: 55ms p95: 165ms p99: 300ms | err: 0.12% (320 errors) | success: 99.88%\n- /search: 227900 req | p50: 75ms p95: 202ms p99: 355ms | err: 0.18% (410 errors) | success: 99.82%\n- /recommendations: 184700 req | p50: 94ms p95: 265ms p99: 450ms | err: 0.22% (400 errors) | success: 99.78%\n\n### Infrastructure\n- gateway-01: CPU 70% | Mem 78% | Disk 63% | Conns: 4950 | Net: 670/805 Mbps\n- gateway-02: CPU 67% | Mem 76% | Disk 62% | Conns: 4785 | Net: 650/785 Mbps\n- service-b-01: CPU 75% | Mem 77% | Disk 60% | Conns: 4520 | Net: 590/700 Mbps\n- metrics-db-01: CPU 45% | Mem 78% | Disk 86% | Conns: 360 | Net: 172/205 Mbps\n\n### Connection Pools\n- primary: active 200 | idle 0 | waiting 190 | exhaustion: 14 | max: 200 | avg_wait: 170ms\n- replica: active 100 | idle 0 | waiting 38 | exhaustion: 5 | max: 100 | avg_wait: 40ms\n- third_party_geo: active 60 | idle 0 | waiting 110 | exhaustion: 15 | max: 60 | avg_wait: 200ms\n\n### CDN & Caching\n- Hit rate: 92.1% | Bandwidth: 8.9 Gbps | Origin requests: 456700\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=primary events=14 waiting=190\n- [CRITICAL] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=710\n\n### Deployments & Changes\n- Deployed service-B (fraud) v2.7.6\n\n### Events\n- Service-B deploy completed\n- A/B test 'checkout-v2' at 15% rollout\n- Adjusted gateway circuit breaker to open after 30 geo failures in 60s\n- Added dashboard panel for third_party_geo pool waiting\n\n### On-Call\n- shift handoff: A. Chen. 12 pages. 4 tickets (SRV-B-258, GW-352, POOL-580, CO-821). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_060",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-03T10:30:00",
          "text": "## 2026-02-01 Daily Operations Summary\n\n### Endpoint Performance\n- /dns-query: REQUESTS req | p50: 16ms p95: 40ms p99: 60ms | err: 0.07% (18 errors) | success: 99.93%\n- /zone-transfer: REQUESTS req | p50: 560ms p95: 1315ms p99: 2000ms | err: 1.1% (45 errors) | success: 98.9%\n- /ttl-change: REQUESTS req | p50: 300ms p95: 900ms p99: 1500ms | err: 0.0% (0 errors) | success: 100%\n- /dnssec-sign: REQUESTS req | p50: 20ms p95: 50ms p99: 70ms | err: 0.0% (0 errors) | success: 100%\n- /health-check: REQUESTS req | p50: 10ms p95: 25ms p99: 40ms | err: 0.0% (0 errors) | success: 100%\n- /change-request: REQUESTS req | p50: 15ms p95: 35ms p99: 50ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- bind-master-01: CPU 29% | Mem 62% | Disk 82% | Conns: 1275 | Net: 86/120 Mbps\n- bind-slave-02: CPU 22% | Mem 50% | Disk 60% | Conns: 1080 | Net: 64/82 Mbps\n- cloud-dns-sync-01: CPU 51% | Mem 69% | Disk 58% | Conns: N/A | Net: 110/152 Mbps\n- signer-01: CPU 35% | Mem 74% | Disk 68% | Conns: N/A | Net: 16/22 Mbps\n\n### Connection Pools\n- pool-dns-queries: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- pool-zone-transfers: active 15 | idle 5 | waiting 2 | exhaustion 0 | max 25 | avg_wait 20ms\n- pool-ttl-changes: active 8 | idle 2 | waiting 1 | exhaustion 0 | max 15 | avg_wait 10ms\n- pool-dnssec-sign: active 3 | idle 1 | waiting 0 | exhaustion 0 | max 5 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 89.0% | Bandwidth: 2.5 Gbps | Origin requests: 12,000\n\n### Alerts\n- [INFO] DNSSEC signature validation event on signer-01: bogus_validation_events: 1\n- [WARNING] High propagation delay on cloud edge sites: avg 66s | p95 158s\n- [INFO] Zone transfer success rate: 98.9% (axfr), 99.0% (ixfr)\n- [INFO] TTL changes applied: 79 records across 12 zones\n- [INFO] DNS queries peak QPS: 69,000\n- [INFO] DNS queries average QPS: 25,800\n- [WARNING] Stale answer percentage: 0.3%\n- [INFO] DNSSEC zones signed: 100\n- [INFO] DNSSEC signature refreshes: 128\n- [INFO] Propagation to cloud edge sites: 36 sites, avg 66s, p95 158s\n- [INFO] Change requests processed: 8, all applied successfully\n- [INFO] TTL harmonization completed for 12 zones\n- [INFO] Start-of-month migration push: 16 zones onboarded, including complex SRV and CAA sets\n- [INFO] DNSSEC enabled for 2 zones, DS records published, validated from multiple resolvers\n- [INFO] Updated on-call playbook for final on-prem cutover procedures\n\n### Events\n- Start-of-month migration push: onboarded 16 zones, including several with complex SRV and CAA sets; parity tests passed.\n- Applied TTL harmonization across 12 zones to 1800 seconds; coordinated with app owners and logged approvals.\n- Ran DNSSEC enablement for 2 zones previously unsigned; published DS records and validated from multiple resolvers.\n- Updated on-call playbook to include step-by-step for final on-prem cutover (freeze updates, AXFR snapshot, final serial bump, delegation switch).\n\n### On-Call\n- shift handoff note: A. Lee. 1 pages. 8 tickets. Status: Stable; migration continues with standardized TTLs and expanding DNSSEC coverage.",
          "meta": {
            "episode_type": "distractor",
            "theme": "dns_migration"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_061",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-03T10:30:00",
          "text": "## 2026-01-02 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-12: CPU 71% | Mem 78% | Disk 93% | Conns: 120 | Net: 7.4/6.1 Mbps\n- stor-b-07: CPU 54% | Mem 66% | Disk 88% | Conns: 120 | Net: 5.2/4.8 Mbps\n- stor-c-19: CPU 62% | Mem 73% | Disk 91% | Conns: 120 | Net: 6.8/6.5 Mbps\n- meta-02: CPU 48% | Mem 81% | Disk 74% | Conns: 120 | Net: 2.1/1.9 Mbps\n\n### Connection Pools\n- pool-storage: active 45 | idle 10 | waiting 5 | exhaustion 0 | max 60 | avg_wait 12ms\n- pool-db: active 30 | idle 15 | waiting 2 | exhaustion 0 | max 50 | avg_wait 8ms\n- pool-api: active 20 | idle 25 | waiting 1 | exhaustion 0 | max 50 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 92.4% | Bandwidth: 3.2 Gbps | Origin requests: 124,000\n\n### Alerts\n- [CRITICAL] disk_media_error on stor-a-12: 1 disk\n- [WARNING] quota_hard_block on tenant 'analytics-42': 1 block\n- [INFO] snapshot verification failures: 2\n- [INFO] rebalance jobs started: 6\n- [INFO] rebalance jobs completed: 5\n- [INFO] data moved: 214 TB\n- [INFO] rebalance avg Mbps: 860\n- [INFO] rebalance queue depth: 3\n- [INFO] skew index: 1.18\n- [INFO] compactions run: 1480\n- [INFO] compaction CPU hours: 96.4\n- [INFO] write amplification: 1.42\n- [INFO] sstables compacted: 22140\n- [INFO] pending compactions: 64\n- [INFO] avg compaction MB/s: 182\n- [INFO] read IOPS p95: 184,000\n- [INFO] write IOPS p95: 92,000\n- [INFO] read MB/s p95: 3120\n- [INFO] write MB/s p95: 1460\n- [INFO] disk busy p95: 83.5%\n- [INFO] fsync p95 ms: 9.4\n- [INFO] replication factor: 3\n- [INFO] erasure coding: 22%\n- [INFO] compression ratio: 1.88\n- [INFO] dedupe ratio: 1.12\n- [INFO] garbage percentage: 6.3%\n- [INFO] tombstone percentage: 2.7%\n- [INFO] snapshots taken: 48\n- [INFO] snapshots verified: 46\n- [INFO] verify failures: 2\n- [INFO] avg verify min: 34\n- [INFO] restore drills: 1\n- [INFO] last restore RTO min: 52\n- [WARNING] tenants over quota: 3\n- [WARNING] quota soft limit hits: 11\n- [WARNING] quota hard blocks: 2\n- [INFO] largest tenant used TB: 412\n- [INFO] new quota requests: 4\n- [INFO] ingest TB: 126\n- [INFO] egress TB: 98\n- [INFO] object count delta M: 42\n- [INFO] avg object KB: 96\n- [INFO] hot partition count: 7\n- [WARNING] disk failures: 1\n- [INFO] disk rebuilds completed: 1\n- [INFO] disk rebuild avg hr: 6.8\n- [WARNING] node restarts: 2\n- [WARNING] unhealthy OSDs: 3\n- [WARNING] scrub errors: 14\n- [INFO] metadata DB size GB: 980\n- [INFO] metadata QPS p95: 56000\n- [INFO] metadata cache hit %: 92.4%\n- [INFO] manifest backlog: 1800\n- [INFO] namespace count: 310\n- [INFO] firmware updates: 4\n- [INFO] kernel patches: 0\n- [INFO] rack power cycles: 0\n- [INFO] network drops: 12\n- [INFO] audit log GB: 74\n\n### Events\n- Triggered scheduled rebalance to reduce rack-level skew after adding 4 replacement disks in rack R12; moved 62 TB overnight.\n- Two snapshot verification failures traced to stale credentials on verifier worker; rotated keys and re-ran checks successfully.\n- One disk in stor-a-12 showed rising media errors; proactively replaced and completed rebuild in 6.8 hours.\n- Quota hard-block applied to tenant 'analytics-42' after exceeding 15 TB over allotment; coordinated temporary burst allowance and clean-up plan.\n\n### On-Call\n- Shift: Priya S. 3 pages. 7 tickets. Status: Stable after disk replacement and verifier key rotation; monitoring hottest nodes >90% and compaction backlog.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_062",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-03T10:30:00",
          "text": "## 2026-01-03 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 120ms p95: 250ms p99: 400ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/node_utilization: REQUESTS req | p50: 50ms p95: 100ms p99: 150ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/rebalance: REQUESTS req | p50: 80ms p95: 180ms p99: 300ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/compaction: REQUESTS req | p50: 70ms p95: 160ms p99: 280ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/io: REQUESTS req | p50: 60ms p95: 140ms p99: 220ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/storage_efficiency: REQUESTS req | p50: 55ms p95: 130ms p99: 210ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/backup_verification: REQUESTS req | p50: 65ms p95: 150ms p99: 240ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/quota_enforcement: REQUESTS req | p50: 45ms p95: 110ms p99: 180ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/ingest: REQUESTS req | p50: 50ms p95: 120ms p99: 200ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/reliability: REQUESTS req | p50: 40ms p95: 100ms p99: 160ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/metadata: REQUESTS req | p50: 55ms p95: 125ms p99: 210ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/maintenance: REQUESTS req | p50: 60ms p95: 140ms p99: 220ms | err: 0.1% (1 error) | success: 99.9%\n\n### Infrastructure\n- stor-a-03: CPU 67% | Mem 74% | Disk 92% | Conns: 120 | Net: 7.0/6.7 Mbps\n- stor-b-14: CPU 59% | Mem 63% | Disk 90% | Conns: 98 | Net: 5.9/5.1 Mbps\n- stor-c-05: CPU 52% | Mem 69% | Disk 86% | Conns: 85 | Net: 4.6/4.2 Mbps\n- meta-01: CPU 51% | Mem 83% | Disk 75% | Conns: 70 | Net: 2.4/2.0 Mbps\n\n### Connection Pools\n- pool-storage: active 45 | idle 10 | waiting 5 | exhaustion 0 | max 60 | avg_wait 12ms\n- pool-metadata: active 20 | idle 15 | waiting 2 | exhaustion 0 | max 40 | avg_wait 8ms\n- pool-rebalance: active 3 | idle 2 | waiting 1 | exhaustion 0 | max 10 | avg_wait 15ms\n- pool-io: active 25 | idle 5 | waiting 3 | exhaustion 0 | max 35 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [WARNING] high node utilization on stor-a-03: 95.1%\n- [INFO] kernel security patches applied on stor-b-14 and stor-c-05\n- [NOTICE] quota increase approved for tenant 'etl-warehouse'\n- [INFO] manifest backlog during compaction wave: 2100 backlog\n\n### Deployments & Changes\n- Kernel security patches applied on two storage nodes during low-traffic window; no service impact observed\n- Tenant 'etl-warehouse' quota increased by 20 TB after review\n- Metadata compaction threads tuned to reduce backlog from 2,100 to under 1,500\n\n### Events\n- Completed tail end of prior day's rebalance; verified placement groups balanced within 3% target across racks\n- Observed elevated manifest backlog during compaction wave; tuned metadata compaction threads and backlog returned under 1.5k\n- No disk failures or disk rebuilds reported\n- One node restart occurred; no service impact\n- Two unhealthy OSDs identified; ongoing monitoring\n- Scrub errors recorded: 9\n\n### On-Call\n- Shift: Marco L. 2 pages. 5 tickets. Status: Healthy; continuing incremental rebalancing to keep hottest nodes under 95% and watching metadata backlog",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_021",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-04T10:00:00",
          "text": "## 2024-02-04 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 152400 | p50: 195ms p95: 530ms p99: 980ms | err: 2.69% (4100 errors) | success: 97.31%\n- /fraud_check: REQUESTS 151900 | p50: 155ms p95: 410ms p99: 720ms | err: 0.62% (940 errors) | success: 99.38%\n- /geo_lookup: REQUESTS 149100 | p50: 290ms p95: 590ms p99: 740ms | err: 0.29% (430 errors) | success: 99.71%\n- /auth: REQUESTS 198000 | p50: 45ms p95: 116ms p99: 215ms | err: 0.14% (270 errors) | success: 99.86%\n- /product_catalog: REQUESTS 266700 | p50: 58ms p95: 172ms p99: 315ms | err: 0.13% (340 errors) | success: 99.87%\n- /search: REQUESTS 231600 | p50: 77ms p95: 210ms p99: 370ms | err: 0.19% (430 errors) | success: 99.81%\n- /recommendations: REQUESTS 187900 | p50: 98ms p95: 275ms p99: 465ms | err: 0.22% (420 errors) | success: 99.78%\n\n### Infrastructure\n- gateway-01: CPU 72% | Mem 79% | Disk 63% | Conns: 5250 | Net: 690/830 Mbps\n- gateway-02: CPU 69% | Mem 77% | Disk 62% | Conns: 5080 | Net: 670/810 Mbps\n- service-b-01: CPU 78% | Mem 79% | Disk 60% | Conns: 4950 | Net: 630/742 Mbps\n- metrics-db-01: CPU 46% | Mem 79% | Disk 86% | Conns: 372 | Net: 176/210 Mbps\n\n### Connection Pools\n- primary: active 200 | idle 0 | waiting 220 | exhaustion: 15 | max: 200 | avg_wait: 185ms\n- replica: active 100 | idle 0 | waiting 45 | exhaustion: 6 | max: 100 | avg_wait: 48ms\n- third_party_geo: active 60 | idle 0 | waiting 125 | exhaustion: 15 | max: 60 | avg_wait: 215ms\n\n### CDN & Caching\n- Hit rate: 92.0% | Bandwidth: 9.2 Gbps | Origin requests: 469200\n\n### Alerts\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-02: error_pct=2.69\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=420 retry_rate_pct=0.34\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=primary events=15 waiting=220\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Enabled feature flag 'skip-geo-on-timeout' for 5% of traffic\n- A/B test 'checkout-v2' at 15% rollout\n- Scaled gateway autoscaling min from 2 to 3\n- Updated pager routing for CHECKOUT-ERROR-RATE to include manager\n\n### On-Call\n- Shift: K. Okafor. 14 pages. 5 tickets (CO-830, SRV-B-266, POOL-588, GW-360, FF-91). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_063",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-04T10:30:00",
          "text": "## 2026-01-04 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-17: CPU 74% | Mem 79% | Disk Used 95% | Disk Busy 92% | Conns: N/A | Net: 7.9/6.9 Mbps\n- stor-b-02: CPU 61% | Mem 71% | Disk Used 91% | Disk Busy 86% | Conns: N/A | Net: 6.1/5.5 Mbps\n- stor-c-22: CPU 57% | Mem 68% | Disk Used 87% | Disk Busy 79% | Conns: N/A | Net: 5.0/4.7 Mbps\n- meta-03: CPU 56% | Mem 84% | Disk Used 77% | Disk Busy 61% | Conns: N/A | Net: 2.6/2.2 Mbps\n\n### Connection Pools\n- pool-1: active 45 | idle 10 | waiting 5 | exhaustion: 0 | max: 60 | avg_wait: 12ms\n- pool-2: active 30 | idle 20 | waiting 2 | exhaustion: 0 | max: 50 | avg_wait: 8ms\n- pool-3: active 15 | idle 25 | waiting 1 | exhaustion: 0 | max: 40 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [WARNING] Disk SMART reallocations reported on stor-a-17: 2 disks replaced, 1 queued for maintenance\n- [INFO] Rebalance jobs started: 7\n- [INFO] Rebalance jobs completed: 6\n- [INFO] Data moved during rebalance: 262 TB\n- [INFO] Avg rebalance Mbps: 875\n- [WARNING] Rebalance queue depth: 4\n- [WARNING] Skew index: 1.15\n- [WARNING] Snapshot verification failure on cold tier segment\n- [INFO] One snapshot verification failed, re-snapshotted\n- [INFO] Quota enforcement blocked writes for tenant 'ml-vision' for 22 minutes\n- [INFO] Created runbook for quota alerts at 95% quota\n\n### Deployments & Changes\n- Firmware update applied to stor-a-17\n- No kernel patches or rack power cycles today\n- Network drops recorded: 15\n- Audit log size: 76 GB\n\n### Events\n- Two disks in stor-a-17 reported SMART reallocations; replaced one immediately and queued the second for maintenance after confirming redundancy.\n- Rebalance jobs increased to relieve hottest node at 95%+; capped per-node bandwidth to avoid saturating east-west links.\n- Snapshot verification had one failure due to checksum mismatch on a cold tier segment; re-snapshotted and scheduled deeper scrub of affected placement groups.\n- Quota enforcement blocked writes for tenant 'ml-vision' for 22 minutes until they purged expired artifacts; created runbook note for proactive alerts at 95% quota.\n\n### On-Call\n- shift handoff note: Dana K. 4 pages. 9 tickets. Status: Degraded but controlled; disk health and compaction backlog are main watch items, with ongoing rebalance throttling.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_064",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-04T10:30:00",
          "text": "## 2026-01-05 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS 120 req | p50: 15ms p95: 45ms p99: 60ms | err: 0.0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS 115 req | p50: 10ms p95: 30ms p99: 50ms | err: 0.0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS 110 req | p50: 20ms p95: 55ms p99: 70ms | err: 0.0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS 105 req | p50: 12ms p95: 35ms p99: 50ms | err: 0.0% (0 errors) | success: 100%\n- /storage/io: REQUESTS 100 req | p50: 18ms p95: 50ms p99: 65ms | err: 0.0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS 98 req | p50: 14ms p95: 40ms p99: 55ms | err: 0.0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS 95 req | p50: 16ms p95: 42ms p99: 58ms | err: 0.0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS 90 req | p50: 11ms p95: 33ms p99: 48ms | err: 0.0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS 85 req | p50: 13ms p95: 37ms p99: 52ms | err: 0.0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS 80 req | p50: 17ms p95: 45ms p99: 62ms | err: 0.0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS 75 req | p50: 19ms p95: 48ms p99: 65ms | err: 0.0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS 70 req | p50: 9ms p95: 28ms p99: 40ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-17: CPU 66% | Mem 76% | Disk Used 94% | Conns: 120 | Net: 6.8/6.0 Mbps\n- stor-b-09: CPU 58% | Mem 65% | Disk Used 90% | Conns: 110 | Net: 5.5/4.9 Mbps\n- stor-c-11: CPU 60% | Mem 70% | Disk Used 88% | Conns: 105 | Net: 5.2/4.8 Mbps\n- meta-02: CPU 49% | Mem 82% | Disk Used 76% | Conns: 95 | Net: 2.2/2.1 Mbps\n\n### Connection Pools\n- pool-storage: active 45 | idle 10 | waiting 5 | exhaustion 0 | max 60 | avg_wait 12ms\n- pool-db: active 30 | idle 15 | waiting 2 | exhaustion 0 | max 50 | avg_wait 8ms\n- pool-cache: active 20 | idle 25 | waiting 1 | exhaustion 0 | max 50 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1500\n\n### Alerts\n- [CRITICAL] disk_failure on stor-a-17: 1 failure\n- [WARNING] node_unhealthy on stor-b-09: 2 unhealthy OSDs\n- [ERROR] scrub_errors on storage cluster: 11 errors\n- [INFO] disk_rebuilds completed: 2 disks\n- [INFO] node_restarts: 1\n- [WARNING] network_drops: 8\n- [INFO] firmware_updates: 3 applied\n- [INFO] rack_power_cycles: 1\n\n### Deployments & Changes\n- Rebalance jobs: 6 started and completed; moved 244 TB at 830 Mbps; queue depth 3; skew index 1.14\n- Rebalance completed; data moved 244 TB\n- Firmware updates applied to three drive controllers\n- Rack R07 power cycle for PDU maintenance; validated node auto-rejoin\n- Restore drill: 2 TB dataset restored; RTO 49 min\n- Scrub errors: 11 detected during maintenance\n\n### Events\n- Completed rebuilds for two previously failed disks; confirmed placement groups fully backfilled and scrubbed clean\n- Performed restore drill for compliance: restored 2 TB sample dataset into isolated namespace; achieved 49-minute RTO within SLO\n- Rack R07 had a planned power cycle for PDU maintenance; validated node auto-rejoin and no lingering under-replicated objects\n- Firmware updates applied to three drive controllers; monitored for post-update error bursts (none observed)\n\n### On-Call\n- Shift: Elena V. 2 pages. 6 tickets. Status: Stable; restore drill successful, utilization still climbing\u2014capacity forecast updated and procurement request drafted",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_065",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-04T10:30:00",
          "text": "## 2026-01-06 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 12ms p95: 25ms p99: 40ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/node_utilization: REQUESTS req | p50: 8ms p95: 18ms p99: 30ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/rebalance: REQUESTS req | p50: 10ms p95: 22ms p99: 35ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/compaction: REQUESTS req | p50: 9ms p95: 20ms p99: 33ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/io: REQUESTS req | p50: 11ms p95: 24ms p99: 38ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/storage_efficiency: REQUESTS req | p50: 7ms p95: 15ms p99: 25ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/backup_verification: REQUESTS req | p50: 13ms p95: 27ms p99: 42ms | err: 0.4% (4 errors) | success: 99.6%\n- /storage/quota_enforcement: REQUESTS req | p50: 6ms p95: 14ms p99: 22ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/ingest: REQUESTS req | p50: 10ms p95: 21ms p99: 34ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/reliability: REQUESTS req | p50: 8ms p95: 19ms p99: 31ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/metadata: REQUESTS req | p50: 12ms p95: 26ms p99: 41ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/maintenance: REQUESTS req | p50: 7ms p95: 16ms p99: 27ms | err: 0.1% (1 error) | success: 99.9%\n\n### Infrastructure\n- stor-a-21: CPU 78% | Mem 80% | Disk Used 96% | Conns: 1200 | Net: 8.2/7.1 Mbps\n- stor-b-06: CPU 63% | Mem 72% | Disk Used 92% | Conns: 950 | Net: 6.4/5.8 Mbps\n- stor-c-03: CPU 55% | Mem 67% | Disk Used 89% | Conns: 870 | Net: 5.3/4.9 Mbps\n- meta-01: CPU 60% | Mem 86% | Disk Used 78% | Conns: 650 | Net: 2.9/2.3 Mbps\n\n### Connection Pools\n- pool-storage: active 45 | idle 10 | waiting 5 | exhaustion 0 | max: 60 | avg_wait: 12ms\n- pool-db: active 20 | idle 15 | waiting 2 | exhaustion 0 | max: 40 | avg_wait: 8ms\n- pool-cache: active 30 | idle 20 | waiting 3 | exhaustion 1 | max: 50 | avg_wait: 10ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12,450\n\n### Alerts\n- [WARNING] compaction backlog on storage cluster: >80 pending\n- [INFO] snapshot verification failure on cold tier shard: transient read error\n- [CRITICAL] quota hard blocks for 3 tenants\n- [INFO] rebalance throughput increased overnight to move 78 TB from top nodes\n\n### Deployments & Changes\n- Adjusted compaction scheduling to avoid peak ingest hours\n- Initiated targeted deep scrub on cold tier shard\n- Coordinated cleanup for tenants over quota\n- Enabled temporary burst for regulated namespace with approval\n- Increased rebalance throughput window overnight\n\n### Events\n- Compaction backlog rose above 80 pending; adjusted compaction scheduling to avoid peak ingest hours and reduced backlog growth.\n- One snapshot verification failure due to transient read error on cold tier shard; initiated targeted deep scrub and re-verified successfully.\n- Quota hard blocks triggered for three tenants after rapid ingest; coordinated cleanup and enabled temporary burst for one regulated namespace with approval.\n- Increased rebalance throughput window overnight to cool hottest node at 96%+; moved 78 TB from top 5 nodes.\n\n### On-Call\n- shift handoff note: Hasan R. 5 pages. 10 tickets. Status: Busy; compaction pressure and quota blocks required coordination. No hardware failures today; continued targeted scrubs for cold tier.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_022",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-05T10:00:00",
          "text": "## 2024-02-05 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 155000 | p50: 205ms p95: 560ms p99: 1050ms | err: 3.1% (4800 errors) | success: 96.9%\n- /fraud_check: REQUESTS 154500 | p50: 170ms p95: 450ms p99: 780ms | err: 0.78% (1200 errors) | success: 99.22%\n- /geo_lookup: REQUESTS 151300 | p50: 320ms p95: 640ms p99: 780ms | err: 0.34% (520 errors) | success: 99.66%\n- /auth: REQUESTS 201400 | p50: 47ms p95: 122ms p99: 230ms | err: 0.16% (320 errors) | success: 99.84%\n- /product_catalog: REQUESTS 271900 | p50: 62ms p95: 185ms p99: 340ms | err: 0.15% (410 errors) | success: 99.85%\n- /search: REQUESTS 236200 | p50: 82ms p95: 225ms p99: 400ms | err: 0.22% (520 errors) | success: 99.78%\n- /recommendations: REQUESTS 191200 | p50: 106ms p95: 300ms p99: 510ms | err: 0.27% (520 errors) | success: 99.73%\n\n### Infrastructure\n- gateway-01: CPU 75% | Mem 81% | Disk 63% | Conns: 5900 | Net: 720/870 Mbps\n- gateway-02: CPU 72% | Mem 79% | Disk 62% | Conns: 5720 | Net: 700/845 Mbps\n- service-b-01: CPU 82% | Mem 82% | Disk 61% | Conns: 6100 | Net: 710/830 Mbps\n- metrics-db-01: CPU 49% | Mem 81% | Disk 87% | Conns: 410 | Net: 185/222 Mbps\n\n### Connection Pools\n- primary: active 200 | idle 0 | waiting 310 | exhaustion: 15 | max: 200 | avg_wait: 240ms\n- replica: active 100 | idle 0 | waiting 70 | exhaustion: 8 | max: 100 | avg_wait: 70ms\n- third_party_geo: active 60 | idle 0 | waiting 180 | exhaustion: 15 | max: 60 | avg_wait: 280ms\n\n### CDN & Caching\n- Hit rate: 91.8% | Bandwidth: 9.6 Gbps | Origin requests: 495100\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=third_party_geo events=15 waiting=180\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=510 retry_rate_pct=0.41\n- [CRITICAL] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=780\n\n### Deployments & Changes\n- Config change: primary pool max_size set to 260\n\n### Events\n- Increased primary pool max_size from 200 to 260\n- A/B test 'checkout-v2' at 15% rollout\n- Service-B autoscaling max set from 10 to 14\n- Temporary rate limit enabled for /checkout at gateway (per-IP)\n\n### On-Call\n- shift handoff note: S. Patel. 16 pages. 6 tickets (POOL-601, SRV-B-275, GW-372, CO-842, RL-33, VEND-102). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "escalation",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_066",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-05T10:30:00",
          "text": "## 2026-01-07 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-08: CPU 69% | Mem 75% | Disk 95% | Conns: N/A | Net: 7.1/6.4 Mbps\n- stor-b-19: CPU 64% | Mem 70% | Disk 92% | Conns: N/A | Net: 6.0/5.6 Mbps\n- stor-c-16: CPU 58% | Mem 69% | Disk 90% | Conns: N/A | Net: 5.4/5.0 Mbps\n- meta-02: CPU 55% | Mem 84% | Disk 79% | Conns: N/A | Net: 2.7/2.2 Mbps\n\n### Connection Pools\n- pool-01: active 12 | idle 5 | waiting 2 | exhaustion: 0 | max: 20 | avg_wait: 15ms\n- pool-02: active 8 | idle 7 | waiting 1 | exhaustion: 0 | max: 15 | avg_wait: 10ms\n- pool-03: active 20 | idle 0 | waiting 4 | exhaustion: 1 | max: 20 | avg_wait: 25ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 124,560\n\n### Alerts\n- [CRITICAL] disk failure on stor-b-19: disk rebuild completed in 6.2 hours\n- [WARNING] NIC driver parameter change on stor-b-19: validated stable link, network drops reduced\n- [INFO] rebalance jobs: started 7, completed 8, queue depth 3\n- [INFO] quota alert thresholds updated to 90/95%, weekly top 10 tenants report added\n\n### Deployments & Changes\n- Replaced failing disk on stor-b-19; rebuild completed; no under-replicated objects remain\n- Restarted two storage nodes after NIC driver parameter change; validated link stability\n- Rebalance completed more jobs than started; queue cleared to depth 3; hottest-node utilization decreased by ~0.6%\n- Updated quota alert thresholds to warn at 90/95%; added weekly report for top 10 tenants\n\n### Events\n- Replaced failing disk on stor-b-19 and completed rebuild in 6.2 hours; confirmed no under-replicated objects remain\n- Two storage nodes restarted after planned NIC driver parameter change; validated stable link and reduced network drops thereafter\n- Rebalance completed more jobs than started, clearing queue to depth 3; hottest-node utilization fell by ~0.6%\n- Updated quota alert thresholds to warn at 90/95% and added weekly report for top 10 fastest-growing tenants\n\n### On-Call\n- shift handoff note: Jae M. 3 pages. 6 tickets. Status: Stable; minor maintenance executed successfully, capacity trend steady upward with improved rebalance effectiveness",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_067",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-05T10:30:00",
          "text": "## 2026-01-08 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-04: CPU 81% | Mem 82% | Disk Used 97% | Disk Busy 94% | Conns: N/A | Net: In 8.5 Gbps / Out 7.4 Gbps\n- stor-b-12: CPU 66% | Mem 73% | Disk Used 93% | Disk Busy 89% | Conns: N/A | Net: In 6.7 Gbps / Out 6.1 Gbps\n- stor-c-09: CPU 60% | Mem 70% | Disk Used 91% | Disk Busy 83% | Conns: N/A | Net: In 5.8 Gbps / Out 5.2 Gbps\n- meta-03: CPU 63% | Mem 87% | Disk Used 80% | Disk Busy 66% | Conns: N/A | Net: In 3.1 Gbps / Out 2.4 Gbps\n\n### Connection Pools\n(Information not provided in data sheet)\n\n### CDN & Caching\n(Information not provided in data sheet)\n\n### Alerts\n- [SEVERITY] Heavy ingest day on hostname: +206 TB\n- [SEVERITY] Snapshot verification failures on hostname: 2 failures\n- [SEVERITY] Scrub errors on hostname: 26 errors\n- [SEVERITY] Rebalance queue depth increased on hostname: 5\n\n### Deployments & Changes\n- Set temporary cap on ingest for two non-critical tenants during peak window.\n- Replaced verifier worker instance after snapshot verification failures.\n- Scheduled staggered deep scrubs overnight following scrub errors.\n- Initiated additional rebalance jobs focusing on hot partitions; queue depth increased to 5.\n\n### Events\n- Heavy ingest (+206 TB) drove compaction pending near 100; set temporary cap on ingest for two non-critical tenants during peak window.\n- Two snapshot verifications failed on the same verifier worker; replaced worker instance and re-ran verification (subsequent passes succeeded).\n- Scrub errors increased on a subset of placement groups; scheduled staggered deep scrubs overnight and monitored for repeatable checksum issues.\n- Initiated additional rebalance jobs focused on hot partitions; queue depth increased to 5 while throttling to protect disk busy levels.\n\n### On-Call\n- Shift: Sofia G. 6 pages. 12 tickets. Status: Degraded; compaction and scrub workload elevated. Mitigations in place (ingest caps, verifier replacement) and deep scrubs scheduled.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_068",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-05T10:30:00",
          "text": "## 2026-01-09 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-04: CPU 73% | Mem 79% | Disk 97% | Conns: N/A | Net: 7.6/6.8 Mbps\n- stor-b-01: CPU 60% | Mem 71% | Disk 92% | Conns: N/A | Net: 6.2/5.7 Mbps\n- stor-c-20: CPU 56% | Mem 68% | Disk 90% | Conns: N/A | Net: 5.1/4.9 Mbps\n- meta-01: CPU 58% | Mem 85% | Disk 80% | Conns: N/A | Net: 2.8/2.4 Mbps\n\n### Connection Pools\n- pool-storage: active 120 | idle 30 | waiting 5 | exhaustion 0 | max 150 | avg_wait 12ms\n- pool-meta: active 20 | idle 10 | waiting 2 | exhaustion 0 | max 40 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [CRITICAL] disk_failure on stor-b-01: 1 disk failure\n- [WARNING] disk_rebuild on stor-b-01: 1 disk rebuild in 6.9 hours\n- [INFO] firmware_update applied to stor-a-04 and stor-c-20\n- [INFO] network_drops: 12 on storage network\n\n### Deployments & Changes\n- Firmware updates applied to stor-a-04 and stor-c-20\n- Rebalance backlog cleared by completing 9 jobs; skew index improved to 1.11\n- Lifted ingest caps after disk busy p95 dropped below 89%\n- Validated no increase in disk error counters post firmware updates\n\n### Events\n- Cleared rebalance backlog by completing 9 jobs; measured improved skew index to 1.11 and reduced hot partition concentration\n- Firmware updates applied to two storage controllers; validated no increase in disk error counters post-change\n- One disk failure on stor-b-01; replaced and rebuilt in 6.9 hours with no customer impact\n- Lifted prior ingest caps after compaction pending fell under 80 and disk busy p95 dropped below 89%\n\n### On-Call\n- shift handoff note: Shift: Tom W. 3 pages. 8 tickets. Status: Stable; recovery from prior compaction pressure continues, with steady utilization and successful maintenance",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_023",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-06T10:00:00",
          "text": "## 2024-02-06 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 157800 | p50: 220ms p95: 590ms p99: 1120ms | err: 3.55% (5600 errors) | success: 96.45%\n- /fraud_check: REQUESTS 157200 | p50: 190ms p95: 500ms p99: 860ms | err: 0.95% (1500 errors) | success: 99.05%\n- /geo_lookup: REQUESTS 153400 | p50: 360ms p95: 710ms p99: 850ms | err: 0.45% (690 errors) | success: 99.55%\n- /auth: REQUESTS 204700 | p50: 50ms p95: 130ms p99: 245ms | err: 0.19% (380 errors) | success: 99.81%\n- /product_catalog: REQUESTS 276300 | p50: 66ms p95: 198ms p99: 365ms | err: 0.17% (480 errors) | success: 99.83%\n- /search: REQUESTS 241100 | p50: 88ms p95: 245ms p99: 440ms | err: 0.27% (640 errors) | success: 99.73%\n- /recommendations: REQUESTS 195400 | p50: 115ms p95: 325ms p99: 560ms | err: 0.33% (640 errors) | success: 99.67%\n\n### Infrastructure\n- gateway-01: CPU 78% | Mem 83% | Disk 63% | Conns: 7100 | Net: 750/910 Mbps\n- gateway-02: CPU 75% | Mem 81% | Disk 62% | Conns: 6905 | Net: 730/885 Mbps\n- service-b-01: CPU 86% | Mem 85% | Disk 61% | Conns: 7800 | Net: 820/960 Mbps\n- metrics-db-01: CPU 52% | Mem 83% | Disk 88% | Conns: 460 | Net: 198/238 Mbps\n\n### Connection Pools\n- primary: active 260 | idle 0 | waiting 420 | exhaustion: 32 | max: 260 | avg_wait: 320ms\n- replica: active 100 | idle 0 | waiting 95 | exhaustion: 12 | max: 100 | avg_wait: 88ms\n- third_party_geo: active 60 | idle 0 | waiting 260 | exhaustion: 40 | max: 60 | avg_wait: 420ms\n\n### CDN & Caching\n- Hit rate: 91.6% | Bandwidth: 9.9 Gbps | Origin requests: 520300\n\n### Alerts\n- [CRITICAL] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=850\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=third_party_geo events=40 waiting=260\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=3.55\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Enabled feature flag 'skip-geo-on-timeout' for 25% of traffic\n- A/B test 'checkout-v2' at 15% rollout\n- Added dedicated egress NAT for geo vendor traffic\n- Opened vendor incident with geo provider (ticket ID provided)\n\n### On-Call\n- Shift: L. Nguyen. 18 pages. 6 tickets (VEND-110, CO-856, POOL-620, NAT-12, GW-389, FF-99). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_069",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-06T10:30:00",
          "text": "## 2026-01-10 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 120ms p95: 250ms p99: 400ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/node_utilization: REQUESTS req | p50: 80ms p95: 200ms p99: 350ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/rebalance: REQUESTS req | p50: 100ms p95: 220ms p99: 370ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/compaction: REQUESTS req | p50: 90ms p95: 210ms p99: 340ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/io: REQUESTS req | p50: 85ms p95: 215ms p99: 360ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/storage_efficiency: REQUESTS req | p50: 75ms p95: 190ms p99: 330ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/backup_verification: REQUESTS req | p50: 70ms p95: 180ms p99: 310ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/quota_enforcement: REQUESTS req | p50: 65ms p95: 170ms p99: 290ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/ingest: REQUESTS req | p50: 60ms p95: 160ms p99: 280ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/reliability: REQUESTS req | p50: 55ms p95: 150ms p99: 260ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/metadata: REQUESTS req | p50: 50ms p95: 140ms p99: 240ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/maintenance: REQUESTS req | p50: 45ms p95: 130ms p99: 220ms | err: 0.1% (1 error) | success: 99.9%\n\n### Infrastructure\n- stor-a-09: CPU 82% | Mem 83% | Disk Used 98% | Disk Busy 95% | Conns: 120 | Net: 8.7/7.6 Gbps\n- stor-b-16: CPU 65% | Mem 72% | Disk Used 94% | Disk Busy 90% | Conns: 95 | Net: 6.9/6.2 Gbps\n- stor-c-07: CPU 59% | Mem 69% | Disk Used 91% | Disk Busy 84% | Conns: 80 | Net: 5.6/5.1 Gbps\n- meta-02: CPU 64% | Mem 88% | Disk Used 81% | Disk Busy 68% | Conns: 60 | Net: 3.2/2.6 Gbps\n\n### Connection Pools\n- rebalance_pool: active 12 | idle 3 | waiting 2 | exhaustion 0 | max 20 | avg_wait 45ms\n- ingest_pool: active 8 | idle 4 | waiting 1 | exhaustion 0 | max 15 | avg_wait 30ms\n- metadata_pool: active 10 | idle 5 | waiting 0 | exhaustion 0 | max 15 | avg_wait 20ms\n- replication_pool: active 14 | idle 2 | waiting 3 | exhaustion 1 | max 20 | avg_wait 50ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 2.5 Gbps | Origin requests: 1500\n\n### Alerts\n- [CRITICAL] Hottest node approached 99% utilization on stor-a-09: 98.6%\n- [WARNING] Snapshot verification failure on storage system: 1 failure\n- [WARNING] Quota hard blocks triggered for 3 tenants\n- [INFO] Deep scrub and replica compare scheduled overnight due to scrub errors\n\n### Deployments & Changes\n- Initiated targeted rebalance from stor-a-09 to underutilized nodes\n- Repaired snapshot manifest index after verification failure\n- Guided tenants to move archival data to erasure-coded pool\n- Re-enabled writes post quota blocks\n- Scheduled deep scrub and replica compare overnight\n\n### Events\n- Hottest node approached 99% utilization; launched targeted rebalance from stor-a-09 to underutilized nodes and set alert at 98.5%\n- One snapshot verification failure due to missing snapshot manifest entry; repaired manifest index and re-verified successfully\n- Quota hard blocks triggered for three tenants during batch loads; guided teams to move archival data to erasure-coded pool and re-enabled writes\n- Scrub error count increased; correlated to same 6 placement groups and scheduled deep scrub plus replica compare overnight\n\n### On-Call\n- shift handoff note: Noor A. 6 pages. 11 tickets. Status: Degraded; immediate risk is hottest-node utilization. Rebalance in progress and quotas actively managed to protect headroom",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_070",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-06T10:30:00",
          "text": "## 2026-01-11 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-09: CPU 70% | Mem 80% | Disk Used 97% | Conns: 0 | Net: 7.3/6.5 Mbps\n- stor-b-16: CPU 61% | Mem 70% | Disk Used 93% | Conns: 0 | Net: 6.1/5.8 Mbps\n- stor-c-15: CPU 57% | Mem 68% | Disk Used 91% | Conns: 0 | Net: 5.0/4.9 Mbps\n- meta-03: CPU 60% | Mem 87% | Disk Used 82% | Conns: 0 | Net: 2.9/2.6 Mbps\n\n### Connection Pools\n- pool-storage: active 120 | idle 0 | waiting 0 | exhaustion: 0 | max: 200 | avg_wait: 0ms\n- pool-metadata: active 50 | idle 10 | waiting 2 | exhaustion: 0 | max: 100 | avg_wait: 1ms\n- pool-rebalance: active 8 | idle 0 | waiting 0 | exhaustion: 0 | max: 20 | avg_wait: 0ms\n\n### CDN & Caching\n- Hit rate: 90.6% | Bandwidth: 4.8 Gbps | Origin requests: 1500\n\n### Alerts\n- [WARNING] high node utilization on stor-a-09: 98.2%\n- [INFO] scrub errors count: 12\n- [INFO] metadata cache hit rate steady at 90.6%\n- [WARNING] network drops detected: 11\n- [INFO] backup verification completed with 0 failures\n- [INFO] restore drill completed in 47 minutes\n- [INFO] applied kernel patch on metadata node\n- [INFO] aggressive rebalance completed, 10 jobs, skew index 1.10\n- [INFO] deep scrubs on placement groups completed\n- [INFO] disk failures: 0, node restarts: 1, unhealthy OSDs: 2\n\n### Deployments & Changes\n- Applied single kernel patch on metadata node; monitored cache hit rate\n- Completed aggressive rebalance (10 jobs); reduced hottest-node utilization from 98.6% to 98.2%; skew index improved to 1.10\n- Ran weekly restore drill restoring object index plus 500 GB data sample; achieved 47-minute RTO and validated checksum parity\n- No firmware updates; one kernel patch applied; no rack power cycles; network drops recorded; audit log size: 77 GB\n\n### Events\n- Completed aggressive rebalance (10 jobs) and reduced hottest-node utilization from 98.6% to 98.2%; skew index improved to 1.10\n- Ran weekly restore drill restoring object index plus 500 GB data sample; achieved 47-minute RTO and validated checksum parity\n- Applied single kernel patch on metadata node; monitored metadata cache hit rate (steady at ~90%)\n- Deep scrubs on previously problematic placement groups completed; scrub errors dropped to 12\n\n### On-Call\n- shift handoff note: Linh P. 3 pages. 7 tickets. Status: Stable; still operating at high utilization, but hottest-node risk reduced and scrub health improved",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_071",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-06T10:30:00",
          "text": "## 2026-01-12 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-02: CPU 84% | Mem 84% | Disk Used 98% | Disk Busy 96% | Conns: N/A | Net: In 9.1 Gbps / Out 7.9 Gbps\n- stor-b-10: CPU 68% | Mem 74% | Disk Used 95% | Disk Busy 91% | Conns: N/A | Net: In 7.1 Gbps / Out 6.4 Gbps\n- stor-c-18: CPU 61% | Mem 71% | Disk Used 92% | Disk Busy 85% | Conns: N/A | Net: In 5.9 Gbps / Out 5.3 Gbps\n- meta-01: CPU 66% | Mem 89% | Disk Used 83% | Disk Busy 69% | Conns: N/A | Net: In 3.3 Gbps / Out 2.8 Gbps\n\n### Connection Pools\n(Information not provided in data sheet)\n\n### CDN & Caching\n(Information not provided in data sheet)\n\n### Alerts\n- [CRITICAL] Disk predictive failure in stor-b-10: scheduled replacement window\n- [WARNING] Quota hard blocks for 4 tenants\n- [INFO] High ingest volume (+212 TB) triggered capacity review\n- [INFO] Compaction pending approaching 100; overnight tuning scheduled\n\n### Deployments & Changes\n- Emergency capacity review initiated due to large batch ingest\n- Quota hard blocks applied to four tenants; rate limits enforced\n- Shard set drained and replacement scheduled for disk in stor-b-10\n- Compaction concurrency tuned to reduce disk saturation and queued overnight\n\n### Events\n- Large batch ingest (+212 TB) drove overall utilization above 80%; initiated emergency capacity review and accelerated procurement timeline\n- Disk in stor-b-10 reported predictive failure; drained shard set and scheduled replacement window with on-site tech\n- Quota hard blocks for four tenants; implemented temporary per-tenant ingest rate limits and requested teams to shift older data to erasure-coded pool\n- Compaction pending approached 100; tuned compaction concurrency down to reduce disk busy saturation and queued overnight catch-up run\n\n### On-Call\n- Shift: Avery C. 7 pages. 14 tickets. Status: Degraded; high utilization and heavy ingest causing compaction pressure and quota incidents. Hardware replacement scheduled and rate limits applied.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_024",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-07T10:00:00",
          "text": "## 2024-02-07 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 160200 | p50: 230ms p95: 620ms p99: 1180ms | err: 3.87% (6200 errors) | success: 96.13%\n- /fraud_check: REQUESTS 159600 | p50: 205ms p95: 540ms p99: 920ms | err: 1.1% (1750 errors) | success: 98.9%\n- /geo_lookup: REQUESTS 155100 | p50: 380ms p95: 740ms p99: 870ms | err: 0.49% (760 errors) | success: 99.51%\n- /auth: REQUESTS 207900 | p50: 52ms p95: 135ms p99: 255ms | err: 0.2% (420 errors) | success: 99.8%\n- /product_catalog: REQUESTS 279900 | p50: 68ms p95: 205ms p99: 380ms | err: 0.19% (520 errors) | success: 99.81%\n- /search: REQUESTS 244600 | p50: 92ms p95: 255ms p99: 460ms | err: 0.29% (710 errors) | success: 99.71%\n- /recommendations: REQUESTS 197800 | p50: 120ms p95: 340ms p99: 590ms | err: 0.35% (700 errors) | success: 99.65%\n\n### Infrastructure\n- gateway-01: CPU 80% | Mem 84% | Disk 63% | Conns: 7600 | Net: 770/935 Mbps\n- gateway-02: CPU 77% | Mem 82% | Disk 62% | Conns: 7420 | Net: 750/910 Mbps\n- service-b-01: CPU 88% | Mem 86% | Disk 62% | Conns: 8600 | Net: 890/1040 Mbps\n- metrics-db-01: CPU 54% | Mem 84% | Disk 88% | Conns: 480 | Net: 205/245 Mbps\n\n### Connection Pools\n- primary: active 260 | idle 0 | waiting 480 | exhaustion: 45 | max: 260 | avg_wait: 360ms\n- replica: active 100 | idle 0 | waiting 110 | exhaustion: 16 | max: 100 | avg_wait: 95ms\n- third_party_geo: active 60 | idle 0 | waiting 310 | exhaustion: 55 | max: 60 | avg_wait: 460ms\n\n### CDN & Caching\n- Hit rate: 91.5% | Bandwidth: 10.1 Gbps | Origin requests: 535800\n\n### Alerts\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=820 retry_rate_pct=0.64\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=primary events=45 waiting=480\n- [CRITICAL] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=870\n\n### Deployments & Changes\n- Config change: service-B retry max_attempts set to 2\n- Config change: service-B retry jitter enabled\n\n### Events\n- Set service-B retry max_attempts from 4 to 2\n- Enabled retry jitter in service-B HTTP client\n- A/B test 'checkout-v2' at 15% rollout\n- Reduced feature flag 'recommendations-ml' from 10% to 0%\n\n### On-Call\n- Shift: M. Rossi. 20 pages. 7 tickets (SRV-B-289 retry cap, POOL-635, CO-870, GW-401, FF-104, VEND-117, OBS-240). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_072",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-07T10:30:00",
          "text": "## 2026-01-13 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-b-10: CPU 62% | Mem 72% | Disk 94% | Conns: N | Net: 6.4/5.9 Gbps\n- stor-a-15: CPU 76% | Mem 81% | Disk 97% | Conns: N | Net: 8.0/7.1 Gbps\n- stor-c-02: CPU 58% | Mem 69% | Disk 92% | Conns: N | Net: 5.4/5.0 Gbps\n- meta-02: CPU 63% | Mem 88% | Disk 83% | Conns: N | Net: 3.0/2.7 Gbps\n\n### Connection Pools\n- pool-storage: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 150 | avg_wait 12ms\n- pool-meta: active 20 | idle 10 | waiting 1 | exhaustion 0 | max 50 | avg_wait 5ms\n- pool-rebalance: active 15 | idle 5 | waiting 2 | exhaustion 1 | max 20 | avg_wait 20ms\n- pool-io: active 50 | idle 15 | waiting 3 | exhaustion 0 | max 70 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 89.8% | Bandwidth: 2.5 Gbps | Origin requests: 15000\n\n### Alerts\n- [WARNING] disk failure on stor-b-10: 1 disk failed\n- [CRITICAL] node restart on stor-a-15: 1 node restarted\n- [ERROR] scrub errors: 19\n- [WARNING] quota soft limit hit: 23 tenants\n- [CRITICAL] quota hard blocks: 3 tenants\n- [INFO] metadata database size: 1105 GB\n- [WARNING] network drops: 14\n- [INFO] backup snapshots taken: 48\n- [ERROR] snapshot verification failure: 1\n- [INFO] last restore RTO: 47 min\n\n### Deployments & Changes\n- Replaced predicted-failure disk on stor-b-10; rebuild completed in 7.6 hours; unhealthy OSD count dropped to 3.\n- Rebalance moved 354 TB-equivalent across jobs; improved skew index to 1.09.\n- Expanded verifier disk; re-verified snapshot successfully.\n- Continued quota rate limits for top offenders; quota hard blocks reduced from 4 to 3.\n\n### Events\n- Replaced predicted-failure disk on stor-b-10; rebuild completed in 7.6 hours and unhealthy OSD count dropped to 3.\n- Rebalance moved 354 TB-equivalent across jobs (logical moved 354 TB); skew index improved to 1.09.\n- Snapshot verification failure tied to verifier filesystem filling; verifier disk expanded and re-verified successfully.\n- Quota rate limits enforced; quota hard blocks decreased from 4 to 3.\n\n### On-Call\n- Shift: Beth N. 4 pages. 9 tickets. Status: Stable but tight; utilization >81% with ongoing quota management. Hardware issue resolved; monitoring verifier capacity and compaction pending.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_073",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-07T10:30:00",
          "text": "## 2026-01-14 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS 120 req | p50: 15ms p95: 45ms p99: 78ms | err: 0.0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS 120 req | p50: 10ms p95: 30ms p99: 55ms | err: 0.0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS 120 req | p50: 20ms p95: 60ms p99: 90ms | err: 0.0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS 120 req | p50: 12ms p95: 35ms p99: 65ms | err: 0.0% (0 errors) | success: 100%\n- /storage/io: REQUESTS 120 req | p50: 18ms p95: 50ms p99: 80ms | err: 0.0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS 120 req | p50: 14ms p95: 40ms p99: 70ms | err: 0.0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS 120 req | p50: 16ms p95: 42ms p99: 72ms | err: 0.0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS 120 req | p50: 11ms p95: 33ms p99: 60ms | err: 0.0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS 120 req | p50: 13ms p95: 38ms p99: 68ms | err: 0.0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS 120 req | p50: 17ms p95: 48ms p99: 75ms | err: 0.0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS 120 req | p50: 19ms p95: 52ms p99: 80ms | err: 0.0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS 120 req | p50: 9ms p95: 28ms p99: 50ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-02: CPU 86% | Mem 85% | Disk Used 99% | Conns: 120 | Net: 9.4/8.2 Mbps\n- stor-b-05: CPU 70% | Mem 76% | Disk Used 95% | Conns: 98 | Net: 7.3/6.7 Mbps\n- stor-c-21: CPU 63% | Mem 72% | Disk Used 93% | Conns: 85 | Net: 6.1/5.5 Mbps\n- meta-03: CPU 68% | Mem 90% | Disk Used 84% | Conns: 75 | Net: 3.5/2.9 Mbps\n\n### Connection Pools\n- pool-storage: active 68 | idle 22 | waiting 4 | exhaustion 2 | max 100 | avg_wait 15ms\n- pool-metadata: active 15 | idle 10 | waiting 1 | exhaustion 0 | max 30 | avg_wait 8ms\n- pool-rebalance: active 9 | idle 3 | waiting 2 | exhaustion 1 | max 15 | avg_wait 20ms\n- pool-io: active 20 | idle 5 | waiting 0 | exhaustion 0 | max 25 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 87% | Bandwidth: 2.3 Gbps | Origin requests: 124,567\n\n### Alerts\n- [CRITICAL] Hottest node hit 99.0% used; initiated emergency hot-spot evacuation rebalance and placed temporary write sheds for two test namespaces\n- [WARNING] Scrub errors climbed to 31; identified correlation with high tombstone percentage in one tenant's workload and scheduled compaction priority for those keyspaces\n- [MAJOR] Quota hard blocks rose to 4; enabled daily quota digest and required tenant owners to acknowledge cleanup SLAs\n- [INFO] Metadata cache hit dipped below 89%; increased cache size on metadata nodes and reduced manifest backlog via batching\n\n### Deployments & Changes\n- Initiated emergency hot-spot evacuation rebalance on hottest node; placed temporary write sheds for two test namespaces\n- Scheduled priority compaction for tenant keyspaces with high tombstone counts\n- Enabled daily quota digest; notified tenant owners to acknowledge cleanup SLAs\n- Increased metadata cache size; reduced manifest backlog through batching\n\n### Events\n- Hottest node hit 99.0% used; initiated emergency hot-spot evacuation rebalance and placed temporary write sheds for two test namespaces\n- Scrub errors climbed to 31; identified correlation with high tombstone percentage in one tenant's workload and scheduled compaction priority for those keyspaces\n- Quota hard blocks rose to 4; enabled daily quota digest and required tenant owners to acknowledge cleanup SLAs\n- Metadata cache hit dipped below 89%; increased cache size on metadata nodes and reduced manifest backlog via batching\n\n### On-Call\n- shift handoff note: Omar T. 8 pages. 15 tickets. Status: Degraded; capacity critically tight on hottest nodes. Active mitigations: write sheds, targeted rebalance, and compaction prioritization for tombstone-heavy tenants",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_074",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-07T10:30:00",
          "text": "## 2026-01-15 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-02: CPU 78% | Mem 82% | Disk 98% | Conns: N/A | Net: 8.0/7.2 Mbps\n- stor-b-05: CPU 66% | Mem 74% | Disk 95% | Conns: N/A | Net: 6.7/6.3 Mbps\n- stor-c-13: CPU 60% | Mem 70% | Disk 93% | Conns: N/A | Net: 5.8/5.3 Mbps\n- meta-01: CPU 62% | Mem 89% | Disk 84% | Conns: N/A | Net: 3.1/2.8 Mbps\n\n### Connection Pools\n- pool-storage: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 150 | avg_wait 12ms\n- pool-meta: active 20 | idle 10 | waiting 1 | exhaustion 0 | max 50 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1500\n\n### Alerts\n- [CRITICAL] disk error on stor-a-02: read errors detected\n- [WARNING] high node utilization on node 70: 98.6%\n- [INFO] compaction pending reduced to 90\n- [INFO] quota hard blocks: 3\n- [INFO] quota soft limit hits: 24\n\n### Deployments & Changes\n- Rebalance jobs: 12 started and completed; moved 406 TB at 1040 Mbps; queue depth 4; skew index 1.08\n- Quota digest email and dashboard rollout completed\n- Removed temporary write sheds after headroom recovery confirmation\n\n### Events\n- Completed 12 rebalance jobs and reduced hottest-node utilization to 98.6%; removed temporary write sheds after confirming headroom recovery\n- One disk began accumulating read errors on stor-a-02; pre-failed disk and scheduled replacement with shard drain to avoid rebuild at peak\n- Compaction pending reduced from 104 to 90 after prioritizing tombstone-heavy keyspaces; scrub errors decreased accordingly\n- Rolled out quota digest email and dashboard; quota hard blocks dropped from 4 to 3\n\n### On-Call\n- Shift: Mei H. 4 pages. 8 tickets. Status: Stable but tight; rebalance effective, pending disk replacement planned, continuing quota governance and compaction prioritization",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_025",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-08T10:00:00",
          "text": "## 2024-02-08 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 158900 | p50: 210ms p95: 570ms p99: 1100ms | err: 3.4% (5400 errors) | success: 96.6%\n- /fraud_check: REQUESTS 158400 | p50: 185ms p95: 500ms p99: 860ms | err: 0.95% (1500 errors) | success: 99.05%\n- /geo_lookup: REQUESTS 153900 | p50: 350ms p95: 700ms p99: 860ms | err: 0.44% (680 errors) | success: 99.56%\n- /auth: REQUESTS 206100 | p50: 50ms p95: 130ms p99: 245ms | err: 0.17% (360 errors) | success: 99.83%\n- /product_catalog: REQUESTS 277400 | p50: 65ms p95: 198ms p99: 365ms | err: 0.17% (460 errors) | success: 99.83%\n- /search: REQUESTS 242200 | p50: 88ms p95: 245ms p99: 440ms | err: 0.26% (620 errors) | success: 99.74%\n- /recommendations: REQUESTS 196100 | p50: 112ms p95: 320ms p99: 560ms | err: 0.31% (600 errors) | success: 99.69%\n\n### Infrastructure\n- gateway-01: CPU 76% | Mem 82% | Disk 63% | Conns: 6900 | Net: 740/900 Mbps\n- gateway-02: CPU 73% | Mem 80% | Disk 62% | Conns: 6720 | Net: 720/875 Mbps\n- service-b-01: CPU 80% | Mem 83% | Disk 61% | Conns: 7200 | Net: 760/890 Mbps\n- metrics-db-01: CPU 50% | Mem 82% | Disk 88% | Conns: 450 | Net: 190/232 Mbps\n\n### Connection Pools\n- primary: active 248 | idle 12 | waiting 320 | exhaustion: 38 | max: 260 | avg_wait: 240ms\n- replica: active 96 | idle 4 | waiting 70 | exhaustion: 12 | max: 100 | avg_wait: 70ms\n- third_party_geo: active 60 | idle 0 | waiting 240 | exhaustion: 48 | max: 60 | avg_wait: 390ms\n\n### CDN & Caching\n- Hit rate: 91.7% | Bandwidth: 9.7 Gbps | Origin requests: 508200\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=third_party_geo events=48 waiting=240\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-02: error_pct=3.40\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Feature flag 'skip-geo-on-timeout' set to 50% of traffic\n- A/B test 'checkout-v2' at 15% rollout\n- Reduced /checkout max in-flight requests from 1200 to 900 (gateway)\n- Vendor status page update received for geo provider\n\n### On-Call\n- Shift: E. Johnson. 16 pages. 5 tickets (GW-412 in-flight cap, CO-882, POOL-649, VEND-124, FF-110). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_075",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-08T10:30:00",
          "text": "## 2026-01-16 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-b-03: CPU 88% | Mem 86% | Disk Used 98% | Disk Busy 97% | Conns: N/A | Net: 9.6/8.4 Mbps\n- stor-a-14: CPU 72% | Mem 79% | Disk Used 96% | Disk Busy 90% | Conns: N/A | Net: 7.5/6.8 Mbps\n- stor-c-06: CPU 64% | Mem 73% | Disk Used 94% | Disk Busy 88% | Conns: N/A | Net: 6.5/5.9 Mbps\n- meta-02: CPU 70% | Mem 91% | Disk Used 85% | Disk Busy 73% | Conns: N/A | Net: 3.7/3.0 Mbps\n\n### Connection Pools\n- pool-requests: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- (No specific pool data provided; omitted due to absence)\n\n### CDN & Caching\n- Hit rate: N/A\n- Bandwidth: N/A Gbps\n- Origin requests: N/A\n\n### Alerts\n- [SEVERITY] quota_hard_blocks on hostname: 5\n- [SEVERITY] verifier_worker_saturation on hostname: 2 failures\n- [SEVERITY] verification_failure on hostname: 2 failures\n- [SEVERITY] metadata_manifest_backlog: 4500\n- [SEVERITY] quota_soft_limit_hits: 29\n- [SEVERITY] quota_over_quota: 6\n- [SEVERITY] disk_failures: 0\n- [SEVERITY] disk_rebuilds_completed: 1\n- [SEVERITY] scrub_errors: 33\n- [SEVERITY] network_drops: 26\n\n### Deployments & Changes\n- Temporary hard cap instituted for two experimental tenants due to quota overload.\n- Expanded verifier worker pool and reran verification after failures.\n- Enabled batching and increased metadata DB write buffer during off-peak to address manifest backlog.\n- Rebuild completed after 7.9 hours, triggered by pre-fail condition.\n\n### Events\n- Very heavy ingest (+224 TB) caused quota hard blocks to jump to 5; instituted temporary hard cap for two experimental tenants and escalated cleanup.\n- Two snapshot verification failures due to verifier worker saturation and missing time window; expanded verifier worker pool and reran verification.\n- Completed one rebuild triggered by earlier pre-fail; rebuild took 7.9 hours and increased compaction pending due to backfill I/O.\n- Metadata manifest backlog grew to 4.5k; enabled batching and increased metadata DB write buffer during off-peak.\n\n### On-Call\n- Shift: Riley J. 9 pages. 17 tickets. Status: Degraded; primary issues were quota incidents and verification failures under load. Capacity remains tight; mitigations applied (caps, verifier pool expansion, batching).",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_076",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-08T10:30:00",
          "text": "## 2026-01-17 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-b-03: CPU 74% | Mem 82% | Disk Used 97% | Conns: N | Net: 7.8/7.0 Mbps\n- stor-a-06: CPU 63% | Mem 74% | Disk Used 95% | Conns: N | Net: 6.2/5.8 Mbps\n- stor-c-10: CPU 59% | Mem 71% | Disk Used 93% | Conns: N | Net: 5.7/5.2 Mbps\n- meta-03: CPU 66% | Mem 90% | Disk Used 85% | Conns: N | Net: 3.4/3.0 Mbps\n\n### Connection Pools\n- pool-storage: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-db: active 45 | idle 10 | waiting 2 | exhaustion: 0 | max: 60 | avg_wait: 8ms\n- pool-cache: active 25 | idle 15 | waiting 1 | exhaustion: 0 | max: 40 | avg_wait: 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 12,450\n\n### Alerts\n- [CRITICAL] disk_failure on stor-a-06: disk replaced and rebuilt in 6.7 hours\n- [WARNING] quota_soft_limit_hits on tenant ID 23: 25 hits\n- [INFO] network_drops on storage network: 15 drops\n- [INFO] verifier worker pool expanded: zero verification failures; verification window avg 37 min\n- [INFO] rebalance completed 11 jobs; skew index reduced to 1.07; hottest node at 98.4%\n- [WARNING] scrub_errors: 18 errors detected in storage scrub logs\n\n### Deployments & Changes\n- Rebalance jobs: 9 started, 11 completed; moved 386 TB at 1010 Mbps; queue depth 3; skew index 1.07\n- Disk failure on stor-a-06 replaced and rebuilt in 6.7 hours\n- Quota hard blocks reduced from 5 to 3; caps maintained on experimental tenants\n- Verifier worker pool expanded; verification window shortened to 37 minutes\n- No firmware updates or kernel patches applied today\n- Network drops recorded: 15 incidents\n- Audit log size: 82 GB\n\n### Events\n- Expanded verifier worker pool resulted in zero verification failures; shortened verification window average to 37 minutes\n- Rebalance completed 11 jobs and reduced skew index to 1.07; hottest node decreased to 98.4% despite overall growth\n- Disk failure on stor-a-06 replaced and rebuilt in 6.7 hours; monitored for compaction surge during backfill\n- Reduced quota hard blocks from 5 to 3 by maintaining caps on experimental tenants and confirming cleanup completion\n\n### On-Call\n- shift handoff note: Shift: Grace F. 4 pages. 9 tickets. Status: Stable; improvements from verifier scaling and rebalance. Continuing high utilization monitoring and managing tenant caps until new capacity arrives",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_077",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-08T10:30:00",
          "text": "## 2026-01-18 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster/usage: REQUESTS 1,200,000 req | p50: 12ms p95: 45ms p99: 78ms | err: 0.2% (2,400 errors) | success: 99.8%\n- /storage/node/utilization: REQUESTS 950,000 req | p50: 8ms p95: 30ms p99: 55ms | err: 0.1% (950 errors) | success: 99.9%\n- /storage/rebalance/status: REQUESTS 150,000 req | p50: 20ms p95: 60ms p99: 90ms | err: 0.3% (450 errors) | success: 99.7%\n- /storage/compaction/run: REQUESTS 300,000 req | p50: 15ms p95: 50ms p99: 85ms | err: 0.2% (600 errors) | success: 99.8%\n- /storage/io/metrics: REQUESTS 500,000 req | p50: 10ms p95: 40ms p99: 70ms | err: 0.1% (500 errors) | success: 99.9%\n- /storage/metadata/db: REQUESTS 400,000 req | p50: 9ms p95: 35ms p99: 65ms | err: 0.1% (400 errors) | success: 99.9%\n- /storage/quota/enforce: REQUESTS 100,000 req | p50: 7ms p95: 25ms p99: 45ms | err: 0.2% (200 errors) | success: 99.8%\n- /storage/ingest: REQUESTS 250,000 req | p50: 11ms p95: 42ms p99: 75ms | err: 0.2% (500 errors) | success: 99.8%\n- /storage/backup/verify: REQUESTS 50,000 req | p50: 38ms p95: 60ms p99: 85ms | err: 0.0% (0 errors) | success: 100%\n- /storage/reliability/status: REQUESTS 80,000 req | p50: 10ms p95: 30ms p99: 55ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-11: CPU 87% | Mem 86% | Disk Used 98% | Conns: 1200 | Net: 9.8/8.6 Gbps\n- stor-b-08: CPU 69% | Mem 76% | Disk Used 96% | Conns: 950 | Net: 7.2/6.6 Gbps\n- stor-c-04: CPU 62% | Mem 72% | Disk Used 93% | Conns: 800 | Net: 6.2/5.6 Gbps\n- meta-01: CPU 71% | Mem 92% | Disk Used 86% | Conns: 600 | Net: 3.8/3.2 Gbps\n\n### Connection Pools\n- storage-rebalance: active 11 | idle 4 | waiting 3 | exhaustion 0 | max 20 | avg_wait 15ms\n- storage-compaction: active 9 | idle 2 | waiting 1 | exhaustion 0 | max 15 | avg_wait 12ms\n- storage-io: active 12 | idle 3 | waiting 2 | exhaustion 0 | max 20 | avg_wait 10ms\n- storage-metadata: active 10 | idle 5 | waiting 2 | exhaustion 0 | max 15 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 88.2% | Bandwidth: 4.2 Gbps | Origin requests: 120,000\n\n### Alerts\n- [CRITICAL] scrub_errors on stor-a-11: 35\n- [WARNING] metadata backlog backlog: 4700\n- [WARNING] quota_hard_blocks: 4\n- [WARNING] network_drops: 24\n\n### Deployments & Changes\n- Weekly restore drill executed on metadata + 1 TB data sample; RTO 50 minutes, checksum validation passed.\n- Added extra metadata worker and enabled batching for namespace operations.\n- Increased scrub concurrency for affected placement groups.\n- Elevated metadata QPS with manifest backlog growth.\n- Enforced archival policy and migrated cold datasets to EC pool.\n\n### Events\n- Weekly restore drill executed on metadata + 1 TB data sample; RTO 50 minutes, checksum validation passed.\n- Scrub errors spiked to 35 during high tombstone churn; increased scrub concurrency only for affected placement groups and scheduled compaction priority.\n- Manifest backlog grew to 4.7k with elevated metadata QPS; added an extra metadata worker and enabled batching for namespace operations.\n- Quota hard blocks increased to 4 with large ingest; on-call coordinated archival policy enforcement and temporary EC pool migration for cold datasets.\n\n### On-Call\n- shift handoff note: Victor S. 7 pages. 13 tickets. Status: Degraded; metadata backlog and scrub/compaction pressure under heavy ingest. Restore drill successful; mitigation focused on batching, targeted scrubs, and tenant data lifecycle actions.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_026",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-09T10:00:00",
          "text": "## 2024-02-09 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 161500 | p50: 240ms p95: 650ms p99: 1240ms | err: 4.27% (6900 errors) | success: 95.73%\n- /fraud_check: REQUESTS 160900 | p50: 215ms p95: 570ms p99: 980ms | err: 1.31% (2100 errors) | success: 98.69%\n- /geo_lookup: REQUESTS 156800 | p50: 410ms p95: 780ms p99: 900ms | err: 0.57% (900 errors) | success: 99.43%\n- /auth: REQUESTS 209800 | p50: 55ms p95: 140ms p99: 265ms | err: 0.22% (460 errors) | success: 99.78%\n- /product_catalog: REQUESTS 282700 | p50: 72ms p95: 215ms p99: 400ms | err: 0.21% (580 errors) | success: 99.79%\n- /search: REQUESTS 247900 | p50: 98ms p95: 270ms p99: 490ms | err: 0.33% (820 errors) | success: 99.67%\n- /recommendations: REQUESTS 199900 | p50: 130ms p95: 370ms p99: 650ms | err: 0.41% (820 errors) | success: 99.59%\n\n### Infrastructure\n- gateway-01: CPU 82% | Mem 86% | Disk 63% | Conns: 8400 | Net: 790/960 Mbps\n- gateway-02: CPU 79% | Mem 84% | Disk 62% | Conns: 8200 | Net: 770/935 Mbps\n- service-b-01: CPU 90% | Mem 88% | Disk 62% | Conns: 9800 | Net: 980/1150 Mbps\n- metrics-db-01: CPU 58% | Mem 86% | Disk 89% | Conns: 520 | Net: 220/270 Mbps\n\n### Connection Pools\n- primary: active 260 | idle 0 | waiting 620 | exhaustion: 62 | max: 260 | avg_wait: 420ms\n- replica: active 100 | idle 0 | waiting 160 | exhaustion: 24 | max: 100 | avg_wait: 140ms\n- third_party_geo: active 60 | idle 0 | waiting 410 | exhaustion: 72 | max: 60 | avg_wait: 560ms\n\n### CDN & Caching\n- Hit rate: 91.3% | Bandwidth: 10.4 Gbps | Origin requests: 560900\n\n### Alerts\n- [CRITICAL] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=900\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=primary events=62 waiting=620\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=860 retry_rate_pct=0.67\n\n### Deployments & Changes\n- Config change: third_party_geo pool max_size set to 120\n\n### Events\n- Increased third_party_geo pool max_size from 60 to 120\n- A/B test 'checkout-v2' at 15% rollout\n- Enabled geo response caching in service-B for 10 minutes\n- Added new synthetic probe for geo vendor from eu-west-1\n\n### On-Call\n- Shift: R. Alvarez. 22 pages. 8 tickets (POOL-661, VEND-130, SRV-B-301 cache, CO-900, GW-425, SYN-52, NET-402, OBS-255). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_078",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-09T10:30:00",
          "text": "## 2026-01-19 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster/usage: REQUESTS 12000 req | p50: 45ms p95: 120ms p99: 250ms | err: 0.2% (24 errors) | success: 99.8%\n- /storage/node/utilization: REQUESTS 9500 req | p50: 30ms p95: 80ms p99: 150ms | err: 0.1% (10 errors) | success: 99.9%\n- /storage/rebalance/status: REQUESTS 3000 req | p50: 20ms p95: 50ms p99: 100ms | err: 0.3% (9 errors) | success: 99.7%\n- /storage/compaction/trigger: REQUESTS 8000 req | p50: 25ms p95: 70ms p99: 140ms | err: 0.2% (16 errors) | success: 99.8%\n- /storage/io/metrics: REQUESTS 15000 req | p50: 35ms p95: 90ms p99: 200ms | err: 0.4% (60 errors) | success: 99.6%\n- /storage/metadata/db: REQUESTS 20000 req | p50: 40ms p95: 100ms p99: 220ms | err: 0.1% (20 errors) | success: 99.9%\n- /storage/quota/status: REQUESTS 5000 req | p50: 22ms p95: 60ms p99: 130ms | err: 0.2% (10 errors) | success: 99.8%\n- /storage/ingest: REQUESTS 7000 req | p50: 28ms p95: 75ms p99: 160ms | err: 0.3% (21 errors) | success: 99.7%\n- /storage/reliability/status: REQUESTS 4000 req | p50: 33ms p95: 85ms p99: 180ms | err: 0.2% (8 errors) | success: 99.8%\n- /storage/backup/verify: REQUESTS 6000 req | p50: 38ms p95: 95ms p99: 210ms | err: 0.1% (6 errors) | success: 99.9%\n\n### Infrastructure\n- stor-a-11: CPU 75% | Mem 83% | Disk Used 97% | Conns: 1200 | Net: 7.9/7.2 Mbps\n- stor-b-08: CPU 64% | Mem 74% | Disk Used 96% | Conns: 950 | Net: 6.6/6.1 Mbps\n- stor-c-17: CPU 60% | Mem 71% | Disk Used 94% | Conns: 880 | Net: 6.0/5.5 Mbps\n- meta-02: CPU 68% | Mem 91% | Disk Used 86% | Conns: 650 | Net: 3.5/3.1 Mbps\n\n### Connection Pools\n- pool-storage: active 150 | idle 30 | waiting 5 | exhaustion 0 | max 200 | avg_wait 12ms\n- pool-metadata: active 50 | idle 20 | waiting 2 | exhaustion 0 | max 80 | avg_wait 8ms\n- pool-rebalance: active 10 | idle 5 | waiting 1 | exhaustion 0 | max 20 | avg_wait 15ms\n\n### CDN & Caching\n- Hit rate: 89.4% | Bandwidth: 2.5 Gbps | Origin requests: 15000\n\n### Alerts\n- [CRITICAL] disk_failure on stor-c-17: disk replaced, rebuild completed in 6.5 hours\n- [WARNING] quota_hard_blocks on tenants: 3\n- [INFO] manifest_backlog reduced from 4.7k to 3.3k\n- [INFO] cache hit rate improved to 89.4%\n- [INFO] rebalance jobs completed: 12\n- [INFO] rebalance jobs started: 10\n- [INFO] disk failure on stor-c-17 replaced; rebuild duration: 6.5 hours\n- [INFO] quota governance actions reduced hard blocks from 4 to 3; two tenants migrated to EC pool\n\n### Deployments & Changes\n- Added extra metadata worker and batching reduced manifest backlog from 4.7k to 3.3k; cache hit improved to 89.4%\n- Completed 12 rebalance jobs, cooling hot partitions and improving skew index to 1.06\n- Disk failure on stor-c-17 replaced; rebuild completed in 6.5 hours with brief increase in disk busy but no data unavailability\n- Quota governance actions reduced hard blocks from 4 to 3; confirmed two tenants completed archival migration to EC pool\n\n### Events\n- Added extra metadata worker and batching reduced manifest backlog from 4.7k to 3.3k; cache hit improved to 89.4%\n- Completed 12 rebalance jobs, cooling hot partitions and improving skew index to 1.06\n- Disk failure on stor-c-17 replaced; rebuild completed in 6.5 hours with brief increase in disk busy but no data unavailability\n- Quota governance actions reduced hard blocks from 4 to 3; confirmed two tenants completed archival migration to EC pool\n\n### On-Call\n- Shift: Kiran D. 4 pages. 8 tickets. Status: Stable; backlog improved, rebalance effective, and disk replacement completed. Utilization remains high with continued focus on tenant lifecycle policies.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_079",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-09T10:30:00",
          "text": "## 2026-01-20 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 120ms p95: 250ms p99: 320ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/node_utilization: REQUESTS req | p50: 80ms p95: 210ms p99: 290ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/rebalance: REQUESTS req | p50: 100ms p95: 220ms p99: 310ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/compaction: REQUESTS req | p50: 90ms p95: 200ms p99: 280ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/io: REQUESTS req | p50: 85ms p95: 210ms p99: 290ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/storage_efficiency: REQUESTS req | p50: 75ms p95: 190ms p99: 270ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/backup_verification: REQUESTS req | p50: 70ms p95: 180ms p99: 260ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/quota_enforcement: REQUESTS req | p50: 65ms p95: 170ms p99: 250ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/ingest: REQUESTS req | p50: 60ms p95: 160ms p99: 240ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/reliability: REQUESTS req | p50: 55ms p95: 150ms p99: 230ms | err: 0.0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 50ms p95: 140ms p99: 220ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/maintenance: REQUESTS req | p50: 45ms p95: 130ms p99: 210ms | err: 0.1% (1 error) | success: 99.9%\n\n### Infrastructure\n- stor-a-20: CPU 90% | Mem 87% | Disk 98% | Conns: 1200 | Net: 10.2/8.9 Mbps\n- stor-b-13: CPU 72% | Mem 77% | Disk 96% | Conns: 950 | Net: 7.6/6.9 Mbps\n- stor-c-01: CPU 65% | Mem 73% | Disk 94% | Conns: 800 | Net: 6.4/5.8 Mbps\n- meta-03: CPU 74% | Mem 93% | Disk 87% | Conns: 600 | Net: 4.0/3.4 Mbps\n\n### Connection Pools\n- pool-storage: active 78 | idle 22 | waiting 5 | exhaustion 2 | max 100 | avg_wait 15ms\n- pool-metadata: active 45 | idle 55 | waiting 3 | exhaustion 1 | max 60 | avg_wait 10ms\n- pool-rebalance: active 12 | idle 8 | waiting 4 | exhaustion 0 | max 20 | avg_wait 20ms\n- pool-compaction: active 30 | idle 70 | waiting 2 | exhaustion 0 | max 100 | avg_wait 12ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.5 Gbps | Origin requests: 12,400\n\n### Alerts\n- [CRITICAL] high disk usage on stor-a-20: 98%\n- [WARNING] high node utilization on stor-a-20: 98.9%\n- [WARNING] high node utilization on stor-b-13: 96%\n- [WARNING] high node utilization on stor-c-01: 94%\n- [WARNING] high node utilization on meta-03: 87%\n- [CRITICAL] metadata backlog: 5200 entries\n- [WARNING] scrub errors: 38\n- [WARNING] quota soft limit hits: 31\n- [CRITICAL] quota hard blocks: 5\n- [WARNING] tenant over quota: 6 tenants\n- [WARNING] metadata QPS: 78,000 p95\n- [WARNING] network drops: 27\n- [WARNING] garbage collection: 7.9%\n- [WARNING] tombstone percentage: 4.8%\n- [WARNING] replication factor: 3\n- [WARNING] erasure coding: 30%\n- [WARNING] compression ratio: 1.75\n- [WARNING] dedupe ratio: 1.04\n- [WARNING] storage utilization: 86.4%\n- [WARNING] active rebalance jobs: 12\n- [WARNING] compactions run: 2408\n- [WARNING] pending compactions: 118\n- [WARNING] scrub errors: 38\n- [WARNING] metadata cache hit: 88%\n- [WARNING] garbage percentage: 7.9%\n- [WARNING] tombstone percentage: 4.8%\n- [WARNING] metadata backlog: 5200\n- [WARNING] snapshot verification failures: 1\n\n### Deployments & Changes\n- Enforced stricter per-tenant ingest caps for non-production namespaces due to high ingest (+232 TB)\n- Added temporary metadata throttling for bulk namespace operations\n- Increased verifier concurrency following snapshot verification failure\n- No firmware updates or kernel patches applied today\n- No rack power cycles or network drops other than 27 noted\n- Audit log size: 98 GB\n\n### Events\n- Heavy ingest (+232 TB) pushed utilization to 86% and increased quota hard blocks to 5; enforced stricter per-tenant ingest caps for non-production namespaces\n- Scrub errors rose to 38; prioritized deep scrubs on placement groups with recent high churn and scheduled follow-up replica comparisons\n- Manifest backlog climbed to 5.2k alongside metadata QPS; added temporary metadata throttling for bulk namespace operations\n- Snapshot verification had one failure due to verifier queue backlog; increased verifier concurrency and re-verified successfully\n\n### On-Call\n- Shift: Paige R. 10 pages. 18 tickets. Status: Degraded; utilization and tenant growth are driving repeated quota and metadata backlogs. Immediate controls: ingest caps, metadata throttling, and targeted scrubs",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_080",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-09T10:30:00",
          "text": "## 2026-01-21 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-20: CPU 78% | Mem 85% | Disk Used 98% | Disk Busy 91% | Conns: N | Net: 8.3/7.4 Mbps\n- stor-b-13: CPU 67% | Mem 76% | Disk Used 96% | Disk Busy 89% | Conns: N | Net: 6.9/6.5 Mbps\n- stor-c-12: CPU 61% | Mem 72% | Disk Used 95% | Disk Busy 88% | Conns: N | Net: 6.3/5.9 Mbps\n- meta-01: CPU 70% | Mem 92% | Disk Used 87% | Disk Busy 73% | Conns: N | Net: 3.7/3.3 Mbps\n\n### Connection Pools\n- pool-main: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-backup: active 45 | idle 20 | waiting 3 | exhaustion: 1 | max: 70 | avg_wait: 8ms\n- pool-io: active 60 | idle 15 | waiting 4 | exhaustion: 0 | max: 80 | avg_wait: 10ms\n\n### CDN & Caching\n- Hit rate: 89.1% | Bandwidth: 4.2 Gbps | Origin requests: 12,340\n\n### Alerts\n- [CRITICAL] disk failure on stor-c-12: replaced and rebuilt in 7.2 hours\n- [WARNING] metadata backlog: 3,600 backlog items\n- [INFO] cache hit rate improved above 89%\n- [WARNING] quota soft limit hits: 28\n- [CRITICAL] disk failures: 1\n- [INFO] scrub errors: 22\n- [WARNING] nodes over 80% utilization: 79\n- [CRITICAL] nodes over 90% utilization: 32\n- [CRITICAL] hottest node utilization: 98.5%\n- [INFO] metadata database size: 1210 GB\n- [INFO] manifest backlog: 3,600\n- [INFO] total tenants: 91\n- [WARNING] tenants over quota: 5\n- [WARNING] quota soft limit hits: 28\n- [CRITICAL] disk rebuilds completed: 1\n- [INFO] last restore RTO: 50 min\n- [INFO] network drops: 16\n- [INFO] audit log size: 85 GB\n\n### Deployments & Changes\n- Completed 13 rebalance jobs, reducing skew index to 1.05 and lowering scrub pressure by spreading hot partitions\n- Disk failure on stor-c-12 replaced and rebuilt in 7.2 hours; monitored for under-replicated objects (none sustained)\n- Metadata throttling and batching reduced manifest backlog from 5.2k to 3.6k; cache hit improved above 89%\n- Quota hard blocks reduced from 5 to 4 after applying ingest caps; processed six quota increase requests with capacity committee review\n\n### Events\n- Completed 13 rebalance jobs, reducing skew index to 1.05 and lowering scrub pressure by spreading hot partitions\n- Disk failure on stor-c-12 replaced and rebuilt in 7.2 hours; watched for any under-replicated objects (none sustained)\n- Metadata throttling and batching reduced manifest backlog from 5.2k to 3.6k; cache hit improved above 89%\n- Quota hard blocks reduced from 5 to 4 after applying ingest caps; processed six quota increase requests with capacity committee review\n\n### On-Call\n- Shift: Ivan B. 5 pages. 10 tickets. Status: Stable; recovery day with improved balance and metadata backlog reduction. Capacity remains constrained; approvals for quota increases are now gated on lifecycle plans.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_027",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-10T10:00:00",
          "text": "## 2024-02-10 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 159300 | p50: 225ms p95: 610ms p99: 1180ms | err: 3.83% (6100 errors) | success: 96.17%\n- /fraud_check: REQUESTS 158800 | p50: 200ms p95: 540ms p99: 920ms | err: 1.13% (1800 errors) | success: 98.87%\n- /geo_lookup: REQUESTS 154900 | p50: 390ms p95: 750ms p99: 880ms | err: 0.53% (820 errors) | success: 99.47%\n- /auth: REQUESTS 207100 | p50: 53ms p95: 136ms p99: 258ms | err: 0.2% (420 errors) | success: 99.8%\n- /product_catalog: REQUESTS 279200 | p50: 70ms p95: 208ms p99: 390ms | err: 0.19% (520 errors) | success: 99.81%\n- /search: REQUESTS 244800 | p50: 94ms p95: 260ms p99: 475ms | err: 0.3% (740 errors) | success: 99.7%\n- /recommendations: REQUESTS 197500 | p50: 122ms p95: 350ms p99: 620ms | err: 0.37% (740 errors) | success: 99.63%\n\n### Infrastructure\n- gateway-01: CPU 78% | Mem 84% | Disk 63% | Conns: 7800 | Net: 760/925 Mbps\n- gateway-02: CPU 75% | Mem 82% | Disk 62% | Conns: 7605 | Net: 742/900 Mbps\n- service-b-01: CPU 84% | Mem 86% | Disk 62% | Conns: 8600 | Net: 860/1010 Mbps\n- metrics-db-01: CPU 54% | Mem 84% | Disk 89% | Conns: 495 | Net: 205/252 Mbps\n\n### Connection Pools\n- primary: active 238 | idle 22 | waiting 410 | exhaustion: 48 | max: 260 | avg_wait: 290ms\n- replica: active 98 | idle 2 | waiting 95 | exhaustion: 16 | max: 100 | avg_wait: 88ms\n- third_party_geo: active 115 | idle 5 | waiting 220 | exhaustion: 40 | max: 120 | avg_wait: 310ms\n\n### CDN & Caching\n- Hit rate: 91.4% | Bandwidth: 10.0 Gbps | Origin requests: 528400\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=third_party_geo events=40 waiting=220\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-01: error_pct=3.83\n\n### Deployments & Changes\n- Deployed service-B (fraud) v2.7.7 (geo cache)\n\n### Events\n- Service-B deploy completed\n- A/B test 'checkout-v2' at 15% rollout\n- Feature flag 'skip-geo-on-timeout' set to 70% of traffic\n- Updated gateway retry policy for upstream geo to 0 retries\n\n### On-Call\n- Shift: D. Kim. 17 pages. 6 tickets (SRV-B-312 deploy, CO-913, POOL-674, GW-438, FF-116, VEND-138). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_081",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-10T10:30:00",
          "text": "## 2026-01-22 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 80000 p95: 80000 p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-05: CPU 92% | Mem 88% | Disk Used 99% | Disk Busy 99% | Conns: N/A | Net: 10.7/9.2 Gbps\n- stor-b-17: CPU 74% | Mem 78% | Disk Used 97% | Disk Busy 94% | Conns: N/A | Net: 7.9/7.1 Gbps\n- stor-c-08: CPU 67% | Mem 74% | Disk Used 95% | Disk Busy 89% | Conns: N/A | Net: 6.8/6.1 Gbps\n- meta-02: CPU 77% | Mem 94% | Disk Used 88% | Disk Busy 78% | Conns: N/A | Net: 4.2/3.6 Gbps\n\n### Connection Pools\n- pool-01: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 150 | avg_wait 12ms\n- pool-02: active 85 | idle 20 | waiting 3 | exhaustion 1 | max 100 | avg_wait 8ms\n- pool-03: active 60 | idle 15 | waiting 2 | exhaustion 0 | max 80 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 87.8% | Bandwidth: 4.2 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [HIGH] compaction pending on cluster: 122\n- [WARNING] scrub errors on cluster: 41\n- [CRITICAL] quota hard blocks on tenants: 5\n- [WARNING] metadata backlog: 5,600\n- [INFO] network drops: 29\n\n### Deployments & Changes\n- Initiated overnight compaction surge window and deep scrub targeting.\n- Enabled stricter batching and deferred non-essential namespace scans.\n- Enforced tenant cleanup commitments to address quota overages.\n- Throttled rebalance bandwidth during peak write periods.\n\n### Events\n- High ingest (+226 TB) drove compaction pending to 122 and scrub errors to 41; initiated overnight compaction surge window and deep scrub targeting.\n- Metadata cache hit fell to 87.8% with manifest backlog 5.6k; enabled stricter batching and deferred non-essential namespace scans.\n- Quota hard blocks increased to 5; required top three tenants to execute 24-hour cleanup/archival plan before any quota relief.\n- Rebalance continued to distribute hot partitions, but disk busy p95 hit 93.9%; throttled rebalance bandwidth during peak writes.\n\n### On-Call\n- shift handoff note: Samira Q. 11 pages. 19 tickets. Status: Degraded; combined ingest, compaction, and metadata pressure. Actions: surge windows, deferrals, throttling, and enforced tenant cleanup commitments.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_082",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-10T10:30:00",
          "text": "## 2026-01-23 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-05: CPU 79% | Mem 86% | Disk 98% | Conns: 120 | Net: 8.5/7.6 Mbps\n- stor-b-17: CPU 68% | Mem 77% | Disk 97% | Conns: 120 | Net: 7.0/6.6 Mbps\n- stor-c-14: CPU 62% | Mem 73% | Disk 96% | Conns: 120 | Net: 6.7/6.0 Mbps\n- meta-03: CPU 72% | Mem 93% | Disk 88% | Conns: 120 | Net: 3.9/3.5 Mbps\n\n### Connection Pools\n- pool-storage: active 82 | idle 38 | waiting 5 | exhaustion 0 | max 120 | avg_wait 12ms\n- pool-meta: active 34 | idle 16 | waiting 2 | exhaustion 0 | max 50 | avg_wait 8ms\n\n### CDN & Caching\n- Hit rate: 88.9% | Bandwidth: 4.0 Gbps | Origin requests: 1,200,000\n\n### Alerts\n- [CRITICAL] disk_failure on stor-c-14: disk replaced and rebuilt in 6.3 hours\n- [WARNING] scrub_errors: 24\n- [INFO] snapshot_verification: 1 failure, 47 verified\n- [INFO] tenant_quota: 5 tenants over quota, 30 soft limit hits, 4 hard blocks\n- [INFO] network_drops: 18\n\n### Deployments & Changes\n- No firmware updates or kernel patches performed\n- Network drops recorded: 18 incidents\n- Tenant cleanup: quota hard blocks reduced from 5 to 4; ongoing ingest caps for non-production namespaces\n\n### Events\n- Overnight compaction surge reduced pending from 122 to 98; disk busy p95 improved to 91.5%; scrub errors dropped to 24\n- Disk failure on stor-c-14 replaced and rebuilt in 6.3 hours; ensured rebalancer avoided overloading rebuilding node\n- Snapshot verification had one failure due to verifier worker eviction; pinned workers to dedicated hosts and re-verified successfully\n- Tenant cleanup commitments lowered quota hard blocks from 5 to 4; continued ingest caps for non-production namespaces\n\n### On-Call\n- shift handoff note: Shift: Colton E. 6 pages. 12 tickets. Status: Stable; surge window helped. Still high utilization, but backlogs are trending down with ongoing tenant cleanup and improved verifier isolation",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_083",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-10T10:30:00",
          "text": "## 2026-01-24 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: 0ms p95: 0ms p99: 0ms | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-16: CPU 94% | Mem 89% | Disk Used 99% | Disk Busy 99% | Conns: N/A | Net: In 11.0 Gbps / Out 9.6 Gbps\n- stor-b-04: CPU 76% | Mem 79% | Disk Used 97% | Disk Busy 95% | Conns: N/A | Net: In 8.1 Gbps / Out 7.4 Gbps\n- stor-c-23: CPU 68% | Mem 75% | Disk Used 96% | Disk Busy 90% | Conns: N/A | Net: In 6.9 Gbps / Out 6.2 Gbps\n- meta-01: CPU 79% | Mem 95% | Disk Used 89% | Disk Busy 80% | Conns: N/A | Net: In 4.5 Gbps / Out 3.8 Gbps\n\n### Connection Pools\n- pool-1: active 120 | idle 30 | waiting 5 | exhaustion: 2 | max: 150 | avg_wait: 12ms\n- pool-2: active 85 | idle 20 | waiting 3 | exhaustion: 1 | max: 100 | avg_wait: 8ms\n- pool-3: active 60 | idle 15 | waiting 2 | exhaustion: 0 | max: 80 | avg_wait: 10ms\n\n### CDN & Caching\n- Hit rate: 87.5% | Bandwidth: 2.5 Gbps | Origin requests: 12,000\n\n### Alerts\n- [SEVERE] quota_hard_blocks on hostname: 6\n- [WARNING] storage_disk_busy on stor-a-16: 99%\n- [CRITICAL] scrub_errors on storage: 44\n- [WARNING] metadata_manifest_backlog: 6100\n- [SEVERE] storage_capacity_utilization: 89.1%\n- [WARNING] node_utilization_max: 98.8%\n- [WARNING] nodes_over_80pct: 84\n- [CRITICAL] nodes_over_90pct: 35\n\n### Deployments & Changes\n- No deployments or changes today.\n\n### Events\n- Another heavy ingest day (+238 TB) pushed overall utilization above 89% and drove quota hard blocks to 6; enforced immediate freeze on new tenant onboarding.\n- Compaction pending jumped to 130 with disk busy p95 94.6%; scheduled extended overnight compaction and reduced rebalance bandwidth during peak.\n- Scrub errors rose to 44; identified top churn namespaces and requested workload changes (lower TTL churn, batch deletes) to reduce tombstone load.\n- Metadata manifest backlog climbed to 6.1k; temporarily disabled non-critical namespace inventory jobs and increased metadata write buffers.\n\n### On-Call\n- Shift: Nisha P. 12 pages. 21 tickets. Status: Degraded; capacity constraints and high churn workloads are stressing compaction/scrubs and triggering quotas. Mitigations: onboarding freeze, workload adjustments, overnight compaction, metadata job deferrals.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_028",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-11T10:00:00",
          "text": "## 2024-02-11 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 156400 | p50: 210ms p95: 580ms p99: 1120ms | err: 3.32% (5200 errors) | success: 96.68%\n- /fraud_check: REQUESTS 155900 | p50: 185ms p95: 500ms p99: 860ms | err: 0.96% (1500 errors) | success: 99.04%\n- /geo_lookup: REQUESTS 151600 | p50: 360ms p95: 710ms p99: 860ms | err: 0.46% (700 errors) | success: 99.54%\n- /auth: REQUESTS 203800 | p50: 50ms p95: 130ms p99: 245ms | err: 0.18% (360 errors) | success: 99.82%\n- /product_catalog: REQUESTS 274600 | p50: 66ms p95: 198ms p99: 365ms | err: 0.17% (460 errors) | success: 99.83%\n- /search: REQUESTS 240900 | p50: 88ms p95: 245ms p99: 440ms | err: 0.26% (620 errors) | success: 99.74%\n- /recommendations: REQUESTS 194600 | p50: 112ms p95: 320ms p99: 560ms | err: 0.31% (610 errors) | success: 99.69%\n\n### Infrastructure\n- gateway-01: CPU 74% | Mem 82% | Disk 63% | Conns: 7050 | Net: 730/890 Mbps\n- gateway-02: CPU 71% | Mem 80% | Disk 62% | Conns: 6880 | Net: 712/865 Mbps\n- service-b-01: CPU 78% | Mem 83% | Disk 61% | Conns: 7400 | Net: 760/890 Mbps\n- metrics-db-01: CPU 50% | Mem 82% | Disk 89% | Conns: 470 | Net: 192/240 Mbps\n\n### Connection Pools\n- primary: active 220 | idle 40 | waiting 260 | exhaustion: 35 | max: 260 | avg_wait: 185ms\n- replica: active 94 | idle 6 | waiting 60 | exhaustion: 12 | max: 100 | avg_wait: 55ms\n- third_party_geo: active 100 | idle 20 | waiting 150 | exhaustion: 32 | max: 120 | avg_wait: 210ms\n\n### CDN & Caching\n- Hit rate: 91.6% | Bandwidth: 9.5 Gbps | Origin requests: 496000\n\n### Alerts\n- [CRITICAL] UPSTREAM-LATENCY on gateway-02: geo_lookup_p99_ms=860\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=830 retry_rate_pct=0.66\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Feature flag 'skip-geo-on-timeout' set to 85% of traffic\n- A/B test 'checkout-v2' at 15% rollout\n- Updated service-B cache size from 50k to 120k entries\n- Removed gateway per-IP rate limit for /checkout\n\n### On-Call\n- Shift: J. Martinez. 14 pages. 4 tickets (FF-121, SRV-B-320 cache size, VEND-145, CO-928). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_084",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-11T10:30:00",
          "text": "## 2026-01-25 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster/usage: REQUESTS req | p50: 12ms p95: 45ms p99: 78ms | err: 0.2% (5 errors) | success: 99.8%\n- /storage/node/utilization: REQUESTS req | p50: 8ms p95: 22ms p99: 35ms | err: 0.1% (3 errors) | success: 99.9%\n- /storage/rebalance/status: REQUESTS req | p50: 10ms p95: 30ms p99: 55ms | err: 0.3% (7 errors) | success: 99.7%\n- /storage/compaction/run: REQUESTS req | p50: 9ms p95: 25ms p99: 50ms | err: 0.2% (4 errors) | success: 99.8%\n- /storage/io/metrics: REQUESTS req | p50: 11ms p95: 40ms p99: 70ms | err: 0.1% (2 errors) | success: 99.9%\n- /storage/metadata/db: REQUESTS req | p50: 7ms p95: 20ms p99: 33ms | err: 0.1% (2 errors) | success: 99.9%\n- /storage/quota/status: REQUESTS req | p50: 6ms p95: 18ms p99: 29ms | err: 0.1% (2 errors) | success: 99.9%\n- /storage/ingest/metrics: REQUESTS req | p50: 8ms p95: 24ms p99: 40ms | err: 0.2% (4 errors) | success: 99.8%\n- /storage/reliability/status: REQUESTS req | p50: 10ms p95: 35ms p99: 60ms | err: 0.3% (6 errors) | success: 99.7%\n- /storage/backup/verify: REQUESTS req | p50: 9ms p95: 28ms p99: 48ms | err: 0.1% (2 errors) | success: 99.9%\n\n### Infrastructure\n- stor-a-16: CPU 80% | Mem 86% | Disk 98% | Conns: 120 | Net: 8.7/7.9 Mbps\n- stor-b-04: CPU 70% | Mem 78% | Disk 97% | Conns: 115 | Net: 7.3/6.9 Mbps\n- stor-c-23: CPU 63% | Mem 74% | Disk 96% | Conns: 102 | Net: 6.3/6.1 Mbps\n- meta-02: CPU 75% | Mem 94% | Disk 89% | Conns: 98 | Net: 4.1/3.7 Mbps\n\n### Connection Pools\n- pool-storage: active 85 | idle 10 | waiting 5 | exhaustion 0 | max 100 | avg_wait 12ms\n- pool-db: active 40 | idle 8 | waiting 2 | exhaustion 0 | max 50 | avg_wait 9ms\n- pool-api: active 25 | idle 5 | waiting 1 | exhaustion 0 | max 30 | avg_wait 7ms\n- pool-cache: active 15 | idle 3 | waiting 0 | exhaustion 0 | max 20 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.2 Gbps | Origin requests: 1,250,000\n\n### Alerts\n- [CRITICAL] disk_failure on stor-b-04: disk rebuild completed in 6.8 hours\n- [WARNING] node_unhealthy on stor-c-23: 3 unhealthy OSDs\n- [INFO] network_drops on meta-02: 19 drops\n- [INFO] scrub_errors reduced from 44 to 28\n- [INFO] pending_compactions decreased from 130 to 104\n- [INFO] disk busy eased to 92.2%\n- [INFO] restore drill executed for tenant namespace; RTO 48 min\n- [INFO] rebalance completed 14 jobs; skew index 1.03; hottest node 98.2%\n- [INFO] overnight compaction run; 2776 compactions; 181.2 CPU hours\n- [INFO] metadata database size 1267 GB; QPS 78,000; cache hit 88.6%\n- [INFO] backup verification: 48 snapshots taken and verified; 0 failures; avg verify 39 min\n- [INFO] tenant quota: 6 over quota; 33 soft limit hits; 5 hard blocks; largest tenant 513 TB\n- [INFO] ingestion: 146 TB in; 145 TB egress; 53M objects; avg object size 92 KB; 9 hot partitions\n- [INFO] storage efficiency: replication factor 3; erasure coding 33%; compression ratio 1.74; dedupe ratio 1.02; garbage 7.7%; tombstones 4.9%\n- [INFO] reliability: 1 disk failure; 1 disk rebuild; 6.8 hr rebuild; 1 node restart; 3 unhealthy OSDs; 28 scrub errors\n- [INFO] maintenance: no firmware updates or kernel patches; network drops 19; audit log 88 GB\n\n### Deployments & Changes\n- Extended overnight compaction reduced pending from 130 to 104; scrub errors fell from 44 to 28; disk busy eased to 92.2%\n- Restore drill executed for regulated tenant namespace; achieved 48-minute RTO and validated object counts against manifests\n- Rebalance completed 14 jobs, bringing skew index to 1.03 and reducing hottest-node utilization to 98.2%\n- Disk failure on a mid-tier node replaced; rebuild completed in 6.8 hours and unhealthy OSD count remained at 3\n\n### Events\n- Extended overnight compaction reduced pending from 130 to 104; scrub errors fell from 44 to 28 and disk busy eased to 92.2%\n- Restore drill executed for regulated tenant namespace; achieved 48-minute RTO and validated object counts against manifests\n- Rebalance completed 14 jobs, bringing skew index to 1.03 and reducing hottest-node utilization to 98.2%\n- Disk failure on a mid-tier node replaced; rebuild completed in 6.8 hours and unhealthy OSD count remained at 3\n\n### On-Call\n- Shift: Elliot Z. 6 pages. 12 tickets. Status: Stable; improvements after compaction surge and rebalance. Still near 90% utilization; onboarding freeze remains, and tenant churn reduction actions are tracked",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_085",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-11T10:30:00",
          "text": "## 2026-01-26 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-01: CPU 96% | Mem 90% | Disk Used 99% | Disk Busy 99% | Conns: N/A | Net: In 11.4 Gbps / Out 10.0 Gbps\n- stor-b-21: CPU 78% | Mem 81% | Disk Used 98% | Disk Busy 96% | Conns: N/A | Net: In 8.5 Gbps / Out 7.8 Gbps\n- stor-c-24: CPU 70% | Mem 76% | Disk Used 96% | Disk Busy 91% | Conns: N/A | Net: In 7.1 Gbps / Out 6.4 Gbps\n- meta-03: CPU 82% | Mem 95% | Disk Used 90% | Disk Busy 82% | Conns: N/A | Net: In 4.8 Gbps / Out 4.0 Gbps\n\n### Connection Pools\n- pool-1: active 120 | idle 30 | waiting 5 | exhaustion 2 | max 150 | avg_wait 12ms\n- pool-2: active 85 | idle 20 | waiting 3 | exhaustion 1 | max 100 | avg_wait 8ms\n- pool-3: active 60 | idle 15 | waiting 2 | exhaustion 0 | max 80 | avg_wait 5ms\n\n### CDN & Caching\n- Hit rate: 87.1% | Bandwidth: 2.5 Gbps | Origin requests: 134,000\n\n### Alerts\n- [CRITICAL] utilization crossed 90%; enacted emergency policy on cluster\n- [WARNING] quota hard blocks rose to 7 with +244 TB ingest\n- [INFO] compaction pending climbed to 138; scrub errors at 47\n- [NOTICE] snapshot verification had one failure due to verifier backlog\n\n### Deployments & Changes\n- Enacted emergency quota policy: no quota increases without approved deletion plan, tightened burst allowances\n- Coordinated tenant backfill pause and cold data shift to EC pool\n- Scheduled two-stage compaction surge: overnight and midday\n- Increased monitoring cadence for compaction and scrub errors\n- Doubled verifier workers for snapshot verification; re-verified within the hour\n\n### Events\n- Utilization crossed 90%; enacted emergency policy: no quota increases without approved deletion plan and tightened burst allowances\n- Quota hard blocks rose to 7 with +244 TB ingest; coordinated with tenant owners to pause non-critical backfills and shift cold data to EC pool\n- Compaction pending climbed to 138 and scrub errors to 47; scheduled two-stage compaction surge (overnight + midday) and increased monitoring cadence\n- Snapshot verification had one failure due to verifier backlog; temporarily doubled verifier workers and re-verified within the hour\n\n### On-Call\n- Shift: Jordan S. 13 pages. 23 tickets. Status: Degraded; cluster operating in emergency capacity mode (>90%). Focus is on strict quota governance, aggressive compaction scheduling, and controlling tenant ingest/backfill behavior",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_086",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-11T10:30:00",
          "text": "## 2026-01-27 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster/usage: REQUESTS req | p50: 120ms p95: 250ms p99: 400ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/node/utilization: REQUESTS req | p50: 80ms p95: 180ms p99: 300ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/rebalance/status: REQUESTS req | p50: 60ms p95: 130ms p99: 220ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/compaction/run: REQUESTS req | p50: 70ms p95: 160ms p99: 290ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/io/metrics: REQUESTS req | p50: 90ms p95: 200ms p99: 350ms | err: 0.4% (4 errors) | success: 99.6%\n- /storage/metadata/db: REQUESTS req | p50: 85ms p95: 190ms p99: 330ms | err: 0.1% (1 error) | success: 99.9%\n- /storage/quota/status: REQUESTS req | p50: 75ms p95: 170ms p99: 310ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/ingest/metrics: REQUESTS req | p50: 65ms p95: 150ms p99: 280ms | err: 0.3% (3 errors) | success: 99.7%\n- /storage/reliability/status: REQUESTS req | p50: 95ms p95: 210ms p99: 370ms | err: 0.2% (2 errors) | success: 99.8%\n- /storage/metadata/cache: REQUESTS req | p50: 80ms p95: 180ms p99: 320ms | err: 0.1% (1 error) | success: 99.9%\n\n### Infrastructure\n- stor-a-01: CPU 82% | Mem 87% | Disk Used 99% | Conns: 1200 | Net: 9.0/8.2 Mbps\n- stor-b-21: CPU 72% | Mem 80% | Disk Used 98% | Conns: 950 | Net: 7.6/7.1 Mbps\n- stor-c-24: CPU 66% | Mem 75% | Disk Used 96% | Conns: 820 | Net: 6.6/6.2 Mbps\n- meta-01: CPU 78% | Mem 95% | Disk Used 90% | Conns: 670 | Net: 4.4/3.9 Mbps\n\n### Connection Pools\n- pool-storage-requests: active 1500 | idle 300 | waiting 50 | exhaustion: 0 | max: 2000 | avg_wait: 12ms\n- pool-db-connections: active 200 | idle 50 | waiting 10 | exhaustion: 0 | max: 300 | avg_wait: 8ms\n- pool-cache-lookup: active 800 | idle 150 | waiting 20 | exhaustion: 0 | max: 1000 | avg_wait: 5ms\n- pool-rebalance: active 12 | idle 3 | waiting 2 | exhaustion: 0 | max: 20 | avg_wait: 15ms\n\n### CDN & Caching\n- Hit rate: 92% | Bandwidth: 3.5 Gbps | Origin requests: 12000\n\n### Alerts\n- [CRITICAL] disk_failure on stor-a-01: 1 failure\n- [WARNING] node_restart on stor-b-21: 1 restart\n- [WARNING] scrub_errors: 30 on storage system\n- [INFO] quota_hard_blocks: 6\n- [INFO] quota_soft_limit_hits: 38\n\n### Deployments & Changes\n- No firmware updates or kernel patches performed.\n- Network drops recorded: 20 incidents.\n- Scheduled maintenance included network adjustments and audit log collection of 90 GB.\n\n### Events\n- Two-stage compaction surge reduced pending from 138 to 110; scrub errors fell to 30 and fsync p95 improved to 15.8 ms.\n- Completed 15 rebalance jobs, improving skew index to 1.02 and keeping hottest node under 98.5%.\n- Disk failure addressed with 7.0-hour rebuild; rebalancer excluded rebuilding node to avoid I/O contention.\n- Quota hard blocks reduced from 7 to 6 after enforcing pause on non-critical backfills and continuing EC migration for cold datasets.\n\n### On-Call\n- Shift: Fatima A. 7 pages. 14 tickets. Status: Stable but operating in emergency capacity mode. Backlogs improved; strict quotas and scheduled compaction remain in effect until expansion lands.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_029",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-12T10:00:00",
          "text": "## 2024-02-12 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: REQUESTS 154800 | p50: 195ms p95: 540ms p99: 1040ms | err: 2.71% (4200 errors) | success: 97.29%\n- /fraud_check: REQUESTS 154300 | p50: 170ms p95: 460ms p99: 800ms | err: 0.71% (1100 errors) | success: 99.29%\n- /geo_lookup: REQUESTS 149700 | p50: 340ms p95: 680ms p99: 850ms | err: 0.41% (620 errors) | success: 99.59%\n- /auth: REQUESTS 201900 | p50: 48ms p95: 124ms p99: 235ms | err: 0.16% (320 errors) | success: 99.84%\n- /product_catalog: REQUESTS 272900 | p50: 63ms p95: 190ms p99: 350ms | err: 0.15% (420 errors) | success: 99.85%\n- /search: REQUESTS 239600 | p50: 84ms p95: 235ms p99: 420ms | err: 0.23% (560 errors) | success: 99.77%\n- /recommendations: REQUESTS 193400 | p50: 106ms p95: 300ms p99: 520ms | err: 0.27% (520 errors) | success: 99.73%\n\n### Infrastructure\n- gateway-01: CPU 69% | Mem 79% | Disk 63% | Conns: 6250 | Net: 700/850 Mbps\n- gateway-02: CPU 66% | Mem 77% | Disk 62% | Conns: 6105 | Net: 682/828 Mbps\n- service-b-01: CPU 70% | Mem 79% | Disk 61% | Conns: 6200 | Net: 660/770 Mbps\n- metrics-db-01: CPU 46% | Mem 79% | Disk 88% | Conns: 430 | Net: 178/225 Mbps\n\n### Connection Pools\n- primary: active 190 | idle 70 | waiting 150 | exhaustion: 22 | max: 260 | avg_wait: 110ms\n- replica: active 86 | idle 14 | waiting 35 | exhaustion: 8 | max: 100 | avg_wait: 32ms\n- third_party_geo: active 85 | idle 35 | waiting 90 | exhaustion: 18 | max: 120 | avg_wait: 130ms\n\n### CDN & Caching\n- Hit rate: 91.9% | Bandwidth: 9.1 Gbps | Origin requests: 472300\n\n### Alerts\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-01: pool=primary events=22 waiting=150\n- [CRITICAL] CHECKOUT-ERROR-RATE on gateway-02: error_pct=2.71\n\n### Deployments & Changes\n- No deployments\n\n### Events\n- Feature flag 'skip-geo-on-timeout' set to 95% of traffic\n- A/B test 'checkout-v2' at 15% rollout\n- Vendor provided alternate geo endpoint host (DNS change prepared)\n- Scaled checkout-worker pool from 12 to 14 instances\n\n### On-Call\n- shift handoff note: Shift: A. Chen. 9 pages. 3 tickets (DNS-77 geo endpoint, CO-940, POOL-689). Status: Mitigating.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_087",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-12T10:30:00",
          "text": "## 2026-01-28 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: 87000 p95: 87000 p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-23: CPU 97% | Mem 91% | Disk Used 99% | Disk Busy 99% | Conns: N/A | Net: 11.8/10.2 Gbps\n- stor-b-22: CPU 80% | Mem 82% | Disk Used 98% | Disk Busy 97% | Conns: N/A | Net: 8.9/8.1 Gbps\n- stor-c-25: CPU 71% | Mem 77% | Disk Used 97% | Disk Busy 92% | Conns: N/A | Net: 7.3/6.7 Gbps\n- meta-02: CPU 85% | Mem 96% | Disk Used 91% | Disk Busy 85% | Conns: N/A | Net: 5.0/4.2 Gbps\n\n### Connection Pools\n- pool-1: active 120 | idle 30 | waiting 5 | exhaustion 2 | max: 150 | avg_wait: 12ms\n- pool-2: active 95 | idle 25 | waiting 3 | exhaustion 1 | max: 130 | avg_wait: 9ms\n- pool-3: active 80 | idle 20 | waiting 4 | exhaustion 0 | max: 100 | avg_wait: 7ms\n\n### CDN & Caching\n- Hit rate: 86.7% | Bandwidth: 4.2 Gbps | Origin requests: 7200\n\n### Alerts\n- [SEVERE] quota_blocks on hostname: 8\n- [SEVERE] capacity_pressure on hostname: 91.6%\n- [WARNING] compaction_pending: 152\n- [WARNING] scrub_errors: 52\n- [CRITICAL] metadata_cache_hit: 86.7%\n- [CRITICAL] manifest_backlog: 7200\n\n### Deployments & Changes\n- No deployments or changes today.\n\n### Events\n- Massive ingest (+246 TB) pushed utilization to 91.6% and triggered 8 quota hard blocks; executed emergency write restrictions for three non-essential namespaces.\n- Compaction pending spiked to 152 with disk busy p95 96.2%; scheduled immediate compaction surge and reduced background scrub concurrency to avoid saturation.\n- Scrub errors rose to 52; focused on the noisiest keyspaces and requested tenant-side delete batching to reduce tombstone fragmentation.\n- Metadata cache hit dropped to 86.7% with manifest backlog 7.2k; paused all optional inventory jobs and increased metadata worker count by 1.\n\n### On-Call\n- Shift: Ben K. 14 pages. 24 tickets. Status: Degraded; severe capacity pressure with quota blocks and compaction/scrub overload. Emergency write restrictions and metadata/job deferrals applied; compaction surge underway.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_088",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-12T10:30:00",
          "text": "## 2026-01-29 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-23: CPU 84% | Mem 88% | Disk Used 99% | Conns: N/A | Net: In 9.4 Gbps / Out 8.5 Gbps\n- stor-b-22: CPU 74% | Mem 81% | Disk Used 98% | Conns: N/A | Net: In 7.9 Gbps / Out 7.4 Gbps\n- stor-c-25: CPU 67% | Mem 76% | Disk Used 97% | Conns: N/A | Net: In 6.8 Gbps / Out 6.5 Gbps\n- meta-03: CPU 82% | Mem 96% | Disk Used 91% | Conns: N/A | Net: In 4.7 Gbps / Out 4.1 Gbps\n\n### Connection Pools\n- pool-1: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- pool-2: active N/A | idle N/A | waiting N/A | exhaustion N/A | max N/A | avg_wait N/A\n- (Note: Specific pool data not provided; placeholders used)\n\n### CDN & Caching\n- Hit rate: N/A\n- Bandwidth: N/A Gbps\n- Origin requests: N/A\n\n### Alerts\n- [CRITICAL] Storage capacity at 92% overall utilization\n- [WARNING] 8 tenants over quota; 43 soft limit hits; 7 hard blocks\n- [ERROR] Disk failure repaired; 1 disk failure recorded\n- [WARNING] 3 unhealthy OSDs\n- [ERROR] 34 scrub errors\n- [INFO] Network drops: 22 incidents\n- [INFO] Metadata database size: 1330 GB\n- [INFO] Manifest backlog: 5200\n- [INFO] Metadata QPS p95: 83,000\n- [INFO] Last restore RTO: 48 min\n\n### Deployments & Changes\n- Emergency compaction surge reduced pending from 152 to 118; scrub errors decreased to 34; fsync p95 improved to 16.7 ms\n- Completed 16 rebalance jobs, achieving skew index 1.01; no node exceeded 99% utilization\n- Disk failure repaired with 6.6-hour rebuild; verified placement groups clean post-rebuild\n- Snapshot verification had one failure due to verifier disk filling; expanded verifier volume and re-verified successfully\n\n### Events\n- Emergency compaction surge reduced pending from 152 to 118; scrub errors decreased to 34 and fsync p95 improved to 16.7 ms\n- Completed 16 rebalance jobs, achieving skew index 1.01 and preventing any node from exceeding 99% used\n- Disk failure repaired with 6.6-hour rebuild; verified placement groups clean post-rebuild\n- Snapshot verification had one failure due to verifier disk filling; expanded verifier volume and re-verified successfully\n\n### On-Call\n- Shift: Helena D. 8 pages. 15 tickets. Status: Stable but extremely constrained; utilization at 92%. Emergency controls remain (write restrictions, onboarding freeze, strict quota policy) until additional capacity is deployed",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_089",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-12T10:30:00",
          "text": "## 2026-01-30 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster/usage: REQUESTS 12000 req | p50: 45ms p95: 120ms p99: 250ms | err: 0.2% (24 errors) | success: 99.8%\n- /storage/node/utilization: REQUESTS 9500 req | p50: 30ms p95: 85ms p99: 150ms | err: 0.1% (10 errors) | success: 99.9%\n- /storage/rebalance/start: REQUESTS 300 req | p50: 60ms p95: 150ms p99: 300ms | err: 0.3% (1 error) | success: 99.7%\n- /storage/compaction/run: REQUESTS 8000 req | p50: 55ms p95: 130ms p99: 280ms | err: 0.2% (16 errors) | success: 99.8%\n- /storage/backup/verify: REQUESTS 2000 req | p50: 42ms p95: 100ms p99: 180ms | err: 0.0% (0 errors) | success: 100%\n- /storage/quota/check: REQUESTS 5000 req | p50: 25ms p95: 70ms p99: 140ms | err: 0.1% (5 errors) | success: 99.9%\n- /storage/ingest: REQUESTS 4000 req | p50: 35ms p95: 90ms p99: 200ms | err: 0.2% (8 errors) | success: 99.8%\n- /storage/metadata/query: REQUESTS 15000 req | p50: 40ms p95: 95ms p99: 170ms | err: 0.1% (15 errors) | success: 99.9%\n- /storage/health/status: REQUESTS 6000 req | p50: 20ms p95: 50ms p99: 100ms | err: 0.0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-07: CPU 98% | Mem 92% | Disk Used 99% | Conns: 1200 | Net: 12.1/10.6 Gbps\n- stor-b-18: CPU 82% | Mem 84% | Disk Used 99% | Conns: 950 | Net: 9.2/8.4 Gbps\n- stor-c-26: CPU 73% | Mem 78% | Disk Used 97% | Conns: 800 | Net: 7.5/6.9 Gbps\n- meta-01: CPU 88% | Mem 97% | Disk Used 92% | Conns: 650 | Net: 5.3/4.5 Gbps\n\n### Connection Pools\n- pool-storage: active 150 | idle 20 | waiting 5 | exhaustion 2 | max 200 | avg_wait 12ms\n- pool-meta: active 50 | idle 10 | waiting 2 | exhaustion 0 | max 70 | avg_wait 8ms\n- pool-rebalance: active 16 | idle 4 | waiting 1 | exhaustion 0 | max 25 | avg_wait 15ms\n- pool-io: active 80 | idle 15 | waiting 3 | exhaustion 1 | max 100 | avg_wait 10ms\n\n### CDN & Caching\n- Hit rate: 85% | Bandwidth: 4.2 Gbps | Origin requests: 12,000\n\n### Alerts\n- [CRITICAL] Capacity threshold on stor-a-07: utilization 99%\n- [SEVERE] Quota hard blocks on tenants: 9 blocks active\n- [WARNING] Pending compactions: 164\n- [ERROR] Scrub errors: 58\n- [INFO] Restore drill completed: RTO 51 min\n\n### Deployments & Changes\n- Capacity controls activated: halted non-critical writes, approval required for bulk ingest\n- War-room established with tenant owners for quota management\n- Continuous compaction mode initiated, discretionary maintenance postponed\n- Restore drill performed for audit readiness, bandwidth reservation documented\n\n### Events\n- Utilization reached 92.8% effective free only 70 TB; invoked incident-level capacity controls: halt all non-critical writes and require approvals for any bulk ingest\n- Quota hard blocks escalated to 9; established war-room with tenant owners to coordinate immediate deletions and archival migrations\n- Compaction pending surged to 164 with scrub errors 58; initiated continuous compaction mode and postponed all discretionary maintenance\n- Restore drill performed for audit readiness; RTO 51 minutes, noted that restore speed is impacted by high disk busy\u2014documented mitigation to reserve bandwidth during drills\n\n### On-Call\n- Shift: Calvin U. 16 pages. 27 tickets. Status: Degraded/Critical; near-capacity operation with strict write controls, continuous compaction, and active tenant coordination for emergency space recovery",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        },
        {
          "episode_id": "cascading_failure_01_ep_030",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-13T10:00:00",
          "text": "## 2024-02-13 Daily Operations Summary\n\n### Endpoint Performance\n- /checkout: 153200 req | p50: 170ms p95: 480ms p99: 940ms | err: 1.96% (3000 errors) | success: 98.04%\n- /fraud_check: 152800 req | p50: 150ms p95: 420ms p99: 740ms | err: 0.54% (820 errors) | success: 99.46%\n- /geo_lookup: 148100 req | p50: 330ms p95: 670ms p99: 850ms | err: 0.39% (580 errors) | success: 99.61%\n- /auth: 200100 req | p50: 45ms p95: 118ms p99: 225ms | err: 0.14% (280 errors) | success: 99.86%\n- /product_catalog: 270500 req | p50: 60ms p95: 182ms p99: 335ms | err: 0.14% (380 errors) | success: 99.86%\n- /search: 237200 req | p50: 80ms p95: 225ms p99: 400ms | err: 0.22% (520 errors) | success: 99.78%\n- /recommendations: 191900 req | p50: 100ms p95: 285ms p99: 500ms | err: 0.25% (480 errors) | success: 99.75%\n\n### Infrastructure\n- gateway-01: CPU 62% | Mem 75% | Disk 63% | Conns: 5400 | Net: 665/805 Mbps\n- gateway-02: CPU 60% | Mem 73% | Disk 62% | Conns: 5250 | Net: 648/782 Mbps\n- service-b-01: CPU 62% | Mem 73% | Disk 61% | Conns: 5200 | Net: 590/680 Mbps\n- metrics-db-01: CPU 42% | Mem 76% | Disk 88% | Conns: 395 | Net: 165/210 Mbps\n\n### Connection Pools\n- primary: active 165 | idle 95 | waiting 80 | exhaustion: 12 | max: 260 | avg_wait: 60ms\n- replica: active 78 | idle 22 | waiting 18 | exhaustion: 4 | max: 100 | avg_wait: 18ms\n- third_party_geo: active 72 | idle 48 | waiting 55 | exhaustion: 10 | max: 120 | avg_wait: 85ms\n\n### CDN & Caching\n- Hit rate: 92.3% | Bandwidth: 8.7 Gbps | Origin requests: 455900\n\n### Alerts\n- [CRITICAL] SERVICE-B-RETRY-COUNT on service-b-01: retry_count=780 retry_rate_pct=0.62\n- [CRITICAL] UPSTREAM-LATENCY on gateway-01: geo_lookup_p99_ms=850\n- [CRITICAL] CONNPOOL-EXHAUSTION on gateway-02: pool=third_party_geo events=10 waiting=55\n\n### Deployments & Changes\n- Config change: geo vendor endpoint switched to alternate host (DNS updated)\n\n### Events\n- Updated DNS to use alternate geo vendor endpoint host\n- A/B test 'checkout-v2' at 15% rollout\n- Feature flag 'skip-geo-on-timeout' set to 95% of traffic\n- Post-incident review meeting scheduled 2024-02-14 17:00Z\n\n### On-Call\n- Shift: K. Okafor. 7 pages. 2 tickets (DNS-79 verify geo endpoint, PIR-12 scheduling). Status: Stabilizing.",
          "meta": {
            "episode_type": "signal",
            "phase": "root_cause",
            "signal_density": "high"
          }
        },
        {
          "episode_id": "cascading_failure_01_dx_090",
          "scope_id": "cascading_failure_01",
          "timestamp": "2024-02-13T10:30:00",
          "text": "## 2026-01-31 Daily Operations Summary\n\n### Endpoint Performance\n- /storage/cluster_capacity: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/node_utilization: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/rebalance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/compaction: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/io: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/storage_efficiency: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/backup_verification: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/quota_enforcement: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/ingest: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/reliability: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/metadata: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n- /storage/maintenance: REQUESTS req | p50: N/A p95: N/A p99: N/A | err: 0% (0 errors) | success: 100%\n\n### Infrastructure\n- stor-a-07: CPU 86% | Mem 90% | Disk Used 99% | Conns: N/A | Net: In 9.7 Gbps / Out 8.9 Gbps\n- stor-b-18: CPU 76% | Mem 83% | Disk Used 99% | Conns: N/A | Net: In 8.1 Gbps / Out 7.6 Gbps\n- stor-c-26: CPU 69% | Mem 77% | Disk Used 97% | Conns: N/A | Net: In 7.0 Gbps / Out 6.8 Gbps\n- meta-02: CPU 84% | Mem 97% | Disk Used 92% | Conns: N/A | Net: In 5.0 Gbps / Out 4.4 Gbps\n\n### Connection Pools\n(Information not provided in data sheet)\n\n### CDN & Caching\n(Information not provided in data sheet)\n\n### Alerts\n- [CRITICAL] Storage cluster capacity high on nodes: 94 nodes over 80% utilization, 41 over 90%, hottest node at 98.4%\n- [WARNING] Quota enforcement: 9 tenants over quota, 49 soft limit hits, 8 hard blocks\n- [INFO] Storage efficiency: Compression ratio 1.71, dedupe ratio 1.01, garbage 8.4%, tombstone 5.6%\n- [CRITICAL] Disk failure: 1 disk failed, 1 rebuild completed in 7.3 hours\n- [WARNING] Node restart: 1 node restarted\n- [WARNING] Unhealthy OSDs: 3\n- [WARNING] Scrub errors: 36\n- [INFO] Network drops: 24\n- [INFO] Metadata database size: 1361 GB\n- [CRITICAL] Manifest backlog: 5600 entries\n- [INFO] Metadata QPS (p95): 86000\n- [INFO] Metadata cache hit rate: 87.4%\n- [INFO] Firmware updates: 0\n- [INFO] Kernel patches: 0\n- [INFO] Rack power cycles: 0\n- [INFO] Audit log size: 96 GB\n\n### Deployments & Changes\n- Emergency space recovery actions (tenant deletions + archival) increased egress to 168 TB; quota hard blocks reduced from 9 to 8; manifest backlog dropped to 5.6k.\n- Continuous compaction mode reduced pending from 164 to 120; scrub errors decreased to 36; disk busy under 96%.\n- Completed 17 rebalance jobs; skew index 1.00; no node exceeded 99% used during recovery.\n- Disk failure replaced and rebuilt in 7.3 hours; limited writes re-enabled for two namespaces after quota and headroom confirmation.\n\n### Events\n- Emergency space recovery actions increased egress to 168 TB; quota hard blocks reduced from 9 to 8; manifest backlog dropped to 5.6k.\n- Continuous compaction mode reduced pending from 164 to 120; scrub errors decreased to 36 while disk busy remained under 96%.\n- 17 rebalance jobs completed, skew index 1.00, no node exceeded 99% utilization during recovery.\n- Disk failure replaced and rebuilt in 7.3 hours; limited writes re-enabled for two namespaces after confirming quota compliance and headroom.\n\n### On-Call\n- Shift: Yara M. 12 pages. 20 tickets. Status: Degraded/Critical; slight recovery via deletions and continuous compaction, but cluster remains above 93% utilized with strict governance and limited write allowances.",
          "meta": {
            "episode_type": "distractor",
            "theme": "storage_capacity"
          }
        }
      ]
    }
  ],
  "questions": [
    {
      "question_id": "cf01_q01_longitudinal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 38,
      "question_type": "longitudinal",
      "prompt": "Based on the logs so far, are there any concerning patterns in service health?",
      "ground_truth": {
        "canonical_answer": "Geo-lookup latency has been gradually increasing. Service-B retry rates are slightly elevated. No user-facing impact yet but worth monitoring.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_010"
        ],
        "key_facts": [
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike"
        ]
      }
    },
    {
      "question_id": "cf01_q02_null",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 38,
      "question_type": "null_hypothesis",
      "prompt": "What were the key operational events on January 20th?",
      "ground_truth": {
        "canonical_answer": "A routine day of operations as documented in the daily log summary.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_006"
        ],
        "key_facts": []
      }
    },
    {
      "question_id": "cf01_q03_longitudinal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "longitudinal",
      "prompt": "What is the root cause of the checkout failures?",
      "ground_truth": {
        "canonical_answer": "Connection pool exhaustion caused by service-B fraud-check retries against a degrading third-party geo-lookup API. The service-C deploy was a red herring.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_024",
          "cascading_failure_01_ep_015"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "service-C deploy is not the root cause",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure",
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor"
        ]
      }
    },
    {
      "question_id": "cf01_q04_action",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 119,
      "question_type": "action_recommendation",
      "prompt": "What actions should the team take to prevent this from recurring?",
      "ground_truth": {
        "canonical_answer": "Separate connection pools for fraud checks and checkout. Add circuit breaker on geo-lookup calls. Monitor third-party API latency with alerting.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_024",
          "cascading_failure_01_ep_027"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure",
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor"
        ]
      }
    },
    {
      "question_id": "cf01_q05_negative",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 58,
      "question_type": "negative",
      "prompt": "Is there evidence of DNS infrastructure failure in the logs?",
      "ground_truth": {
        "canonical_answer": "No. DNS metrics are stable throughout. The issue is geo-lookup API latency degradation and connection pool exhaustion, not DNS infrastructure failure.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_003",
          "cascading_failure_01_ep_011"
        ],
        "key_facts": [
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure",
          "geo-lookup API latency increasing"
        ]
      }
    },
    {
      "question_id": "cf01_q06_paraphrase",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "paraphrase",
      "prompt": "Why are checkout requests failing?",
      "ground_truth": {
        "canonical_answer": "Connection pool exhaustion caused by service-B fraud-check retries against a degrading third-party geo-lookup API. The service-C deploy was a red herring.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_024",
          "cascading_failure_01_ep_015"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "service-C deploy is not the root cause",
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure"
        ]
      }
    },
    {
      "question_id": "cf01_q07_temporal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "temporal",
      "prompt": "When did geo-lookup latency first start degrading, and over how many reporting periods did it progress to the current level?",
      "ground_truth": {
        "canonical_answer": "Geo-lookup latency began degrading during the early signal phase, starting around episode 9. It progressed over approximately 15-20 reporting periods from 200ms baseline to 800ms+, accelerating during the escalation phase.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_012",
          "cascading_failure_01_ep_017",
          "cascading_failure_01_ep_025"
        ],
        "key_facts": [
          "geo-lookup API latency increasing",
          "connection pool exhaustion",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure",
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor"
        ]
      }
    },
    {
      "question_id": "cf01_q08_counterfactual",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "counterfactual",
      "prompt": "If the service-C deployment were responsible for the checkout failures, what pattern would you expect in the data? Does that match what you see?",
      "ground_truth": {
        "canonical_answer": "If the service-C deploy caused the failures, you would expect a sudden onset coinciding with the deploy and resolution after rollback. Instead, the degradation was gradual (geo-lookup latency creeping over weeks) and the rollback did not resolve the issue. The actual cause is upstream geo-lookup API degradation driving retry storms and pool exhaustion.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_015",
          "cascading_failure_01_ep_016",
          "cascading_failure_01_ep_024"
        ],
        "key_facts": [
          "service-C deploy is not the root cause",
          "geo-lookup API latency increasing",
          "connection pool exhaustion",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike",
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor"
        ]
      }
    },
    {
      "question_id": "cf01_q09_longitudinal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 18,
      "question_type": "longitudinal",
      "prompt": "Based on the first few days of logs, are there any service health patterns emerging?",
      "ground_truth": {
        "canonical_answer": "No significant patterns are visible yet. All services are operating within baseline parameters. Metrics are consistent with normal operations.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_003",
          "cascading_failure_01_ep_005"
        ],
        "key_facts": []
      }
    },
    {
      "question_id": "cf01_q10_evidence",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 18,
      "question_type": "evidence_sufficiency",
      "prompt": "Do you have sufficient data to identify any concerning service trends?",
      "ground_truth": {
        "canonical_answer": "No. Only 5 days of baseline data are available, which is insufficient to establish trends or identify anomalies. More observation time is needed.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_003"
        ],
        "key_facts": []
      }
    },
    {
      "question_id": "cf01_q11_longitudinal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "longitudinal",
      "prompt": "What is the current impact on user-facing services and what appears to be driving it?",
      "ground_truth": {
        "canonical_answer": "Checkout failures are increasing. The root cause appears to be connection pool exhaustion driven by service-B retry storms against the degrading geo-lookup API. The service-C rollback did not resolve the issue.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_016",
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_020"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "service-C deploy is not the root cause"
        ]
      }
    },
    {
      "question_id": "cf01_q12_severity",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "severity_assessment",
      "prompt": "How would you assess the severity of the current service degradation?",
      "ground_truth": {
        "canonical_answer": "High severity. Checkout failures are directly impacting revenue. The issue is escalating \u2014 geo-lookup latency continues to worsen, retry storms are amplifying the problem, and connection pool exhaustion is spreading to affect other services sharing the pool.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_020"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "service-B retry rate elevated"
        ]
      }
    },
    {
      "question_id": "cf01_q13_paraphrase",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 38,
      "question_type": "paraphrase",
      "prompt": "Are any upstream dependencies showing degraded performance?",
      "ground_truth": {
        "canonical_answer": "Yes. Geo-lookup latency has been gradually increasing. Service-B retry rates are slightly elevated. No user-facing impact yet but worth monitoring.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_010"
        ],
        "key_facts": [
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike"
        ]
      }
    },
    {
      "question_id": "cf01_q14_paraphrase",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "paraphrase",
      "prompt": "What sequence of events led to the checkout outage?",
      "ground_truth": {
        "canonical_answer": "Geo-lookup API latency degradation caused service-B fraud-check retries to escalate, which exhausted the shared connection pool, leading to checkout failures. The service-C deploy was a coincidence, not the cause.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_024",
          "cascading_failure_01_ep_015"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "service-B retry rate elevated",
          "service-C deploy is not the root cause",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure"
        ]
      }
    },
    {
      "question_id": "cf01_q15_negative",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "negative",
      "prompt": "Is there evidence of storage capacity issues contributing to the service failures?",
      "ground_truth": {
        "canonical_answer": "No. Storage cluster metrics show normal utilization with no capacity or performance issues. The service degradation is caused by connection pool exhaustion from geo-lookup API latency, not storage.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_004",
          "cascading_failure_01_ep_011"
        ],
        "key_facts": [
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "connection pool exhaustion"
        ]
      }
    },
    {
      "question_id": "cf01_q16_negative",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "negative",
      "prompt": "Are there authentication or SSO failures contributing to the checkout issues?",
      "ground_truth": {
        "canonical_answer": "No. Authentication systems show normal operation with no anomalies. The checkout failures are caused by connection pool exhaustion from geo-lookup API latency degradation.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_005",
          "cascading_failure_01_ep_012"
        ],
        "key_facts": [
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor",
          "connection pool exhaustion",
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure"
        ]
      }
    },
    {
      "question_id": "cf01_q17_temporal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "temporal",
      "prompt": "How quickly is the checkout error rate escalating?",
      "ground_truth": {
        "canonical_answer": "The checkout error rate accelerated significantly during the escalation phase. Initially minimal during early signal detection, it ramped up rapidly as connection pool exhaustion worsened, roughly doubling every few reporting periods.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_016",
          "cascading_failure_01_ep_018",
          "cascading_failure_01_ep_020"
        ],
        "key_facts": [
          "connection pool exhaustion",
          "geo-lookup API latency increasing",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike"
        ]
      }
    },
    {
      "question_id": "cf01_q18_temporal",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 58,
      "question_type": "temporal",
      "prompt": "When did service-B retry rates first show elevation above baseline?",
      "ground_truth": {
        "canonical_answer": "Service-B retry rates first showed elevation during the early signal phase, starting around episode 10, coinciding with the initial geo-lookup latency degradation.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_010"
        ],
        "key_facts": [
          "service-B retry rate elevated",
          "geo-lookup API latency increasing",
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor"
        ]
      }
    },
    {
      "question_id": "cf01_q19_distractor",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 58,
      "question_type": "distractor_resistance",
      "prompt": "Are there any DNS zone transfer or propagation issues affecting service resolution?",
      "ground_truth": {
        "canonical_answer": "No. DNS infrastructure is operating normally with no zone transfer failures or propagation delays. The service degradation involves the geo-lookup API latency, which is a third-party API call, not a DNS infrastructure issue.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_003",
          "cascading_failure_01_ep_009"
        ],
        "key_facts": [
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure",
          "geo-lookup API latency increasing"
        ]
      }
    },
    {
      "question_id": "cf01_q20_distractor",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 99,
      "question_type": "distractor_resistance",
      "prompt": "Is storage rebalancing or compaction activity correlating with the service latency spikes?",
      "ground_truth": {
        "canonical_answer": "No. Storage operations are running normally. The latency spikes are caused by the geo-lookup API degradation and resulting connection pool exhaustion, not storage-related activity.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_004",
          "cascading_failure_01_ep_018"
        ],
        "key_facts": [
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "geo-lookup API latency increasing",
          "connection pool exhaustion"
        ]
      }
    },
    {
      "question_id": "cf01_q21_distractor",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "distractor_resistance",
      "prompt": "Are IAM policy changes or token rotation events related to the service disruption?",
      "ground_truth": {
        "canonical_answer": "No. IAM operations are routine. The service failures are caused by connection pool exhaustion from geo-lookup API degradation, not authentication or access control issues.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_005",
          "cascading_failure_01_ep_016"
        ],
        "key_facts": [
          "user authentication completes successfully before reaching the failing checkout path, ruling out auth as a contributing factor",
          "connection pool exhaustion",
          "the checkout failure chain originates in the geo-lookup service layer, not in storage or persistence infrastructure",
          "the actual root cause is geo-lookup API latency cascading into connection pool exhaustion, not DNS infrastructure failure"
        ]
      }
    },
    {
      "question_id": "cf01_q22_counterfactual",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "counterfactual",
      "prompt": "If the checkout failures were caused by a spike in traffic volume, what pattern would you expect? Does the data match?",
      "ground_truth": {
        "canonical_answer": "A traffic volume spike would show increased hit counts across all endpoints proportionally, with latency degradation correlated to load. Instead, the data shows selective latency degradation in geo-lookup calls with normal traffic volumes, indicating a dependency failure rather than a capacity issue.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_016",
          "cascading_failure_01_ep_018"
        ],
        "key_facts": [
          "geo-lookup API latency increasing",
          "connection pool exhaustion",
          "geo-lookup latency degraded progressively over multiple reporting periods with p99 values increasing from baseline through each successive day rather than appearing as a single sudden spike"
        ]
      }
    },
    {
      "question_id": "cf01_q23_null",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 79,
      "question_type": "null_hypothesis",
      "prompt": "What was the CDN cache hit rate reported on February 1st?",
      "ground_truth": {
        "canonical_answer": "The CDN cache hit rate as documented in the daily API gateway log summary.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_018"
        ],
        "key_facts": []
      }
    },
    {
      "question_id": "cf01_q24_evidence",
      "scope_id": "cascading_failure_01",
      "checkpoint_after": 58,
      "question_type": "evidence_sufficiency",
      "prompt": "Is there sufficient evidence to begin investigating a specific root cause for the emerging service patterns?",
      "ground_truth": {
        "canonical_answer": "Yes. The correlation between geo-lookup latency increase and service-B retry elevation is visible across multiple reporting periods. While the full causal chain is not yet clear, there is enough evidence to investigate the geo-lookup dependency.",
        "required_evidence_refs": [
          "cascading_failure_01_ep_009",
          "cascading_failure_01_ep_010",
          "cascading_failure_01_ep_012"
        ],
        "key_facts": [
          "geo-lookup API latency increasing",
          "service-B retry rate elevated"
        ]
      }
    }
  ]
}